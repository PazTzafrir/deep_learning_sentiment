{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pruning Analysis\n",
    "\n",
    "**L1 Unstructured Pruning with 50% Sparsity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taltzafrir/At_Bay/paz/deep/deep_learning_sentiment/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch.nn.utils.prune as prune\n",
    "import evaluate\n",
    "import wandb\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured for CPU-only execution\n",
      "Please restart the kernel and run all cells from the beginning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables to force CPU usage and disable MPS\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "print(\"Environment configured for CPU-only execution\")\n",
    "print(\"Please restart the kernel and run all cells from the beginning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q evaluate\n",
    "%pip install -q emoji==0.6.0\n",
    "%pip install -q torch\n",
    "%pip install -q transformers\n",
    "%pip install -q accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "**IMPORTANT:** Before running the notebook, modify the `MODEL_TYPE` flag in the next cell to select your desired model:\n",
    "- Set `MODEL_TYPE = \"roberta\"` for RoBERTa model\n",
    "- Set `MODEL_TYPE = \"bert\"` for BERTweet model\n",
    "\n",
    "This is your responsibility to ensure the correct model and paths are configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ROBERTA model: cardiffnlp/twitter-roberta-base-sentiment\n",
      "Model path: ./HuggingFace/roberta/best_roberta_model_so_far\n",
      "Model file: model_roberta.pt\n"
     ]
    }
   ],
   "source": [
    "# Model selection flag\n",
    "MODEL_TYPE = \"roberta\"   # Change to \"roberta\" for RoBERTa\n",
    "\n",
    "# Model configuration based on flag\n",
    "if MODEL_TYPE == \"roberta\":\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    best_model_path = \"./HuggingFace/roberta/best_roberta_model_so_far\"\n",
    "    model_file = \"model_roberta.pt\"\n",
    "elif MODEL_TYPE == \"bert\":\n",
    "    model_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "    best_model_path = \"./Full model/bert/best_bert_model_so_far\"\n",
    "    model_file = \"model_bert.pt\"\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "print(f\"Using {MODEL_TYPE.upper()} model: {model_name}\")\n",
    "print(f\"Model path: {best_model_path}\")\n",
    "print(f\"Model file: {model_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "MPS backend disabled - using CPU only\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'  # Fallback to CPU for unsupported ops\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # Disable MPS\n",
    "\n",
    "# Force CPU device\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Disable MPS backend if available (Apple Silicon)\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    torch.backends.mps.is_built = lambda: False\n",
    "    print(\"MPS backend disabled - using CPU only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging to wandb.ai account\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/taltzafrir/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mttzafrir\u001b[0m (\u001b[33mat-bay-data-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('logging to wandb.ai account')\n",
    "wandb.login(key=\"6dd13a6018f089606e418d323dd8b502f31bca4e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"OOT_train.csv\", encoding='latin-1')\n",
    "val = pd.read_csv(\"OOT_val.csv\", encoding='latin-1')\n",
    "test = pd.read_csv(\"OOT_test.csv\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the labels numerically from Sentiment\n",
    "ordinal_mapping = {\n",
    "    'Extremely Negative': 0,\n",
    "    'Negative': 1,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 3,\n",
    "    'Extremely Positive': 4\n",
    "}\n",
    "\n",
    "# map to ordinal labels\n",
    "train[\"ordinal_label_id\"] = train[\"Sentiment\"].map(ordinal_mapping)\n",
    "val[\"ordinal_label_id\"] = val[\"Sentiment\"].map(ordinal_mapping)\n",
    "test[\"ordinal_label_id\"] = test[\"Sentiment\"].map(ordinal_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the input string from multiple columns\n",
    "def build_augmented_input(row):\n",
    "    parts = []\n",
    "\n",
    "    if pd.notna(row.get('clean_tweet')):\n",
    "        parts.append(f\"{row['clean_tweet']}\")\n",
    "\n",
    "    if pd.notna(row.get('Location_standardized')) and row['Location_standardized'].lower() != 'unknown':\n",
    "        parts.append(f\"{row['Location_standardized']}\")\n",
    "\n",
    "    if pd.notna(row.get('TweetAt')):\n",
    "        parts.append(f\"{row['TweetAt']}\")\n",
    "\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "# Apply to the DataFrames\n",
    "train['model_input'] = train.apply(build_augmented_input, axis=1)\n",
    "val['model_input'] = val.apply(build_augmented_input, axis=1)\n",
    "test['model_input'] = test.apply(build_augmented_input, axis=1)\n",
    "\n",
    "# Create new DataFrames with only what's needed for modeling\n",
    "formatted_train = train[['model_input', 'ordinal_label_id']].copy()\n",
    "formatted_val = val[['model_input', 'ordinal_label_id']].copy()\n",
    "formatted_test = test[['model_input', 'ordinal_label_id']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "ordinal_label_id\n",
      "0     5175\n",
      "1     9230\n",
      "2     6784\n",
      "3    10140\n",
      "4     5845\n",
      "Name: count, dtype: int64\n",
      "Class 0: 5000 samples (undersampled)\n",
      "Class 1: 5000 samples (undersampled)\n",
      "Class 2: 5000 samples (undersampled)\n",
      "Class 3: 5000 samples (undersampled)\n",
      "Class 4: 5000 samples (undersampled)\n",
      "Balanced dataset: 25000 total samples\n"
     ]
    }
   ],
   "source": [
    "# Balance dataset by undersampling\n",
    "def balance_dataset(df, target_samples_per_class=5000):\n",
    "    balanced_dfs = []\n",
    "    print(\"Original class distribution:\")\n",
    "    print(df['ordinal_label_id'].value_counts().sort_index())\n",
    "    \n",
    "    for class_id in range(5):\n",
    "        class_data = df[df['ordinal_label_id'] == class_id]\n",
    "        if len(class_data) > target_samples_per_class:\n",
    "            class_data = class_data.sample(n=target_samples_per_class, random_state=42)\n",
    "            print(f\"Class {class_id}: {len(class_data)} samples (undersampled)\")\n",
    "        else:\n",
    "            print(f\"Class {class_id}: {len(class_data)} samples (kept all)\")\n",
    "        balanced_dfs.append(class_data)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True).sample(frac=1, random_state=42)\n",
    "    print(f\"Balanced dataset: {len(balanced_df)} total samples\")\n",
    "    return balanced_df\n",
    "\n",
    "formatted_train = balance_dataset(formatted_train, target_samples_per_class=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_data(data, max_length=128):\n",
    "    return tokenizer(\n",
    "        data['model_input'].tolist(),\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(formatted_train)\n",
    "val_encodings = tokenize_data(formatted_val)\n",
    "test_encodings = tokenize_data(formatted_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a PyTorch Dataset\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert labels to integers\n",
    "train_labels = formatted_train['ordinal_label_id'].tolist()\n",
    "val_labels = formatted_val['ordinal_label_id'].tolist()\n",
    "test_labels = formatted_test['ordinal_label_id'].tolist()\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset = TweetDataset(val_encodings, val_labels)\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detailed_metrics(eval_pred):\n",
    "    \"\"\"Enhanced metrics using HuggingFace Evaluate library\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Load HuggingFace metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average='macro'))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average='macro'))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average='macro'))\n",
    "\n",
    "    # Custom ordinal metrics\n",
    "    results['mae'] = np.mean(np.abs(predictions - labels))\n",
    "    results['adjacent_accuracy'] = np.sum(np.abs(predictions - labels) <= 1) / len(labels)\n",
    "\n",
    "    # Quadratic Weighted Kappa\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    try:\n",
    "        qwk = cohen_kappa_score(labels, predictions, weights='quadratic')\n",
    "        results['quadratic_weighted_kappa'] = qwk\n",
    "    except:\n",
    "        results['quadratic_weighted_kappa'] = 0.0\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply L1 Unstructured Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the best model from hyperparameter tuning...\n",
      "Model loaded successfully!\n",
      "Applying L1 unstructured pruning (50% sparsity)...\n",
      "Model pruning complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the original model\n",
    "print(\"Loading the best model from hyperparameter tuning...\")\n",
    "original_model = torch.load(os.path.join(best_model_path, model_file), map_location=device, weights_only=False)\n",
    "original_model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Create a copy for pruning\n",
    "print(\"Applying L1 unstructured pruning (50% sparsity)...\")\n",
    "pruned_model = torch.load(os.path.join(best_model_path, model_file), map_location='cpu', weights_only=False)\n",
    "pruned_model = pruned_model.to('cpu')\n",
    "pruned_model.eval()\n",
    "\n",
    "# Apply L1 unstructured pruning to all Linear and Embedding layers\n",
    "parameters_to_prune = []\n",
    "for name, module in pruned_model.named_modules():\n",
    "    if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):\n",
    "        parameters_to_prune.append((module, 'weight'))\n",
    "\n",
    "# Apply global L1 unstructured pruning with 50% sparsity\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.3,  # 50% sparsity\n",
    ")\n",
    "\n",
    "# Make pruning permanent by removing the pruning masks\n",
    "for module, param_name in parameters_to_prune:\n",
    "    prune.remove(module, param_name)\n",
    "\n",
    "print(\"Model pruning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nModel Size Comparison:\n",
      "Original Model Size: 475.51 MB\n",
      "Pruned Model Size (with zeros): 475.51 MB\n",
      "Effective Pruned Model Size: 237.99 MB\n",
      "Compression Ratio: 2.00x\n",
      "Size Reduction: 50.0%\n"
     ]
    }
   ],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    param_size = sum(param.nelement() * param.element_size() for param in model.parameters())\n",
    "    buffer_size = sum(buffer.nelement() * buffer.element_size() for buffer in model.buffers())\n",
    "    return (param_size + buffer_size) / (1024 * 1024)\n",
    "\n",
    "def get_effective_model_size(model):\n",
    "    \"\"\"Calculate effective model size excluding pruned (zero) parameters\"\"\"\n",
    "    param_size = sum(torch.count_nonzero(param).item() * param.element_size() for param in model.parameters())\n",
    "    buffer_size = sum(buffer.nelement() * buffer.element_size() for buffer in model.buffers())\n",
    "    return (param_size + buffer_size) / (1024 * 1024)\n",
    "\n",
    "# Compare model sizes\n",
    "original_size = get_model_size(original_model)\n",
    "pruned_size = get_model_size(pruned_model)\n",
    "effective_pruned_size = get_effective_model_size(pruned_model)\n",
    "compression_ratio = original_size / effective_pruned_size\n",
    "\n",
    "print(f\"\\\\nModel Size Comparison:\")\n",
    "print(f\"Original Model Size: {original_size:.2f} MB\")\n",
    "print(f\"Pruned Model Size (with zeros): {pruned_size:.2f} MB\")\n",
    "print(f\"Effective Pruned Model Size: {effective_pruned_size:.2f} MB\")\n",
    "print(f\"Compression Ratio: {compression_ratio:.2f}x\")\n",
    "print(f\"Size Reduction: {(1 - effective_pruned_size/original_size) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring inference speed...\n",
      "\\nInference Speed Comparison:\n",
      "Original Model: 45.58 ¬± 6.09 ms\n",
      "Pruned Model: 44.68 ¬± 6.04 ms\n",
      "Speedup: 1.02x\n"
     ]
    }
   ],
   "source": [
    "def measure_inference_time(model, tokenizer, sample_text, num_runs=50):\n",
    "    \"\"\"Measure average inference time for a model\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    # Warm-up runs\n",
    "    for _ in range(5):\n",
    "        inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    # Actual timing runs\n",
    "    for _ in range(num_runs):\n",
    "        inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "# Test inference speed\n",
    "sample_tweet = \"COVID-19 vaccines have been crucial in reducing hospitalizations and saving lives worldwide.\"\n",
    "\n",
    "print(\"Measuring inference speed...\")\n",
    "original_time, original_std = measure_inference_time(original_model, tokenizer, sample_tweet)\n",
    "pruned_time, pruned_std = measure_inference_time(pruned_model, tokenizer, sample_tweet)\n",
    "\n",
    "speedup = original_time / pruned_time\n",
    "\n",
    "print(f\"\\\\nInference Speed Comparison:\")\n",
    "print(f\"Original Model: {original_time*1000:.2f} ¬± {original_std*1000:.2f} ms\")\n",
    "print(f\"Pruned Model: {pruned_time*1000:.2f} ¬± {pruned_std*1000:.2f} ms\")\n",
    "print(f\"Speedup: {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/taltzafrir/At_Bay/paz/deep/deep_learning_sentiment/wandb/run-20250820_005921-wxlhk8gn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning/runs/wxlhk8gn' target=\"_blank\">roberta-l1-unstructured-pruning</a></strong> to <a href='https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning' target=\"_blank\">https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning/runs/wxlhk8gn' target=\"_blank\">https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning/runs/wxlhk8gn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning/runs/wxlhk8gn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1c1632fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize W&B for Pruning Results\n",
    "wandb.init(\n",
    "    project=\"covid-tweet-sentiment-pruning\",\n",
    "    name=f\"{MODEL_TYPE}-l1-unstructured-pruning\",\n",
    "    config={\n",
    "        \"model_type\": f\"{MODEL_TYPE}-base\",\n",
    "        \"pruning_method\": \"l1_unstructured\",\n",
    "        \"sparsity_level\": 0.5,\n",
    "        \"device\": \"cpu\",\n",
    "        \"original_model_size_mb\": original_size,\n",
    "        \"pruned_model_size_mb\": effective_pruned_size,\n",
    "        \"compression_ratio\": compression_ratio,\n",
    "        \"inference_speedup\": speedup\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Original Model on Validation Set...\n",
      "Using 6 threads for inference (out of 12 available cores)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='137' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [137/137 05:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nEvaluating Pruned Model on Validation Set...\n",
      "Using 6 threads for inference (out of 12 available cores)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='137' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [137/137 05:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taltzafrir/At_Bay/paz/deep/deep_learning_sentiment/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nEvaluating Original Model on Test Set...\n",
      "Using 6 threads for inference (out of 12 available cores)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='107' max='107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [107/107 03:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nEvaluating Pruned Model on Test Set...\n",
      "Using 6 threads for inference (out of 12 available cores)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='107' max='107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [107/107 04:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taltzafrir/At_Bay/paz/deep/deep_learning_sentiment/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataset, model_name):\n",
    "    \"\"\"Evaluate a model on a given dataset\"\"\"\n",
    "    model = model.to('cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    # Configure multi-core inference\n",
    "    num_cores = os.cpu_count()\n",
    "    num_threads = max(1, num_cores // 2)\n",
    "    torch.set_num_threads(num_threads)\n",
    "    print(f\"Using {num_threads} threads for inference (out of {num_cores} available cores)\")\n",
    "    \n",
    "    # Create trainer for evaluation\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./temp\",\n",
    "            per_device_eval_batch_size=32, \n",
    "            remove_unused_columns=False,\n",
    "            use_cpu=True,\n",
    "            dataloader_num_workers=0,\n",
    "            dataloader_pin_memory=False,\n",
    "        ),\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_detailed_metrics,\n",
    "    )\n",
    "    \n",
    "    results = trainer.evaluate(dataset)\n",
    "    \n",
    "    # Format results with model name prefix\n",
    "    formatted_results = {}\n",
    "    for key, value in results.items():\n",
    "        new_key = f\"{model_name}_{key}\"\n",
    "        formatted_results[new_key] = value\n",
    "    \n",
    "    return results, formatted_results\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Evaluating Original Model on Validation Set...\")\n",
    "val_results_original, val_formatted_original = evaluate_model(original_model, val_dataset, \"original\")\n",
    "\n",
    "print(\"\\\\nEvaluating Pruned Model on Validation Set...\")\n",
    "val_results_pruned, val_formatted_pruned = evaluate_model(pruned_model, val_dataset, \"pruned\")\n",
    "\n",
    "print(\"\\\\nEvaluating Original Model on Test Set...\")\n",
    "test_results_original, test_formatted_original = evaluate_model(original_model, test_dataset, \"original\")\n",
    "\n",
    "print(\"\\\\nEvaluating Pruned Model on Test Set...\")\n",
    "test_results_pruned, test_formatted_pruned = evaluate_model(pruned_model, test_dataset, \"pruned\")\n",
    "\n",
    "# Log to wandb\n",
    "wandb.log({\n",
    "    **val_formatted_original,\n",
    "    **val_formatted_pruned,\n",
    "    **test_formatted_original,\n",
    "    **test_formatted_pruned\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRUNING RESULTS SUMMARY\n",
      "================================================================================\n",
      "\\nüìä MODEL COMPRESSION:\n",
      "Original Model Size: 475.51 MB\n",
      "Pruned Model Size (with zeros): 475.51 MB\n",
      "Effective Pruned Model Size: 237.99 MB\n",
      "Compression Ratio: 2.00x\n",
      "Size Reduction: 50.0%\n",
      "\\n‚ö° INFERENCE SPEED:\n",
      "Original Model: 45.58 ms\n",
      "Pruned Model: 44.68 ms\n",
      "Speedup: 1.02x\n",
      "\\nüéØ VALIDATION SET PERFORMANCE:\n",
      "                      Original    Pruned      Difference\n",
      "Accuracy:             0.8393      0.5761      -0.2633\n",
      "F1-Score:             0.8412      0.4154      -0.4258\n",
      "QWK:                  0.9297      0.7411      -0.1886\n",
      "\\nüß™ TEST SET PERFORMANCE:\n",
      "                      Original    Pruned      Difference\n",
      "Accuracy:             0.8338      0.5818      -0.2520\n",
      "F1-Score:             0.8369      0.4200      -0.4169\n",
      "QWK:                  0.9248      0.7469      -0.1779\n",
      "\\nüéØ PERFORMANCE RETENTION:\n",
      "Accuracy Retention: 69.8%\n",
      "F1-Score Retention: 50.2%\n",
      "QWK Retention: 80.8%\n",
      "\\nüí° PRUNING SUMMARY:\n",
      "‚Ä¢ Achieved 2.0x model compression with 1.0x inference speedup\n",
      "‚Ä¢ Performance degradation: 0.4169 F1-score points\n",
      "‚Ä¢ Efficiency Score: 0.0 (higher is better)\n",
      "\\n‚úÖ Pruning analysis complete! Results logged to W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>efficiency_score</td><td>‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñà‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/adjacent_accuracy</td><td>‚ñà‚ñÇ‚ñá‚ñÅ</td></tr><tr><td>eval/f1</td><td>‚ñà‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñÅ‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/mae</td><td>‚ñÅ‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/model_preparation_time</td><td>‚ñà‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/precision</td><td>‚ñà‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>‚ñà‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñà‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñà‚ñÅ‚ñÜ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÖ‚ñÖ‚ñà‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÖ‚ñÜ‚ñà‚ñÅ</td></tr><tr><td>final_accuracy_retention</td><td>‚ñÅ</td></tr><tr><td>final_compression_ratio</td><td>‚ñÅ</td></tr><tr><td>final_f1_retention</td><td>‚ñÅ</td></tr><tr><td>final_qwk_retention</td><td>‚ñÅ</td></tr><tr><td>final_speedup</td><td>‚ñÅ</td></tr><tr><td>original_eval_accuracy</td><td>‚ñÅ</td></tr><tr><td>original_eval_adjacent_accuracy</td><td>‚ñÅ</td></tr><tr><td>original_eval_f1</td><td>‚ñÅ</td></tr><tr><td>original_eval_loss</td><td>‚ñÅ</td></tr><tr><td>original_eval_mae</td><td>‚ñÅ</td></tr><tr><td>original_eval_model_preparation_time</td><td>‚ñÅ</td></tr><tr><td>original_eval_precision</td><td>‚ñÅ</td></tr><tr><td>original_eval_quadratic_weighted_kappa</td><td>‚ñÅ</td></tr><tr><td>original_eval_recall</td><td>‚ñÅ</td></tr><tr><td>original_eval_runtime</td><td>‚ñÅ</td></tr><tr><td>original_eval_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>original_eval_steps_per_second</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_accuracy</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_adjacent_accuracy</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_f1</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_loss</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_mae</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_model_preparation_time</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_precision</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_quadratic_weighted_kappa</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_recall</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_runtime</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>pruned_eval_steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>efficiency_score</td><td>0.04889</td></tr><tr><td>eval/accuracy</td><td>0.58178</td></tr><tr><td>eval/adjacent_accuracy</td><td>0.93721</td></tr><tr><td>eval/f1</td><td>0.42</td></tr><tr><td>eval/loss</td><td>1.60302</td></tr><tr><td>eval/mae</td><td>0.4854</td></tr><tr><td>eval/model_preparation_time</td><td>0.0022</td></tr><tr><td>eval/precision</td><td>0.37226</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>0.74686</td></tr><tr><td>eval/recall</td><td>0.49473</td></tr><tr><td>eval/runtime</td><td>287.6953</td></tr><tr><td>eval/samples_per_second</td><td>11.901</td></tr><tr><td>eval/steps_per_second</td><td>0.372</td></tr><tr><td>final_accuracy_retention</td><td>69.77233</td></tr><tr><td>final_compression_ratio</td><td>1.99801</td></tr><tr><td>final_f1_retention</td><td>50.18459</td></tr><tr><td>final_qwk_retention</td><td>80.76198</td></tr><tr><td>final_speedup</td><td>1.02008</td></tr><tr><td>original_eval_accuracy</td><td>0.83382</td></tr><tr><td>original_eval_adjacent_accuracy</td><td>0.97605</td></tr><tr><td>original_eval_f1</td><td>0.83691</td></tr><tr><td>original_eval_loss</td><td>0.59774</td></tr><tr><td>original_eval_mae</td><td>0.19305</td></tr><tr><td>original_eval_model_preparation_time</td><td>0.0021</td></tr><tr><td>original_eval_precision</td><td>0.83099</td></tr><tr><td>original_eval_quadratic_weighted_kappa</td><td>0.92477</td></tr><tr><td>original_eval_recall</td><td>0.84771</td></tr><tr><td>original_eval_runtime</td><td>228.3995</td></tr><tr><td>original_eval_samples_per_second</td><td>14.991</td></tr><tr><td>original_eval_steps_per_second</td><td>0.468</td></tr><tr><td>pruned_eval_accuracy</td><td>0.58178</td></tr><tr><td>pruned_eval_adjacent_accuracy</td><td>0.93721</td></tr><tr><td>pruned_eval_f1</td><td>0.42</td></tr><tr><td>pruned_eval_loss</td><td>1.60302</td></tr><tr><td>pruned_eval_mae</td><td>0.4854</td></tr><tr><td>pruned_eval_model_preparation_time</td><td>0.0022</td></tr><tr><td>pruned_eval_precision</td><td>0.37226</td></tr><tr><td>pruned_eval_quadratic_weighted_kappa</td><td>0.74686</td></tr><tr><td>pruned_eval_recall</td><td>0.49473</td></tr><tr><td>pruned_eval_runtime</td><td>287.6953</td></tr><tr><td>pruned_eval_samples_per_second</td><td>11.901</td></tr><tr><td>pruned_eval_steps_per_second</td><td>0.372</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">roberta-l1-unstructured-pruning</strong> at: <a href='https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning/runs/wxlhk8gn' target=\"_blank\">https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning/runs/wxlhk8gn</a><br> View project at: <a href='https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning' target=\"_blank\">https://wandb.ai/at-bay-data-science/covid-tweet-sentiment-pruning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250820_005921-wxlhk8gn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Comprehensive Results Summary\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PRUNING RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\\\nüìä MODEL COMPRESSION:\")\n",
    "print(f\"Original Model Size: {original_size:.2f} MB\")\n",
    "print(f\"Pruned Model Size (with zeros): {pruned_size:.2f} MB\")\n",
    "print(f\"Effective Pruned Model Size: {effective_pruned_size:.2f} MB\")\n",
    "print(f\"Compression Ratio: {compression_ratio:.2f}x\")\n",
    "print(f\"Size Reduction: {(1 - effective_pruned_size/original_size) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\\\n‚ö° INFERENCE SPEED:\")\n",
    "print(f\"Original Model: {original_time*1000:.2f} ms\")\n",
    "print(f\"Pruned Model: {pruned_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {speedup:.2f}x\")\n",
    "\n",
    "print(f\"\\\\nüéØ VALIDATION SET PERFORMANCE:\")\n",
    "print(f\"                      Original    Pruned      Difference\")\n",
    "print(f\"Accuracy:             {val_results_original['eval_accuracy']:.4f}      {val_results_pruned['eval_accuracy']:.4f}      {val_results_pruned['eval_accuracy'] - val_results_original['eval_accuracy']:+.4f}\")\n",
    "print(f\"F1-Score:             {val_results_original['eval_f1']:.4f}      {val_results_pruned['eval_f1']:.4f}      {val_results_pruned['eval_f1'] - val_results_original['eval_f1']:+.4f}\")\n",
    "print(f\"QWK:                  {val_results_original['eval_quadratic_weighted_kappa']:.4f}      {val_results_pruned['eval_quadratic_weighted_kappa']:.4f}      {val_results_pruned['eval_quadratic_weighted_kappa'] - val_results_original['eval_quadratic_weighted_kappa']:+.4f}\")\n",
    "\n",
    "print(f\"\\\\nüß™ TEST SET PERFORMANCE:\")\n",
    "print(f\"                      Original    Pruned      Difference\")\n",
    "print(f\"Accuracy:             {test_results_original['eval_accuracy']:.4f}      {test_results_pruned['eval_accuracy']:.4f}      {test_results_pruned['eval_accuracy'] - test_results_original['eval_accuracy']:+.4f}\")\n",
    "print(f\"F1-Score:             {test_results_original['eval_f1']:.4f}      {test_results_pruned['eval_f1']:.4f}      {test_results_pruned['eval_f1'] - test_results_original['eval_f1']:+.4f}\")\n",
    "print(f\"QWK:                  {test_results_original['eval_quadratic_weighted_kappa']:.4f}      {test_results_pruned['eval_quadratic_weighted_kappa']:.4f}      {test_results_pruned['eval_quadratic_weighted_kappa'] - test_results_original['eval_quadratic_weighted_kappa']:+.4f}\")\n",
    "\n",
    "# Calculate performance retention\n",
    "acc_retention = (test_results_pruned['eval_accuracy'] / test_results_original['eval_accuracy']) * 100\n",
    "f1_retention = (test_results_pruned['eval_f1'] / test_results_original['eval_f1']) * 100\n",
    "qwk_retention = (test_results_pruned['eval_quadratic_weighted_kappa'] / test_results_original['eval_quadratic_weighted_kappa']) * 100\n",
    "\n",
    "print(f\"\\\\nüéØ PERFORMANCE RETENTION:\")\n",
    "print(f\"Accuracy Retention: {acc_retention:.1f}%\")\n",
    "print(f\"F1-Score Retention: {f1_retention:.1f}%\")\n",
    "print(f\"QWK Retention: {qwk_retention:.1f}%\")\n",
    "\n",
    "print(f\"\\\\nüí° PRUNING SUMMARY:\")\n",
    "efficiency_score = (compression_ratio * speedup) / max(1, abs(test_results_pruned['eval_f1'] - test_results_original['eval_f1']) * 100)\n",
    "print(f\"‚Ä¢ Achieved {compression_ratio:.1f}x model compression with {speedup:.1f}x inference speedup\")\n",
    "print(f\"‚Ä¢ Performance degradation: {abs(test_results_pruned['eval_f1'] - test_results_original['eval_f1']):.4f} F1-score points\")\n",
    "print(f\"‚Ä¢ Efficiency Score: {efficiency_score:.1f} (higher is better)\")\n",
    "\n",
    "# Final wandb log with summary metrics\n",
    "wandb.log({\n",
    "    \"final_compression_ratio\": compression_ratio,\n",
    "    \"final_speedup\": speedup,\n",
    "    \"final_f1_retention\": f1_retention,\n",
    "    \"final_accuracy_retention\": acc_retention,\n",
    "    \"final_qwk_retention\": qwk_retention,\n",
    "    \"efficiency_score\": efficiency_score\n",
    "})\n",
    "\n",
    "print(f\"\\\\n‚úÖ Pruning analysis complete! Results logged to W&B.\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
