{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93692495-381f-444f-9f40-cedcd68e9b21",
   "metadata": {},
   "source": [
    "# Roberta - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dae3ed-e502-473c-8a12-a94ed4106d45",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c72c2-b2cc-4f37-aa1f-e459591d91c7",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52983362",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52983362",
    "outputId": "73af9f0e-dae4-40ce-f08a-d970216b509d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/395.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q optuna\n",
    "!pip install -q evaluate\n",
    "!pip install -q emoji==0.6.0\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8caf727",
   "metadata": {
    "id": "d8caf727"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import *\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_scheduler\n",
    "\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936d458-6734-4244-bfb1-c69b302909c6",
   "metadata": {},
   "source": [
    "### Device Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296be31e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "296be31e",
    "outputId": "b59cb9ef-0349-4346-f387-536b343895d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244d7e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5244d7e5",
    "outputId": "c089ea6b-563d-4dfe-9bbe-18bf74e5f23f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmayachn3\u001b[0m (\u001b[33mmayachn3-maya-bondar\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Weights & Biases Setup\n",
    "wandb.login(key=\"<wandb key>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c6cdd5",
   "metadata": {
    "id": "b6c6cdd5"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4b55a-3e84-4a0a-915d-c4cd47b4bb3d",
   "metadata": {},
   "source": [
    "## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a0a73c",
   "metadata": {
    "id": "d3a0a73c"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"OOT_train.csv\", encoding='latin-1')\n",
    "val = pd.read_csv(\"OOT_val.csv\", encoding='latin-1')\n",
    "test = pd.read_csv(\"OOT_test.csv\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38cadc1a",
   "metadata": {
    "id": "38cadc1a"
   },
   "outputs": [],
   "source": [
    "# train = train.head(100)\n",
    "# val = val.head(100)\n",
    "# test = test.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687deaa",
   "metadata": {
    "id": "4687deaa"
   },
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9966ecc5",
   "metadata": {
    "id": "9966ecc5"
   },
   "outputs": [],
   "source": [
    "#encoding the labels numerically from Sentiment\n",
    "ordinal_mapping = {\n",
    "    'Extremely Negative': 0,\n",
    "    'Negative': 1,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 3,\n",
    "    'Extremely Positive': 4\n",
    "}\n",
    "\n",
    "# map to ordinal labels\n",
    "train[\"label_id\"] = train[\"Sentiment\"].map(ordinal_mapping)\n",
    "val[\"label_id\"] = val[\"Sentiment\"].map(ordinal_mapping)\n",
    "test[\"label_id\"] = test[\"Sentiment\"].map(ordinal_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b78e181",
   "metadata": {
    "id": "2b78e181"
   },
   "outputs": [],
   "source": [
    "# Concat the relevant columns into one string with seperation.\n",
    "# for example: \"Tweet: my food stock is low | Location: Canada | Date: 2020-03-17 | URL: https://t.co/abcd\"\n",
    "\n",
    "# Function to build the input string from multiple columns\n",
    "def build_augmented_input(row):\n",
    "    parts = []\n",
    "\n",
    "    if pd.notna(row.get('clean_tweet')):\n",
    "        parts.append(f\"{row['clean_tweet']}\")\n",
    "\n",
    "    if pd.notna(row.get('Location_standardized')) and row['Location_standardized'].lower() != 'unknown':\n",
    "        parts.append(f\"{row['Location_standardized']}\")\n",
    "\n",
    "    if pd.notna(row.get('TweetAt')):\n",
    "        parts.append(f\"{row['TweetAt']}\")\n",
    "\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "# Apply to the DataFrames\n",
    "train['model_input'] = train.apply(build_augmented_input, axis=1)\n",
    "val['model_input'] = val.apply(build_augmented_input, axis=1)\n",
    "test['model_input'] = test.apply(build_augmented_input, axis=1)\n",
    "\n",
    "# Create  new DataFrames with only what's needed for modeling\n",
    "formatted_train = train[['model_input', 'label_id']].copy()\n",
    "formatted_val = val[['model_input', 'label_id']].copy()\n",
    "formatted_test = test[['model_input', 'label_id']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ad0dc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1ad0dc3",
    "outputId": "ec66677c-b84d-4eeb-c917-7148c15ada26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "label_id\n",
      "0     5175\n",
      "1     9230\n",
      "2     6784\n",
      "3    10140\n",
      "4     5845\n",
      "Name: count, dtype: int64\n",
      "Class 0: 5000 samples (undersampled)\n",
      "Class 1: 5000 samples (undersampled)\n",
      "Class 2: 5000 samples (undersampled)\n",
      "Class 3: 5000 samples (undersampled)\n",
      "Class 4: 5000 samples (undersampled)\n",
      "Balanced dataset: 25000 total samples\n",
      "New distribution:\n",
      "label_id\n",
      "0    5000\n",
      "1    5000\n",
      "2    5000\n",
      "3    5000\n",
      "4    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def balance_dataset(df, target_samples_per_class=5000):\n",
    "    \"\"\"Balance dataset by undersampling\"\"\"\n",
    "    balanced_dfs = []\n",
    "\n",
    "    print(\"Original class distribution:\")\n",
    "    print(df['label_id'].value_counts().sort_index())\n",
    "\n",
    "    for class_id in range(5):\n",
    "        class_data = df[df['label_id'] == class_id]\n",
    "\n",
    "        if len(class_data) > target_samples_per_class:\n",
    "            class_data = class_data.sample(n=target_samples_per_class, random_state=42)\n",
    "            print(f\"Class {class_id}: {len(class_data)} samples (undersampled)\")\n",
    "        else:\n",
    "            print(f\"Class {class_id}: {len(class_data)} samples (kept all)\")\n",
    "\n",
    "        balanced_dfs.append(class_data)\n",
    "\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "    print(f\"Balanced dataset: {len(balanced_df)} total samples\")\n",
    "    print(\"New distribution:\")\n",
    "    print(balanced_df['label_id'].value_counts().sort_index())\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "# Apply balancing to training data\n",
    "formatted_train = balance_dataset(formatted_train, target_samples_per_class=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13dad1",
   "metadata": {
    "id": "6c13dad1"
   },
   "source": [
    "## Tokenization and Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3b916d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235,
     "referenced_widgets": [
      "c17dd0277609411d94672d5d928d8c36",
      "866b4656d2614d5eac38e2f77bd31aa5",
      "d5acd8753d084191a201335153c3c885",
      "f82f72f6f1e9489ca0970782019c164d",
      "5f024dfb446c4bc29743b0dced15b6e2",
      "db331bea80d4490f9d31b408d3607333",
      "ed281cccf0d841b0b20698e4393552de",
      "7403a722bd6c4c7eb1116a60f1869fea",
      "03f794890d994ee69bc6df6b5b492b1a",
      "e9d726b492c941fc89a82194b31ee5bf",
      "00454005c106401db1769318286f9169",
      "31773b5049214b048b2c88356c0232cc",
      "b2e6b89978e2421a9b4c6e897b10fc6f",
      "011db3b3e5cb4013a5202bba057dc835",
      "e79a1fb2e2354f78abdd3f177106959f",
      "aff0faf084d1475db3856dfac9137af3",
      "ac6735ce42894521a0f92cf15cb9889c",
      "1e75c53bd40f49ef95f915c82565b7dc",
      "fb11b30150f1404083362c2bbc4f4e5e",
      "36f534bdf49f45b9b5914f7cdff7b3c1",
      "ff4d9b0d08004c0bbebbf9b7219caffb",
      "d08ab1ed029440c9ae851f6f9105d848",
      "0d2877ba0b5249c496cde99c510fa020",
      "048a5c8978964d3289578377d4fcddcd",
      "87b88b35894f4eb5b6b8674e8771fcab",
      "ad134be56c8a4a83987df79a0d14512b",
      "52708a194fac47928203f2a4cbef8532",
      "a3083e2000af402ab7327b13ad30d106",
      "e5322c88c7d041d1879e98dabf67d132",
      "5de3987322414ecaa140416ed53fc9a4",
      "88d25d74174b4ef1bb5524382df3073d",
      "c7e394de208e46b5bdc63405b8219d26",
      "efbd5a1ef7224d5c81aaf3e38f37966b"
     ]
    },
    "id": "cc3b916d",
    "outputId": "da8be19a-f715-4e6b-a1ff-05b9193864a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17dd0277609411d94672d5d928d8c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/565 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31773b5049214b048b2c88356c0232cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2877ba0b5249c496cde99c510fa020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_data(data, max_length=128):\n",
    "    return tokenizer(\n",
    "        data['model_input'].tolist(),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(formatted_train)\n",
    "val_encodings = tokenize_data(formatted_val)\n",
    "test_encodings = tokenize_data(formatted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0576ecae",
   "metadata": {
    "id": "0576ecae"
   },
   "outputs": [],
   "source": [
    "## define a PyTorch Dataset\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels  # Should be integers\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])  # For training\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert labels to integers if not already\n",
    "train_labels = formatted_train['label_id'].tolist()\n",
    "val_labels = formatted_val['label_id'].tolist()\n",
    "test_labels = formatted_test['label_id'].tolist()\n",
    "\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset = TweetDataset(val_encodings, val_labels)\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3460f26b",
   "metadata": {
    "id": "3460f26b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# define mapping between label id and sentiment for later use and conveniency\n",
    "ordinal_label2id = ordinal_mapping\n",
    "ordinal_id2label = {v: k for k, v in ordinal_mapping.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef394b35",
   "metadata": {
    "id": "ef394b35"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "xs3PTvj6lvs_",
   "metadata": {
    "id": "xs3PTvj6lvs_"
   },
   "outputs": [],
   "source": [
    "def save_training_checkpoint(model, optimizer, scheduler, epoch, loss, trial_params, filepath, trial_number, current_score, best_score, model_name=\"cardiffnlp/twitter-roberta-base\"):\n",
    "    \"\"\"Save complete training checkpoint and handle best model updates\"\"\"\n",
    "    global global_best_qwk, global_best_model_state\n",
    "\n",
    "    # Get the trial directory from filepath\n",
    "    trial_dir = os.path.dirname(filepath)\n",
    "\n",
    "    # Save trial checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'loss': loss,\n",
    "        'trial_params': trial_params,\n",
    "        'model_config': model.config.to_dict(),\n",
    "        'model_name': model_name,\n",
    "        'current_score': current_score,\n",
    "        'trial_number': trial_number,\n",
    "    }\n",
    "\n",
    "    # Save all trial files in the same directory\n",
    "    torch.save(checkpoint, filepath)\n",
    "    torch.save(model.state_dict(), os.path.join(trial_dir, 'model_roberta_weights.pt'))\n",
    "    torch.save(model, os.path.join(trial_dir, 'model_roberta.pt'))\n",
    "\n",
    "    print(f\"✅ Trial checkpoint saved: {filepath}\")\n",
    "    print(f\"✅ Model files saved in: {trial_dir}\")\n",
    "\n",
    "    # Update best model if needed\n",
    "    if current_score > best_score:\n",
    "        best_model_path = \"./best_roberta_model_so_far\"\n",
    "\n",
    "        # Ensure best model directory exists\n",
    "        os.makedirs(best_model_path, exist_ok=True)\n",
    "\n",
    "        # Save our custom .pt files in best model directory\n",
    "        best_checkpoint_path = os.path.join(best_model_path, 'best_checkpoint.ckpt')\n",
    "        best_weights_path = os.path.join(best_model_path, 'model_roberta_weights.pt')\n",
    "        best_model_file_path = os.path.join(best_model_path, 'model_roberta.pt')\n",
    "\n",
    "        torch.save(checkpoint, best_checkpoint_path)\n",
    "        torch.save(model.state_dict(), best_weights_path)\n",
    "        torch.save(model, best_model_file_path)\n",
    "\n",
    "        # Update global variables\n",
    "        global_best_qwk = current_score\n",
    "        global_best_model_state = model.state_dict().copy()\n",
    "\n",
    "        print(f\"🏆 New best model saved! Score: {current_score:.4f} (Trial {trial_number})\")\n",
    "        print(f\"🏆 Best model files saved in: {best_model_path}\")\n",
    "\n",
    "        return True  # Indicates new best model\n",
    "    else:\n",
    "        print(f\"📊 Trial {trial_number} score: {current_score:.4f} (Best: {best_score:.4f})\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mySRBv5iuu0N",
   "metadata": {
    "id": "mySRBv5iuu0N"
   },
   "outputs": [],
   "source": [
    "def calculate_per_class_metrics(y_true, y_pred, class_names):\n",
    "    \"\"\"Calculate detailed per-class metrics\"\"\"\n",
    "    # Get per-class precision, recall, f1\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=list(range(len(class_names))), zero_division=0\n",
    "    )\n",
    "\n",
    "    # Calculate per-class accuracy (correct predictions for each class)\n",
    "    per_class_accuracy = []\n",
    "    for class_id in range(len(class_names)):\n",
    "        class_mask = (y_true == class_id)\n",
    "        if class_mask.sum() > 0:  # If class exists in true labels\n",
    "            class_acc = ((y_pred == class_id) & (y_true == class_id)).sum() / class_mask.sum()\n",
    "        else:\n",
    "            class_acc = 0.0\n",
    "        per_class_accuracy.append(class_acc)\n",
    "\n",
    "    return precision, recall, f1, per_class_accuracy\n",
    "\n",
    "\n",
    "def print_epoch_summary_table(epoch, train_loss, train_accuracy, val_loss, val_accuracy,\n",
    "                             val_precision, val_recall, val_f1, val_mae, val_adjacent_accuracy,\n",
    "                             val_qwk, per_class_metrics, class_names, epoch_time):\n",
    "    \"\"\"Print a comprehensive epoch summary table\"\"\"\n",
    "\n",
    "    precision, recall, f1, per_class_accuracy = per_class_metrics\n",
    "\n",
    "    print(f\"\\nEPOCH {epoch} DETAILED SUMMARY\")\n",
    "    print(\"=\" * 120)\n",
    "\n",
    "    # Overall metrics\n",
    "    print(f\"Epoch Time: {epoch_time:.1f}s\")\n",
    "    print(f\"Training   - Loss: {train_loss:.6f} | Accuracy: {train_accuracy:.6f}\")\n",
    "    print(f\"Validation - Loss: {val_loss:.6f} | Accuracy: {val_accuracy:.6f}\")\n",
    "    print(f\"Overall    - F1: {val_f1:.6f} | Precision: {val_precision:.6f} | Recall: {val_recall:.6f}\")\n",
    "    print(f\"Metrics    - MAE: {val_mae:.6f} | Adjacent Acc: {val_adjacent_accuracy:.6f} | QWK: {val_qwk:.6f}\")\n",
    "\n",
    "    print(\"\\n\" + \"─\" * 120)\n",
    "    print(\"PER-CLASS BREAKDOWN:\")\n",
    "    print(\"─\" * 120)\n",
    "\n",
    "    # Header\n",
    "    print(f\"{'Class':<20} {'F1':<10} {'Precision':<12} {'Recall':<10} {'Accuracy':<10}\")\n",
    "    print(\"─\" * 65)\n",
    "\n",
    "    # Per-class rows\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name:<20} {f1[i]:<10.6f} {precision[i]:<12.6f} {recall[i]:<10.6f} {per_class_accuracy[i]:<10.6f}\")\n",
    "\n",
    "    print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c7522-4db5-40b8-a135-2e72311aac4c",
   "metadata": {
    "id": "fbfcc570"
   },
   "source": [
    "### Training and Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05Pz-IxQJLEe",
   "metadata": {
    "id": "05Pz-IxQJLEe"
   },
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    param_size = 0\n",
    "    buffer_size = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "def get_model_parameters(model):\n",
    "    \"\"\"Get total number of parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "\n",
    "def measure_inference_time(model, sample_batch, device, num_runs=10):\n",
    "    \"\"\"Measure average inference time\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = model(sample_batch['input_ids'].to(device),\n",
    "                     attention_mask=sample_batch['attention_mask'].to(device))\n",
    "\n",
    "    # Measure inference time\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(sample_batch['input_ids'].to(device),\n",
    "                     attention_mask=sample_batch['attention_mask'].to(device))\n",
    "\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    end_time = time.time()\n",
    "\n",
    "    avg_inference_time = (end_time - start_time) / num_runs * 1000  # Convert to milliseconds\n",
    "    return avg_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9575d145",
   "metadata": {
    "id": "9575d145"
   },
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_val_qwk, best_val_qwk_epoch, current_val_qwk, current_val_qwk_epoch):\n",
    "    early_stop_flag = False\n",
    "    if current_val_qwk > best_val_qwk:\n",
    "        best_val_qwk = current_val_qwk\n",
    "        best_val_qwk_epoch = current_val_qwk_epoch\n",
    "    else:\n",
    "        if current_val_qwk_epoch - best_val_qwk_epoch > patience:\n",
    "            early_stop_flag = True\n",
    "    return best_val_qwk, best_val_qwk_epoch, early_stop_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f798b13",
   "metadata": {
    "id": "3f798b13"
   },
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, trial, scheduler=None, max_grad_norm=None, save_checkpoints=True):\n",
    "    # GPU Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    trial_number = trial.number if trial else 0\n",
    "    checkpoint_dir = None\n",
    "    if save_checkpoints:\n",
    "        checkpoint_dir = Path(f\"./checkpoints_roberta/trial_{trial_number}\")\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Calculate model analysis AFTER moving to device\n",
    "    model_size_mb = get_model_size(model)\n",
    "    total_params, trainable_params = get_model_parameters(model)\n",
    "\n",
    "    # Get a sample batch for inference timing\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    inference_time_ms = measure_inference_time(model, sample_batch, device)\n",
    "\n",
    "    best_val_qwk = 0.0\n",
    "    best_val_qwk_epoch = 0\n",
    "    early_stop_flag = False\n",
    "    best_model_state = None\n",
    "    best_checkpoint_path = None\n",
    "\n",
    "    # Print training configuration with model info\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"   • Device: {device}\")\n",
    "    print(f\"   • Model Size: {model_size_mb:.2f} MB\")\n",
    "    print(f\"   • Total Parameters: {total_params:,}\")\n",
    "    print(f\"   • Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"   • Avg Inference Time: {inference_time_ms:.2f} ms\")\n",
    "    print(f\"   • Total Epochs: {epochs}\")\n",
    "    print(f\"   • Train Batches: {len(train_loader)}\")\n",
    "    print(f\"   • Validation Batches: {len(val_loader)}\")\n",
    "    print(f\"   • Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    print(f\"   • Batch Size: {train_loader.batch_size}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Continue with the rest of training loop\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training Phase\n",
    "        print(f\"\\nEPOCH {epoch}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Training loop with speed optimizations\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "        correct_train_predictions = 0\n",
    "\n",
    "        # Initialize mixed precision scaler\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Training\",\n",
    "                         bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}',\n",
    "                         ncols=100)\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "\n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            # Mixed precision backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if max_grad_norm is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Accumulate metrics (less frequently for speed)\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(train_loader) - 1:\n",
    "                batch_size = input_ids.size(0)\n",
    "                train_loss += loss.item() * batch_size\n",
    "                total_train_samples += batch_size\n",
    "                with torch.no_grad():\n",
    "                    correct_train_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                current_train_loss = train_loss / total_train_samples\n",
    "                current_train_acc = correct_train_predictions / total_train_samples\n",
    "\n",
    "                train_pbar.set_postfix({\n",
    "                    'Loss': f'{current_train_loss:.4f}',\n",
    "                    'Acc': f'{current_train_acc:.3f}',\n",
    "                    'LR': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "                })\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "        ###  Validation loop  ###\n",
    "        print(f\"\\n📋 Validation Phase...\")\n",
    "\n",
    "        # Faster validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Validation\",\n",
    "                       bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}',\n",
    "                       ncols=100)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "                labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "                # Use mixed precision for validation too\n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                    logits = outputs.logits\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "                batch_size = input_ids.size(0)\n",
    "                val_loss += loss.item() * batch_size\n",
    "                total_val_samples += batch_size\n",
    "                correct_val_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "                current_val_loss = val_loss / total_val_samples\n",
    "                current_val_acc = correct_val_predictions / total_val_samples\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{current_val_loss:.4f}',\n",
    "                    'Acc': f'{current_val_acc:.3f}'\n",
    "                })\n",
    "\n",
    "        # calculate metrics\n",
    "        all_val_preds = np.array(all_val_preds)\n",
    "        all_val_labels = np.array(all_val_labels)\n",
    "        val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "        val_precision = precision_score(all_val_labels, all_val_preds, average='macro') #### change macro\n",
    "        val_recall = recall_score(all_val_labels, all_val_preds, average='macro')#### change macro\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')#### change macro\n",
    "        val_mae = np.mean(np.abs(all_val_preds - all_val_labels))\n",
    "        val_adjacent_accuracy = np.sum(np.abs(all_val_preds - all_val_labels) <= 1) / len(all_val_labels)\n",
    "        val_qwk = cohen_kappa_score(all_val_labels, all_val_preds, weights='quadratic')\n",
    "\n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        # Calculate per-class metrics\n",
    "        class_names = list(ordinal_id2label.values())\n",
    "        per_class_metrics = calculate_per_class_metrics(all_val_labels, all_val_preds, class_names)\n",
    "\n",
    "        # Print comprehensive epoch summary\n",
    "        print_epoch_summary_table(\n",
    "            epoch, train_loss, train_accuracy, val_loss, val_accuracy,\n",
    "            val_precision, val_recall, val_f1, val_mae, val_adjacent_accuracy,\n",
    "            val_qwk, per_class_metrics, class_names, epoch_time\n",
    "        )\n",
    "\n",
    "        # Check for early stopping\n",
    "        patience = 2\n",
    "        best_val_qwk, best_val_qwk_epoch, early_stop_flag = early_stop_check(patience, best_val_qwk, best_val_qwk_epoch, val_qwk, epoch)\n",
    "\n",
    "        # Save the best model under the best_model_state parameter\n",
    "        if val_qwk == best_val_qwk:\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"New best QWK: {val_qwk:.4f} (saved model)\")\n",
    "        else:\n",
    "            print(f\"QWK: {val_qwk:.4f} (best: {best_val_qwk:.4f} at epoch {best_val_qwk_epoch})\")\n",
    "\n",
    "        detailed_metrics = {\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,  # Usually not computed during training\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Model Size (MB)\": model_size_mb,\n",
    "            \"Total Parameters\": total_params,\n",
    "            \"Trainable Parameters\": trainable_params,\n",
    "            \"Inference Time (ms)\": inference_time_ms,\n",
    "            \"Parameters per MB\": total_params / model_size_mb if model_size_mb > 0 else 0,\n",
    "            \"Validation F1\": val_f1,\n",
    "            \"Validation MAE\": val_mae,\n",
    "            \"Validation Adjacent Accuracy\": val_adjacent_accuracy,\n",
    "            \"Validation QWK\": val_qwk,\n",
    "            \"Learning_Rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"Epoch_Time\": epoch_time  # Added epoch time to wandb logging\n",
    "        }\n",
    "\n",
    "        # Log metrics to Weights & Biases\n",
    "        wandb.log(detailed_metrics)\n",
    "\n",
    "        patience = 2\n",
    "        best_val_qwk, best_val_qwk_epoch, early_stop_flag = early_stop_check(\n",
    "            patience, best_val_qwk, best_val_qwk_epoch, val_qwk, epoch\n",
    "        )\n",
    "\n",
    "        if val_qwk == best_val_qwk:\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"🌟 New best QWK: {val_qwk:.4f} (saved model)\")\n",
    "        else:\n",
    "            print(f\"📉 QWK: {val_qwk:.4f} (best: {best_val_qwk:.4f} at epoch {best_val_qwk_epoch})\")\n",
    "\n",
    "        if early_stop_flag:  # Checks whether the early stopping condition has been met, as indicated by the early_stop_flag\n",
    "            print(f\"\\n⏹️  EARLY STOPPING triggered at epoch {epoch}\")\n",
    "            print(f\"    No improvement in QWK for {patience} epochs\")\n",
    "            break# Exits the training loop immediately if the early stopping condition is satisfied\n",
    "\n",
    "    # Training completion summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Best QWK: {best_val_qwk:.4f} (Epoch {best_val_qwk_epoch})\")\n",
    "    print(f\"Total Epochs: {epoch}\")\n",
    "    print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "    print(f\"Parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "    print(f\"Inference Time: {inference_time_ms:.2f} ms per sample\")\n",
    "    print(f\"Efficiency: {total_params/1000000:.1f}M params, {inference_time_ms:.1f}ms\")\n",
    "    if best_checkpoint_path:\n",
    "        print(f\"Best Checkpoint: {best_checkpoint_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return best_val_qwk, best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37d2126c",
   "metadata": {
    "id": "37d2126c"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    global global_best_qwk, global_best_model_state\n",
    "\n",
    "    ######## new HP and wider range\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 3e-5, 5e-4, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.05, 0.15)\n",
    "    epochs = trial.suggest_int(\"epochs\", 10, 15)\n",
    "    # Advanced optimization parameters\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.15)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.05, 0.15)\n",
    "    # Model architecture parameters\n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0.3, 0.4)\n",
    "    hidden_dropout = trial.suggest_float(\"hidden_dropout\", 0.3, 0.4)\n",
    "\n",
    "    # Save trial configuration\n",
    "    trial_config = {\n",
    "        'trial_number': trial.number,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'label_smoothing': label_smoothing,\n",
    "        'epochs': epochs,\n",
    "        'warmup_ratio': warmup_ratio,\n",
    "        'weight_decay': weight_decay,\n",
    "        'attention_dropout': attention_dropout,\n",
    "        'hidden_dropout': hidden_dropout,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,  # Parallel data loading\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        persistent_workers=True  # Keep workers alive\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=5,\n",
    "        id2label=ordinal_id2label,\n",
    "        label2id=ordinal_label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    model_size_mb = get_model_size(model)\n",
    "    total_params, trainable_params = get_model_parameters(model)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # dropout configutation:\n",
    "    model.config.hidden_dropout_prob = hidden_dropout\n",
    "    model.config.attention_probs_dropout_prob = attention_dropout\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999),  # Default values, but explicit\n",
    "        eps=1e-8,            # Better numerical stability\n",
    "        amsgrad=False        # Standard setting\n",
    "    )\n",
    "\n",
    "    scheduler = None\n",
    "    if warmup_ratio > 0:\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "        scheduler = get_scheduler(\n",
    "            \"cosine\",  # Linear warmup then decay\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "    wandb.init(project=\"sentiment-full-roberta\",\n",
    "                group=model_name.split('/')[-1],\n",
    "                config={\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"weight_decay\": weight_decay,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"label_smoothing\": label_smoothing,\n",
    "                    \"warmup_ratio\": warmup_ratio,\n",
    "                    \"attention_dropout\": attention_dropout,\n",
    "                    \"hidden_dropout\": hidden_dropout,\n",
    "                    \"epochs\": epochs,\n",
    "                    \"model\": model_name},\n",
    "                name=f\"{model_name.split('/')[-1]}_trial_{trial.number}\")\n",
    "\n",
    "    best_val_qwk, best_model_state = train_model_with_hyperparams(\n",
    "        model, train_loader, val_loader, optimizer, criterion, epochs, trial=trial, scheduler=scheduler, max_grad_norm = max_grad_norm, save_checkpoints=True\n",
    "    )\n",
    "\n",
    "    trial_dir = f\"./checkpoints_roberta/trial_{trial.number}\"\n",
    "    os.makedirs(trial_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(trial_dir, f'final_epoch_{epochs}.ckpt')\n",
    "\n",
    "    # Prepare trial parameters\n",
    "    trial_params = {\n",
    "        'trial_number': trial.number,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'label_smoothing': label_smoothing,\n",
    "        'epochs': epochs,\n",
    "        'warmup_ratio': warmup_ratio,\n",
    "        'weight_decay': weight_decay,\n",
    "        'attention_dropout': attention_dropout,\n",
    "        'hidden_dropout': hidden_dropout,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "\n",
    "    # Save checkpoint and update best model if needed\n",
    "    is_new_best = save_training_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        epoch=epochs,\n",
    "        loss=0.0,  # track final loss\n",
    "        trial_params=trial_params,\n",
    "        filepath=checkpoint_path,\n",
    "        trial_number=trial.number,\n",
    "        current_score=best_val_qwk,\n",
    "        best_score=global_best_qwk,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    # Update global variables if new best\n",
    "    if is_new_best:\n",
    "        global_best_qwk = best_val_qwk\n",
    "        global_best_model_state = best_model_state.copy()\n",
    "\n",
    "    wandb.finish()\n",
    "    return best_val_qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00463d80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2017ae54d2e7498fb88b17e8f4cda26a",
      "20f02aae3cdd469ba7473fc747596080",
      "37cadb113e2d44a7ae91bdaf0c221721",
      "7f243a55281648d88f60755db1a2cd3a",
      "290119f0f97f4208bed233eccd9ceed9",
      "bdf64b1aeb444fc68e6f2cdf6d6a7694",
      "7e0656e70b5a40fcb901c07ea31e630a",
      "9795a32d4ec44407a8fc1f9da3bbfec4",
      "8292b8a468264145af673d554715b0ff",
      "aced22573e8a4e5b8e183db46077ccd5",
      "3ef0b4dd65874af5aba7cfaca1a38b94",
      "cff436214ce34ad581af2a1ba0e51986",
      "9a64b2549b7e4ebbab9144e39579ab45",
      "9c3b34c425c244cb9529da29a1e8cbcf",
      "7daeada907644944a32f7abda1ca1294",
      "197b4da506aa47db9988d891d3179ad6",
      "5458de6e8c7f4a80b0de294e277c4c1c",
      "ea4a2856f5794a82b4715594b4ff4fb2",
      "aa22225a8eb7475d83dd64b16cec9e6a",
      "9ec59c5fbf124d1baa0934733a2e5410",
      "77b8b95191d0448b8aad3516fb4df48c",
      "02a9e4626a0a469aae2fcfc553bb03ab"
     ]
    },
    "id": "00463d80",
    "outputId": "2032bcfd-01f6-4b55-d65d-505b06c33533"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:50:56,906] A new study created in memory with name: no-name-ae1bbb5d-b50b-4b9f-9f4e-d38fe9bb9d2b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized global_best_qwk: 0.0\n",
      "Initialized global_best_model_state: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2017ae54d2e7498fb88b17e8f4cda26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff436214ce34ad581af2a1ba0e51986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_145112-cp3paoml</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/cp3paoml' target=\"_blank\">twitter-roberta-base_trial_0</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/cp3paoml' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/cp3paoml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training Configuration:\n",
      "   • Device: cuda\n",
      "   • Model Size: 475.51 MB\n",
      "   • Total Parameters: 124,649,477\n",
      "   • Trainable Parameters: 124,649,477\n",
      "   • Avg Inference Time: 93.35 ms\n",
      "   • Total Epochs: 13\n",
      "   • Train Batches: 1563\n",
      "   • Validation Batches: 273\n",
      "   • Learning Rate: 0.00e+00\n",
      "   • Batch Size: 16\n",
      "============================================================\n",
      "\n",
      "EPOCH 1/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:33<00:00,  7.32it/s, Loss=1.1516, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 31.86it/s, Loss=0.9994, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 222.2s\n",
      "Training   - Loss: 1.151573 | Accuracy: 0.559921\n",
      "Validation - Loss: 0.999417 | Accuracy: 0.711728\n",
      "Overall    - F1: 0.720465 | Precision: 0.723388 | Recall: 0.732338\n",
      "Metrics    - MAE: 0.368832 | Adjacent Acc: 0.928850 | QWK: 0.831307\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.751799   0.669872     0.856557   0.856557  \n",
      "Negative             0.655772   0.633401     0.679781   0.679781  \n",
      "Neutral              0.777261   0.718929     0.845894   0.845894  \n",
      "Positive             0.655518   0.701880     0.614902   0.614902  \n",
      "Extremely Positive   0.761974   0.892857     0.664557   0.664557  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8313 (saved model)\n",
      "🌟 New best QWK: 0.8313 (saved model)\n",
      "\n",
      "EPOCH 2/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:28<00:00,  7.51it/s, Loss=0.8876, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.90it/s, Loss=0.8149, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 216.4s\n",
      "Training   - Loss: 0.887619 | Accuracy: 0.741270\n",
      "Validation - Loss: 0.814872 | Accuracy: 0.781272\n",
      "Overall    - F1: 0.781797 | Precision: 0.784448 | Recall: 0.783633\n",
      "Metrics    - MAE: 0.273583 | Adjacent Acc: 0.949736 | QWK: 0.880401\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.757974   0.698962     0.827869   0.827869  \n",
      "Negative             0.703247   0.721010     0.686339   0.686339  \n",
      "Neutral              0.844058   0.870813     0.818898   0.818898  \n",
      "Positive             0.776772   0.748003     0.807843   0.807843  \n",
      "Extremely Positive   0.826936   0.883453     0.777215   0.777215  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8804 (saved model)\n",
      "🌟 New best QWK: 0.8804 (saved model)\n",
      "\n",
      "EPOCH 3/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.54it/s, Loss=0.7464, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 33.19it/s, Loss=0.9021, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 3 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.6s\n",
      "Training   - Loss: 0.746392 | Accuracy: 0.818651\n",
      "Validation - Loss: 0.902143 | Accuracy: 0.754648\n",
      "Overall    - F1: 0.757325 | Precision: 0.754505 | Recall: 0.787211\n",
      "Metrics    - MAE: 0.299289 | Adjacent Acc: 0.953408 | QWK: 0.882846\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.757806   0.644189     0.920082   0.920082  \n",
      "Negative             0.689492   0.749679     0.638251   0.638251  \n",
      "Neutral              0.868815   0.889282     0.849269   0.849269  \n",
      "Positive             0.696558   0.824223     0.603137   0.603137  \n",
      "Extremely Positive   0.773954   0.665150     0.925316   0.925316  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8828 (saved model)\n",
      "🌟 New best QWK: 0.8828 (saved model)\n",
      "\n",
      "EPOCH 4/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.53it/s, Loss=0.7008, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 33.00it/s, Loss=0.9013, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 4 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.9s\n",
      "Training   - Loss: 0.700827 | Accuracy: 0.841667\n",
      "Validation - Loss: 0.901338 | Accuracy: 0.736975\n",
      "Overall    - F1: 0.750294 | Precision: 0.755156 | Recall: 0.772738\n",
      "Metrics    - MAE: 0.314666 | Adjacent Acc: 0.953408 | QWK: 0.880626\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.809615   0.762681     0.862705   0.862705  \n",
      "Negative             0.752900   0.802225     0.709290   0.709290  \n",
      "Neutral              0.855972   0.892552     0.822272   0.822272  \n",
      "Positive             0.601746   0.726164     0.513725   0.513725  \n",
      "Extremely Positive   0.731235   0.592157     0.955696   0.955696  \n",
      "========================================================================================================================\n",
      "QWK: 0.8806 (best: 0.8828 at epoch 3)\n",
      "📉 QWK: 0.8806 (best: 0.8828 at epoch 3)\n",
      "\n",
      "EPOCH 5/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.53it/s, Loss=0.6419, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 31.29it/s, Loss=0.7777, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 5 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 216.2s\n",
      "Training   - Loss: 0.641911 | Accuracy: 0.871032\n",
      "Validation - Loss: 0.777737 | Accuracy: 0.822125\n",
      "Overall    - F1: 0.824111 | Precision: 0.824696 | Recall: 0.826834\n",
      "Metrics    - MAE: 0.221483 | Adjacent Acc: 0.959835 | QWK: 0.904092\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.820463   0.775547     0.870902   0.870902  \n",
      "Negative             0.753505   0.809285     0.704918   0.704918  \n",
      "Neutral              0.859954   0.885578     0.835771   0.835771  \n",
      "Positive             0.809506   0.779797     0.841569   0.841569  \n",
      "Extremely Positive   0.877127   0.873275     0.881013   0.881013  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9041 (saved model)\n",
      "🌟 New best QWK: 0.9041 (saved model)\n",
      "\n",
      "EPOCH 6/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.54it/s, Loss=0.5834, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.06it/s, Loss=0.8087, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 6 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.7s\n",
      "Training   - Loss: 0.583355 | Accuracy: 0.903175\n",
      "Validation - Loss: 0.808715 | Accuracy: 0.813174\n",
      "Overall    - F1: 0.817738 | Precision: 0.817153 | Recall: 0.821229\n",
      "Metrics    - MAE: 0.222171 | Adjacent Acc: 0.968097 | QWK: 0.908854\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.841237   0.846473     0.836066   0.836066  \n",
      "Negative             0.772780   0.818071     0.732240   0.732240  \n",
      "Neutral              0.834260   0.825083     0.843645   0.843645  \n",
      "Positive             0.789474   0.802920     0.776471   0.776471  \n",
      "Extremely Positive   0.850939   0.793217     0.917722   0.917722  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9089 (saved model)\n",
      "🌟 New best QWK: 0.9089 (saved model)\n",
      "\n",
      "EPOCH 7/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.53it/s, Loss=0.5367, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 33.49it/s, Loss=0.8971, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 7 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.8s\n",
      "Training   - Loss: 0.536686 | Accuracy: 0.921429\n",
      "Validation - Loss: 0.897082 | Accuracy: 0.809961\n",
      "Overall    - F1: 0.811011 | Precision: 0.803894 | Recall: 0.828539\n",
      "Metrics    - MAE: 0.225155 | Adjacent Acc: 0.970392 | QWK: 0.910313\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.811111   0.739865     0.897541   0.897541  \n",
      "Negative             0.756694   0.809465     0.710383   0.710383  \n",
      "Neutral              0.855234   0.846748     0.863892   0.863892  \n",
      "Positive             0.788780   0.860853     0.727843   0.727843  \n",
      "Extremely Positive   0.843237   0.762538     0.943038   0.943038  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9103 (saved model)\n",
      "🌟 New best QWK: 0.9103 (saved model)\n",
      "\n",
      "EPOCH 8/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.53it/s, Loss=0.4988, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 31.77it/s, Loss=0.8932, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 8 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 216.3s\n",
      "Training   - Loss: 0.498830 | Accuracy: 0.946429\n",
      "Validation - Loss: 0.893221 | Accuracy: 0.817994\n",
      "Overall    - F1: 0.819687 | Precision: 0.812500 | Recall: 0.833147\n",
      "Metrics    - MAE: 0.220565 | Adjacent Acc: 0.965114 | QWK: 0.909178\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.809050   0.724473     0.915984   0.915984  \n",
      "Negative             0.752294   0.743070     0.761749   0.761749  \n",
      "Neutral              0.856357   0.838362     0.875141   0.875141  \n",
      "Positive             0.809205   0.867265     0.758431   0.758431  \n",
      "Extremely Positive   0.871530   0.889328     0.854430   0.854430  \n",
      "========================================================================================================================\n",
      "QWK: 0.9092 (best: 0.9103 at epoch 7)\n",
      "📉 QWK: 0.9092 (best: 0.9103 at epoch 7)\n",
      "\n",
      "EPOCH 9/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.53it/s, Loss=0.4674, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.51it/s, Loss=0.8211, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 9 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.9s\n",
      "Training   - Loss: 0.467367 | Accuracy: 0.954762\n",
      "Validation - Loss: 0.821107 | Accuracy: 0.834060\n",
      "Overall    - F1: 0.836527 | Precision: 0.829963 | Recall: 0.848148\n",
      "Metrics    - MAE: 0.187514 | Adjacent Acc: 0.981180 | QWK: 0.928758\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.851958   0.797853     0.913934   0.913934  \n",
      "Negative             0.788262   0.832321     0.748634   0.748634  \n",
      "Neutral              0.838298   0.795156     0.886389   0.886389  \n",
      "Positive             0.826069   0.877425     0.780392   0.780392  \n",
      "Extremely Positive   0.878049   0.847059     0.911392   0.911392  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9288 (saved model)\n",
      "🌟 New best QWK: 0.9288 (saved model)\n",
      "\n",
      "EPOCH 10/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.54it/s, Loss=0.4218, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 33.27it/s, Loss=0.8427, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 10 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.5s\n",
      "Training   - Loss: 0.421834 | Accuracy: 0.978175\n",
      "Validation - Loss: 0.842729 | Accuracy: 0.839339\n",
      "Overall    - F1: 0.842037 | Precision: 0.838689 | Recall: 0.848289\n",
      "Metrics    - MAE: 0.183842 | Adjacent Acc: 0.979803 | QWK: 0.927511\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.855701   0.842942     0.868852   0.868852  \n",
      "Negative             0.798206   0.819333     0.778142   0.778142  \n",
      "Neutral              0.843437   0.793651     0.899888   0.899888  \n",
      "Positive             0.831609   0.880035     0.788235   0.788235  \n",
      "Extremely Positive   0.881231   0.857485     0.906329   0.906329  \n",
      "========================================================================================================================\n",
      "QWK: 0.9275 (best: 0.9288 at epoch 9)\n",
      "📉 QWK: 0.9275 (best: 0.9288 at epoch 9)\n",
      "\n",
      "EPOCH 11/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.55it/s, Loss=0.4045, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.15it/s, Loss=0.8952, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 11 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.5s\n",
      "Training   - Loss: 0.404525 | Accuracy: 0.984921\n",
      "Validation - Loss: 0.895249 | Accuracy: 0.830158\n",
      "Overall    - F1: 0.833657 | Precision: 0.831500 | Recall: 0.839292\n",
      "Metrics    - MAE: 0.193941 | Adjacent Acc: 0.979344 | QWK: 0.923443\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.853634   0.852761     0.854508   0.854508  \n",
      "Negative             0.793651   0.824499     0.765027   0.765027  \n",
      "Neutral              0.829294   0.779980     0.885264   0.885264  \n",
      "Positive             0.818520   0.865385     0.776471   0.776471  \n",
      "Extremely Positive   0.873188   0.834873     0.915190   0.915190  \n",
      "========================================================================================================================\n",
      "QWK: 0.9234 (best: 0.9288 at epoch 9)\n",
      "📉 QWK: 0.9234 (best: 0.9288 at epoch 9)\n",
      "\n",
      "EPOCH 12/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.54it/s, Loss=0.3886, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.04it/s, Loss=0.9118, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 12 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.7s\n",
      "Training   - Loss: 0.388634 | Accuracy: 0.990476\n",
      "Validation - Loss: 0.911800 | Accuracy: 0.826486\n",
      "Overall    - F1: 0.830921 | Precision: 0.826528 | Recall: 0.841081\n",
      "Metrics    - MAE: 0.195318 | Adjacent Acc: 0.980950 | QWK: 0.926118\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.858283   0.836576     0.881148   0.881148  \n",
      "Negative             0.796117   0.833732     0.761749   0.761749  \n",
      "Neutral              0.834479   0.787425     0.887514   0.887514  \n",
      "Positive             0.803209   0.870082     0.745882   0.745882  \n",
      "Extremely Positive   0.862515   0.804825     0.929114   0.929114  \n",
      "========================================================================================================================\n",
      "QWK: 0.9261 (best: 0.9288 at epoch 9)\n",
      "📉 QWK: 0.9261 (best: 0.9288 at epoch 9)\n",
      "\n",
      "⏹️  EARLY STOPPING triggered at epoch 12\n",
      "    No improvement in QWK for 2 epochs\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best QWK: 0.9288 (Epoch 9)\n",
      "Total Epochs: 12\n",
      "Model Size: 475.51 MB\n",
      "Parameters: 124,649,477 total, 124,649,477 trainable\n",
      "Inference Time: 93.35 ms per sample\n",
      "Efficiency: 124.6M params, 93.4ms\n",
      "============================================================\n",
      "✅ Trial checkpoint saved: ./checkpoints_roberta/trial_0/final_epoch_13.ckpt\n",
      "✅ Model files saved in: ./checkpoints_roberta/trial_0\n",
      "🏆 New best model saved! Score: 0.9288 (Trial 0)\n",
      "🏆 Best model files saved in: ./best_roberta_model_so_far\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>Epoch_Time</td><td>█▂▁▁▂▁▁▂▁▁▁▁</td></tr><tr><td>Inference Time (ms)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Learning_Rate</td><td>▇██▇▆▅▅▄▃▂▁▁</td></tr><tr><td>Model Size (MB)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters per MB</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Total Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▆▇▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▆▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>Trainable Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▅▃▂▇▇▆▇██▇▇</td></tr><tr><td>Validation Adjacent Accuracy</td><td>▁▄▄▄▅▆▇▆████</td></tr><tr><td>Validation F1</td><td>▁▅▃▃▇▇▆▇███▇</td></tr><tr><td>Validation Loss</td><td>█▂▅▅▁▂▅▅▂▃▅▅</td></tr><tr><td>Validation MAE</td><td>█▄▅▆▂▂▃▂▁▁▁▁</td></tr><tr><td>Validation Precision</td><td>▁▅▃▃▇▇▆▆▇██▇</td></tr><tr><td>Validation QWK</td><td>▁▅▅▅▆▇▇▇████</td></tr><tr><td>Validation Recall</td><td>▁▄▄▃▇▆▇▇██▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>12</td></tr><tr><td>Epoch_Time</td><td>215.74854</td></tr><tr><td>Inference Time (ms)</td><td>93.35043</td></tr><tr><td>Learning_Rate</td><td>0.0</td></tr><tr><td>Model Size (MB)</td><td>475.50786</td></tr><tr><td>Parameters per MB</td><td>262139.6762</td></tr><tr><td>Total Parameters</td><td>124649477</td></tr><tr><td>Train Accuracy</td><td>0.99048</td></tr><tr><td>Train Loss</td><td>0.38863</td></tr><tr><td>Trainable Parameters</td><td>124649477</td></tr><tr><td>Validation Accuracy</td><td>0.82649</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.98095</td></tr><tr><td>Validation F1</td><td>0.83092</td></tr><tr><td>Validation Loss</td><td>0.9118</td></tr><tr><td>Validation MAE</td><td>0.19532</td></tr><tr><td>Validation Precision</td><td>0.82653</td></tr><tr><td>Validation QWK</td><td>0.92612</td></tr><tr><td>Validation Recall</td><td>0.84108</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twitter-roberta-base_trial_0</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/cp3paoml' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/cp3paoml</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_145112-cp3paoml/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:35:10,010] Trial 0 finished with value: 0.9287578437259792 and parameters: {'learning_rate': 8.121966143467401e-05, 'batch_size': 16, 'label_smoothing': 0.08982725417139796, 'epochs': 13, 'warmup_ratio': 0.08897856362142124, 'weight_decay': 0.07002198795879708, 'attention_dropout': 0.3373100903793905, 'hidden_dropout': 0.3482683601095847}. Best is trial 0 with value: 0.9287578437259792.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_153511-nvdsbzw3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/nvdsbzw3' target=\"_blank\">twitter-roberta-base_trial_1</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/nvdsbzw3' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/nvdsbzw3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training Configuration:\n",
      "   • Device: cuda\n",
      "   • Model Size: 475.51 MB\n",
      "   • Total Parameters: 124,649,477\n",
      "   • Trainable Parameters: 124,649,477\n",
      "   • Avg Inference Time: 383.25 ms\n",
      "   • Total Epochs: 14\n",
      "   • Train Batches: 391\n",
      "   • Validation Batches: 69\n",
      "   • Learning Rate: 0.00e+00\n",
      "   • Batch Size: 64\n",
      "============================================================\n",
      "\n",
      "EPOCH 1/14\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:13<00:00,  2.93it/s, Loss=1.2512, Acc=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00,  9.99it/s, Loss=1.1452, Acc=0.6\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 140.3s\n",
      "Training   - Loss: 1.251158 | Accuracy: 0.536278\n",
      "Validation - Loss: 1.145232 | Accuracy: 0.617627\n",
      "Overall    - F1: 0.601160 | Precision: 0.677977 | Recall: 0.628011\n",
      "Metrics    - MAE: 0.489098 | Adjacent Acc: 0.915309 | QWK: 0.771057\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.660280   0.515535     0.918033   0.918033  \n",
      "Negative             0.528614   0.574359     0.489617   0.489617  \n",
      "Neutral              0.746946   0.773494     0.722160   0.722160  \n",
      "Positive             0.638167   0.565797     0.731765   0.731765  \n",
      "Extremely Positive   0.431796   0.960699     0.278481   0.278481  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.7711 (saved model)\n",
      "🌟 New best QWK: 0.7711 (saved model)\n",
      "\n",
      "EPOCH 2/14\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:11<00:00,  2.98it/s, Loss=1.4543, Acc=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.71it/s, Loss=1.6122, Acc=0.1\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 137.8s\n",
      "Training   - Loss: 1.454279 | Accuracy: 0.350946\n",
      "Validation - Loss: 1.612245 | Accuracy: 0.181317\n",
      "Overall    - F1: 0.061395 | Precision: 0.036263 | Recall: 0.200000\n",
      "Metrics    - MAE: 1.778747 | Adjacent Acc: 0.473950 | QWK: 0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.000000   0.000000     0.000000   0.000000  \n",
      "Negative             0.000000   0.000000     0.000000   0.000000  \n",
      "Neutral              0.000000   0.000000     0.000000   0.000000  \n",
      "Positive             0.000000   0.000000     0.000000   0.000000  \n",
      "Extremely Positive   0.306975   0.181317     1.000000   1.000000  \n",
      "========================================================================================================================\n",
      "QWK: 0.0000 (best: 0.7711 at epoch 1)\n",
      "📉 QWK: 0.0000 (best: 0.7711 at epoch 1)\n",
      "\n",
      "EPOCH 3/14\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:11<00:00,  2.98it/s, Loss=1.6143, Acc=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.70it/s, Loss=1.6162, Acc=0.2\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 3 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 137.6s\n",
      "Training   - Loss: 1.614252 | Accuracy: 0.194006\n",
      "Validation - Loss: 1.616172 | Accuracy: 0.210007\n",
      "Overall    - F1: 0.069423 | Precision: 0.042001 | Recall: 0.200000\n",
      "Metrics    - MAE: 1.445261 | Adjacent Acc: 0.526050 | QWK: 0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.000000   0.000000     0.000000   0.000000  \n",
      "Negative             0.347117   0.210007     1.000000   1.000000  \n",
      "Neutral              0.000000   0.000000     0.000000   0.000000  \n",
      "Positive             0.000000   0.000000     0.000000   0.000000  \n",
      "Extremely Positive   0.000000   0.000000     0.000000   0.000000  \n",
      "========================================================================================================================\n",
      "QWK: 0.0000 (best: 0.7711 at epoch 1)\n",
      "📉 QWK: 0.0000 (best: 0.7711 at epoch 1)\n",
      "\n",
      "EPOCH 4/14\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:10<00:00,  2.99it/s, Loss=1.6121, Acc=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.51it/s, Loss=1.6082, Acc=0.1\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 4 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 137.6s\n",
      "Training   - Loss: 1.612104 | Accuracy: 0.192429\n",
      "Validation - Loss: 1.608159 | Accuracy: 0.181317\n",
      "Overall    - F1: 0.061395 | Precision: 0.036263 | Recall: 0.200000\n",
      "Metrics    - MAE: 1.778747 | Adjacent Acc: 0.473950 | QWK: 0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.000000   0.000000     0.000000   0.000000  \n",
      "Negative             0.000000   0.000000     0.000000   0.000000  \n",
      "Neutral              0.000000   0.000000     0.000000   0.000000  \n",
      "Positive             0.000000   0.000000     0.000000   0.000000  \n",
      "Extremely Positive   0.306975   0.181317     1.000000   1.000000  \n",
      "========================================================================================================================\n",
      "QWK: 0.0000 (best: 0.7711 at epoch 1)\n",
      "📉 QWK: 0.0000 (best: 0.7711 at epoch 1)\n",
      "\n",
      "⏹️  EARLY STOPPING triggered at epoch 4\n",
      "    No improvement in QWK for 2 epochs\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best QWK: 0.7711 (Epoch 1)\n",
      "Total Epochs: 4\n",
      "Model Size: 475.51 MB\n",
      "Parameters: 124,649,477 total, 124,649,477 trainable\n",
      "Inference Time: 383.25 ms per sample\n",
      "Efficiency: 124.6M params, 383.3ms\n",
      "============================================================\n",
      "✅ Trial checkpoint saved: ./checkpoints_roberta/trial_1/final_epoch_14.ckpt\n",
      "✅ Model files saved in: ./checkpoints_roberta/trial_1\n",
      "📊 Trial 1 score: 0.7711 (Best: 0.9288)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▆█</td></tr><tr><td>Epoch_Time</td><td>█▂▁▁</td></tr><tr><td>Inference Time (ms)</td><td>▁▁▁▁</td></tr><tr><td>Learning_Rate</td><td>▁█▇▅</td></tr><tr><td>Model Size (MB)</td><td>▁▁▁▁</td></tr><tr><td>Parameters per MB</td><td>▁▁▁▁</td></tr><tr><td>Total Parameters</td><td>▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>█▄▁▁</td></tr><tr><td>Train Loss</td><td>▁▅██</td></tr><tr><td>Trainable Parameters</td><td>▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>█▁▁▁</td></tr><tr><td>Validation Adjacent Accuracy</td><td>█▁▂▁</td></tr><tr><td>Validation F1</td><td>█▁▁▁</td></tr><tr><td>Validation Loss</td><td>▁███</td></tr><tr><td>Validation MAE</td><td>▁█▆█</td></tr><tr><td>Validation Precision</td><td>█▁▁▁</td></tr><tr><td>Validation QWK</td><td>█▁▁▁</td></tr><tr><td>Validation Recall</td><td>█▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>4</td></tr><tr><td>Epoch_Time</td><td>137.57125</td></tr><tr><td>Inference Time (ms)</td><td>383.25491</td></tr><tr><td>Learning_Rate</td><td>0.00035</td></tr><tr><td>Model Size (MB)</td><td>475.50786</td></tr><tr><td>Parameters per MB</td><td>262139.6762</td></tr><tr><td>Total Parameters</td><td>124649477</td></tr><tr><td>Train Accuracy</td><td>0.19243</td></tr><tr><td>Train Loss</td><td>1.6121</td></tr><tr><td>Trainable Parameters</td><td>124649477</td></tr><tr><td>Validation Accuracy</td><td>0.18132</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.47395</td></tr><tr><td>Validation F1</td><td>0.06139</td></tr><tr><td>Validation Loss</td><td>1.60816</td></tr><tr><td>Validation MAE</td><td>1.77875</td></tr><tr><td>Validation Precision</td><td>0.03626</td></tr><tr><td>Validation QWK</td><td>0</td></tr><tr><td>Validation Recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twitter-roberta-base_trial_1</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/nvdsbzw3' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/nvdsbzw3</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_153511-nvdsbzw3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:45:04,095] Trial 1 finished with value: 0.7710569911885861 and parameters: {'learning_rate': 0.0003960522302093908, 'batch_size': 64, 'label_smoothing': 0.14345513969602255, 'epochs': 14, 'warmup_ratio': 0.09127842838516399, 'weight_decay': 0.11815407603100057, 'attention_dropout': 0.33674866155080546, 'hidden_dropout': 0.3447852113721086}. Best is trial 0 with value: 0.9287578437259792.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_154506-xia0z64d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/xia0z64d' target=\"_blank\">twitter-roberta-base_trial_2</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/xia0z64d' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/xia0z64d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training Configuration:\n",
      "   • Device: cuda\n",
      "   • Model Size: 475.51 MB\n",
      "   • Total Parameters: 124,649,477\n",
      "   • Trainable Parameters: 124,649,477\n",
      "   • Avg Inference Time: 101.99 ms\n",
      "   • Total Epochs: 15\n",
      "   • Train Batches: 1563\n",
      "   • Validation Batches: 273\n",
      "   • Learning Rate: 0.00e+00\n",
      "   • Batch Size: 16\n",
      "============================================================\n",
      "\n",
      "EPOCH 1/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.53it/s, Loss=1.1680, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 31.50it/s, Loss=1.0764, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 216.3s\n",
      "Training   - Loss: 1.168040 | Accuracy: 0.544048\n",
      "Validation - Loss: 1.076383 | Accuracy: 0.613036\n",
      "Overall    - F1: 0.608848 | Precision: 0.637203 | Recall: 0.675321\n",
      "Metrics    - MAE: 0.474409 | Adjacent Acc: 0.932063 | QWK: 0.828978\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.625000   0.465000     0.952869   0.952869  \n",
      "Negative             0.463261   0.643969     0.361749   0.361749  \n",
      "Neutral              0.806892   0.855164     0.763780   0.763780  \n",
      "Positive             0.464730   0.686064     0.351373   0.351373  \n",
      "Extremely Positive   0.684355   0.535817     0.946835   0.946835  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8290 (saved model)\n",
      "🌟 New best QWK: 0.8290 (saved model)\n",
      "\n",
      "EPOCH 2/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:27<00:00,  7.54it/s, Loss=0.8326, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 33.63it/s, Loss=0.7971, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.5s\n",
      "Training   - Loss: 0.832603 | Accuracy: 0.755556\n",
      "Validation - Loss: 0.797075 | Accuracy: 0.772091\n",
      "Overall    - F1: 0.776980 | Precision: 0.786599 | Recall: 0.780971\n",
      "Metrics    - MAE: 0.279091 | Adjacent Acc: 0.952490 | QWK: 0.883731\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.770787   0.853234     0.702869   0.702869  \n",
      "Negative             0.759641   0.735174     0.785792   0.785792  \n",
      "Neutral              0.863085   0.887173     0.840270   0.840270  \n",
      "Positive             0.698671   0.770321     0.639216   0.639216  \n",
      "Extremely Positive   0.792716   0.687094     0.936709   0.936709  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8837 (saved model)\n",
      "🌟 New best QWK: 0.8837 (saved model)\n",
      "\n",
      "EPOCH 3/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.57it/s, Loss=0.7573, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.39it/s, Loss=0.8826, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 3 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.0s\n",
      "Training   - Loss: 0.757253 | Accuracy: 0.791270\n",
      "Validation - Loss: 0.882619 | Accuracy: 0.755336\n",
      "Overall    - F1: 0.750147 | Precision: 0.750405 | Recall: 0.780023\n",
      "Metrics    - MAE: 0.295387 | Adjacent Acc: 0.955015 | QWK: 0.886028\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.710266   0.564692     0.956967   0.956967  \n",
      "Negative             0.603749   0.738924     0.510383   0.510383  \n",
      "Neutral              0.860252   0.876313     0.844769   0.844769  \n",
      "Positive             0.756757   0.791774     0.724706   0.724706  \n",
      "Extremely Positive   0.819712   0.780320     0.863291   0.863291  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8860 (saved model)\n",
      "🌟 New best QWK: 0.8860 (saved model)\n",
      "\n",
      "EPOCH 4/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.57it/s, Loss=0.6323, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.58it/s, Loss=0.7607, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 4 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 214.8s\n",
      "Training   - Loss: 0.632342 | Accuracy: 0.860714\n",
      "Validation - Loss: 0.760693 | Accuracy: 0.811109\n",
      "Overall    - F1: 0.815871 | Precision: 0.817263 | Recall: 0.821678\n",
      "Metrics    - MAE: 0.242598 | Adjacent Acc: 0.950654 | QWK: 0.893572\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.814070   0.798817     0.829918   0.829918  \n",
      "Negative             0.753337   0.687726     0.832787   0.832787  \n",
      "Neutral              0.860409   0.924968     0.804274   0.804274  \n",
      "Positive             0.787521   0.851413     0.732549   0.732549  \n",
      "Extremely Positive   0.864019   0.823394     0.908861   0.908861  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8936 (saved model)\n",
      "🌟 New best QWK: 0.8936 (saved model)\n",
      "\n",
      "EPOCH 5/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.57it/s, Loss=0.5792, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 33.71it/s, Loss=0.8239, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 5 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 214.6s\n",
      "Training   - Loss: 0.579200 | Accuracy: 0.883333\n",
      "Validation - Loss: 0.823871 | Accuracy: 0.792747\n",
      "Overall    - F1: 0.793156 | Precision: 0.787551 | Recall: 0.819246\n",
      "Metrics    - MAE: 0.238926 | Adjacent Acc: 0.972229 | QWK: 0.911384\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.788512   0.685325     0.928279   0.928279  \n",
      "Negative             0.737349   0.821477     0.668852   0.668852  \n",
      "Neutral              0.854803   0.830329     0.880765   0.880765  \n",
      "Positive             0.758256   0.864458     0.675294   0.675294  \n",
      "Extremely Positive   0.826859   0.736166     0.943038   0.943038  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9114 (saved model)\n",
      "🌟 New best QWK: 0.9114 (saved model)\n",
      "\n",
      "EPOCH 6/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.57it/s, Loss=0.5483, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.68it/s, Loss=0.8301, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 6 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 214.9s\n",
      "Training   - Loss: 0.548305 | Accuracy: 0.911508\n",
      "Validation - Loss: 0.830106 | Accuracy: 0.815928\n",
      "Overall    - F1: 0.820085 | Precision: 0.811336 | Recall: 0.839513\n",
      "Metrics    - MAE: 0.228598 | Adjacent Acc: 0.960294 | QWK: 0.904458\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.831647   0.754591     0.926230   0.926230  \n",
      "Negative             0.771930   0.751553     0.793443   0.793443  \n",
      "Neutral              0.859532   0.851934     0.867267   0.867267  \n",
      "Positive             0.777138   0.893075     0.687843   0.687843  \n",
      "Extremely Positive   0.860177   0.805525     0.922785   0.922785  \n",
      "========================================================================================================================\n",
      "QWK: 0.9045 (best: 0.9114 at epoch 5)\n",
      "📉 QWK: 0.9045 (best: 0.9114 at epoch 5)\n",
      "\n",
      "EPOCH 7/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.58it/s, Loss=0.5038, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 31.99it/s, Loss=0.8004, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 7 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 214.8s\n",
      "Training   - Loss: 0.503822 | Accuracy: 0.921429\n",
      "Validation - Loss: 0.800363 | Accuracy: 0.823961\n",
      "Overall    - F1: 0.826374 | Precision: 0.821652 | Recall: 0.835004\n",
      "Metrics    - MAE: 0.206794 | Adjacent Acc: 0.972458 | QWK: 0.918397\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.832370   0.785455     0.885246   0.885246  \n",
      "Negative             0.781662   0.821687     0.745355   0.745355  \n",
      "Neutral              0.857143   0.878394     0.836895   0.836895  \n",
      "Positive             0.806245   0.823385     0.789804   0.789804  \n",
      "Extremely Positive   0.854449   0.799338     0.917722   0.917722  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9184 (saved model)\n",
      "🌟 New best QWK: 0.9184 (saved model)\n",
      "\n",
      "EPOCH 8/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.57it/s, Loss=0.4748, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.44it/s, Loss=0.7711, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 8 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.0s\n",
      "Training   - Loss: 0.474811 | Accuracy: 0.938492\n",
      "Validation - Loss: 0.771089 | Accuracy: 0.839798\n",
      "Overall    - F1: 0.842593 | Precision: 0.848153 | Recall: 0.838451\n",
      "Metrics    - MAE: 0.192105 | Adjacent Acc: 0.971311 | QWK: 0.917917\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.850054   0.897494     0.807377   0.807377  \n",
      "Negative             0.805151   0.825488     0.785792   0.785792  \n",
      "Neutral              0.858412   0.847588     0.869516   0.869516  \n",
      "Positive             0.826440   0.814787     0.838431   0.838431  \n",
      "Extremely Positive   0.872908   0.855407     0.891139   0.891139  \n",
      "========================================================================================================================\n",
      "QWK: 0.9179 (best: 0.9184 at epoch 7)\n",
      "📉 QWK: 0.9179 (best: 0.9184 at epoch 7)\n",
      "\n",
      "EPOCH 9/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.55it/s, Loss=0.4477, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 32.66it/s, Loss=0.8175, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 9 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.3s\n",
      "Training   - Loss: 0.447740 | Accuracy: 0.950000\n",
      "Validation - Loss: 0.817488 | Accuracy: 0.828552\n",
      "Overall    - F1: 0.832229 | Precision: 0.824616 | Recall: 0.844626\n",
      "Metrics    - MAE: 0.200826 | Adjacent Acc: 0.973835 | QWK: 0.920106\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.847490   0.801095     0.899590   0.899590  \n",
      "Negative             0.796009   0.807649     0.784699   0.784699  \n",
      "Neutral              0.847696   0.817992     0.879640   0.879640  \n",
      "Positive             0.802883   0.873616     0.742745   0.742745  \n",
      "Extremely Positive   0.867066   0.822727     0.916456   0.916456  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9201 (saved model)\n",
      "🌟 New best QWK: 0.9201 (saved model)\n",
      "\n",
      "EPOCH 10/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.58it/s, Loss=0.4335, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 31.98it/s, Loss=0.8270, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 10 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 214.9s\n",
      "Training   - Loss: 0.433515 | Accuracy: 0.957143\n",
      "Validation - Loss: 0.826962 | Accuracy: 0.839569\n",
      "Overall    - F1: 0.842093 | Precision: 0.844042 | Recall: 0.845915\n",
      "Metrics    - MAE: 0.176268 | Adjacent Acc: 0.985770 | QWK: 0.934203\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.860582   0.842829     0.879098   0.879098  \n",
      "Negative             0.794765   0.872063     0.730055   0.730055  \n",
      "Neutral              0.826286   0.755121     0.912261   0.912261  \n",
      "Positive             0.842063   0.858891     0.825882   0.825882  \n",
      "Extremely Positive   0.886768   0.891304     0.882278   0.882278  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9342 (saved model)\n",
      "🌟 New best QWK: 0.9342 (saved model)\n",
      "\n",
      "EPOCH 11/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.57it/s, Loss=0.3806, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 33.57it/s, Loss=0.8211, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 11 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 214.7s\n",
      "Training   - Loss: 0.380561 | Accuracy: 0.977778\n",
      "Validation - Loss: 0.821077 | Accuracy: 0.838880\n",
      "Overall    - F1: 0.842738 | Precision: 0.842159 | Recall: 0.848295\n",
      "Metrics    - MAE: 0.180399 | Adjacent Acc: 0.983245 | QWK: 0.930620\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.864811   0.839768     0.891393   0.891393  \n",
      "Negative             0.804651   0.859627     0.756284   0.756284  \n",
      "Neutral              0.822869   0.753271     0.906637   0.906637  \n",
      "Positive             0.833741   0.867684     0.802353   0.802353  \n",
      "Extremely Positive   0.887619   0.890446     0.884810   0.884810  \n",
      "========================================================================================================================\n",
      "QWK: 0.9306 (best: 0.9342 at epoch 10)\n",
      "📉 QWK: 0.9306 (best: 0.9342 at epoch 10)\n",
      "\n",
      "EPOCH 12/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:25<00:00,  7.59it/s, Loss=0.3629, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 33.02it/s, Loss=0.8637, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 12 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 214.2s\n",
      "Training   - Loss: 0.362920 | Accuracy: 0.984127\n",
      "Validation - Loss: 0.863654 | Accuracy: 0.837962\n",
      "Overall    - F1: 0.839808 | Precision: 0.836516 | Recall: 0.847280\n",
      "Metrics    - MAE: 0.181547 | Adjacent Acc: 0.982557 | QWK: 0.930995\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.848837   0.805147     0.897541   0.897541  \n",
      "Negative             0.792584   0.843403     0.747541   0.747541  \n",
      "Neutral              0.831590   0.777126     0.894263   0.894263  \n",
      "Positive             0.838265   0.867450     0.810980   0.810980  \n",
      "Extremely Positive   0.887762   0.889454     0.886076   0.886076  \n",
      "========================================================================================================================\n",
      "QWK: 0.9310 (best: 0.9342 at epoch 10)\n",
      "📉 QWK: 0.9310 (best: 0.9342 at epoch 10)\n",
      "\n",
      "EPOCH 13/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/1563 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 1563/1563 [03:26<00:00,  7.56it/s, Loss=0.3541, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/273 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 273/273 [00:08<00:00, 31.49it/s, Loss=0.8677, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 13 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 215.5s\n",
      "Training   - Loss: 0.354122 | Accuracy: 0.988492\n",
      "Validation - Loss: 0.867700 | Accuracy: 0.837273\n",
      "Overall    - F1: 0.841095 | Precision: 0.839473 | Recall: 0.848153\n",
      "Metrics    - MAE: 0.181317 | Adjacent Acc: 0.984393 | QWK: 0.931056\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.868952   0.855159     0.883197   0.883197  \n",
      "Negative             0.799302   0.854478     0.750820   0.750820  \n",
      "Neutral              0.822497   0.759771     0.896513   0.896513  \n",
      "Positive             0.832159   0.882250     0.787451   0.787451  \n",
      "Extremely Positive   0.882567   0.845708     0.922785   0.922785  \n",
      "========================================================================================================================\n",
      "QWK: 0.9311 (best: 0.9342 at epoch 10)\n",
      "📉 QWK: 0.9311 (best: 0.9342 at epoch 10)\n",
      "\n",
      "⏹️  EARLY STOPPING triggered at epoch 13\n",
      "    No improvement in QWK for 2 epochs\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best QWK: 0.9342 (Epoch 10)\n",
      "Total Epochs: 13\n",
      "Model Size: 475.51 MB\n",
      "Parameters: 124,649,477 total, 124,649,477 trainable\n",
      "Inference Time: 101.99 ms per sample\n",
      "Efficiency: 124.6M params, 102.0ms\n",
      "============================================================\n",
      "✅ Trial checkpoint saved: ./checkpoints_roberta/trial_2/final_epoch_15.ckpt\n",
      "✅ Model files saved in: ./checkpoints_roberta/trial_2\n",
      "🏆 New best model saved! Score: 0.9342 (Trial 2)\n",
      "🏆 Best model files saved in: ./best_roberta_model_so_far\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>Epoch_Time</td><td>█▅▄▃▂▃▃▄▅▃▃▁▅</td></tr><tr><td>Inference Time (ms)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Learning_Rate</td><td>▅██▇▇▆▅▅▄▃▂▁▁</td></tr><tr><td>Model Size (MB)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters per MB</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Total Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▆▇▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▅▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>Trainable Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▆▅▇▇▇███████</td></tr><tr><td>Validation Adjacent Accuracy</td><td>▁▄▄▃▆▅▆▆▆████</td></tr><tr><td>Validation F1</td><td>▁▆▅▇▇▇███████</td></tr><tr><td>Validation Loss</td><td>█▂▄▁▂▃▂▁▂▂▂▃▃</td></tr><tr><td>Validation MAE</td><td>█▃▄▃▂▂▂▁▂▁▁▁▁</td></tr><tr><td>Validation Precision</td><td>▁▆▅▇▆▇▇█▇████</td></tr><tr><td>Validation QWK</td><td>▁▅▅▅▆▆▇▇▇████</td></tr><tr><td>Validation Recall</td><td>▁▅▅▇▇█▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>13</td></tr><tr><td>Epoch_Time</td><td>215.52803</td></tr><tr><td>Inference Time (ms)</td><td>101.99196</td></tr><tr><td>Learning_Rate</td><td>0.0</td></tr><tr><td>Model Size (MB)</td><td>475.50786</td></tr><tr><td>Parameters per MB</td><td>262139.6762</td></tr><tr><td>Total Parameters</td><td>124649477</td></tr><tr><td>Train Accuracy</td><td>0.98849</td></tr><tr><td>Train Loss</td><td>0.35412</td></tr><tr><td>Trainable Parameters</td><td>124649477</td></tr><tr><td>Validation Accuracy</td><td>0.83727</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.98439</td></tr><tr><td>Validation F1</td><td>0.8411</td></tr><tr><td>Validation Loss</td><td>0.8677</td></tr><tr><td>Validation MAE</td><td>0.18132</td></tr><tr><td>Validation Precision</td><td>0.83947</td></tr><tr><td>Validation QWK</td><td>0.93106</td></tr><tr><td>Validation Recall</td><td>0.84815</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twitter-roberta-base_trial_2</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/xia0z64d' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/xia0z64d</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_154506-xia0z64d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 16:32:09,094] Trial 2 finished with value: 0.9342031958692001 and parameters: {'learning_rate': 7.344263440645085e-05, 'batch_size': 16, 'label_smoothing': 0.07751264012187695, 'epochs': 15, 'warmup_ratio': 0.10608220117570766, 'weight_decay': 0.09364120174857993, 'attention_dropout': 0.3449861962830879, 'hidden_dropout': 0.36332499415611835}. Best is trial 2 with value: 0.9342031958692001.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_163210-938jyy85</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/938jyy85' target=\"_blank\">twitter-roberta-base_trial_3</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/938jyy85' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/938jyy85</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training Configuration:\n",
      "   • Device: cuda\n",
      "   • Model Size: 475.51 MB\n",
      "   • Total Parameters: 124,649,477\n",
      "   • Trainable Parameters: 124,649,477\n",
      "   • Avg Inference Time: 389.05 ms\n",
      "   • Total Epochs: 13\n",
      "   • Train Batches: 391\n",
      "   • Validation Batches: 69\n",
      "   • Learning Rate: 0.00e+00\n",
      "   • Batch Size: 64\n",
      "============================================================\n",
      "\n",
      "EPOCH 1/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:13<00:00,  2.93it/s, Loss=1.1838, Acc=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.31it/s, Loss=1.1061, Acc=0.6\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 140.1s\n",
      "Training   - Loss: 1.183783 | Accuracy: 0.582413\n",
      "Validation - Loss: 1.106078 | Accuracy: 0.654579\n",
      "Overall    - F1: 0.653255 | Precision: 0.656107 | Recall: 0.683093\n",
      "Metrics    - MAE: 0.453982 | Adjacent Acc: 0.912555 | QWK: 0.797777\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.672683   0.560876     0.840164   0.840164  \n",
      "Negative             0.491964   0.682171     0.384699   0.384699  \n",
      "Neutral              0.776906   0.774302     0.779528   0.779528  \n",
      "Positive             0.600567   0.620401     0.581961   0.581961  \n",
      "Extremely Positive   0.724157   0.642787     0.829114   0.829114  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.7978 (saved model)\n",
      "🌟 New best QWK: 0.7978 (saved model)\n",
      "\n",
      "EPOCH 2/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.96it/s, Loss=1.2443, Acc=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.59it/s, Loss=1.5887, Acc=0.2\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 138.7s\n",
      "Training   - Loss: 1.244301 | Accuracy: 0.535883\n",
      "Validation - Loss: 1.588690 | Accuracy: 0.292633\n",
      "Overall    - F1: 0.090554 | Precision: 0.058527 | Recall: 0.200000\n",
      "Metrics    - MAE: 1.141382 | Adjacent Acc: 0.677989 | QWK: 0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.000000   0.000000     0.000000   0.000000  \n",
      "Negative             0.000000   0.000000     0.000000   0.000000  \n",
      "Neutral              0.000000   0.000000     0.000000   0.000000  \n",
      "Positive             0.452770   0.292633     1.000000   1.000000  \n",
      "Extremely Positive   0.000000   0.000000     0.000000   0.000000  \n",
      "========================================================================================================================\n",
      "QWK: 0.0000 (best: 0.7978 at epoch 1)\n",
      "📉 QWK: 0.0000 (best: 0.7978 at epoch 1)\n",
      "\n",
      "EPOCH 3/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:11<00:00,  2.98it/s, Loss=1.6185, Acc=0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.50it/s, Loss=1.6017, Acc=0.2\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 3 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 137.7s\n",
      "Training   - Loss: 1.618545 | Accuracy: 0.199921\n",
      "Validation - Loss: 1.601720 | Accuracy: 0.204039\n",
      "Overall    - F1: 0.067785 | Precision: 0.040808 | Recall: 0.200000\n",
      "Metrics    - MAE: 1.089282 | Adjacent Acc: 0.706679 | QWK: 0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.000000   0.000000     0.000000   0.000000  \n",
      "Negative             0.000000   0.000000     0.000000   0.000000  \n",
      "Neutral              0.338925   0.204039     1.000000   1.000000  \n",
      "Positive             0.000000   0.000000     0.000000   0.000000  \n",
      "Extremely Positive   0.000000   0.000000     0.000000   0.000000  \n",
      "========================================================================================================================\n",
      "QWK: 0.0000 (best: 0.7978 at epoch 1)\n",
      "📉 QWK: 0.0000 (best: 0.7978 at epoch 1)\n",
      "\n",
      "EPOCH 4/13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:10<00:00,  3.00it/s, Loss=1.6099, Acc=0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.68it/s, Loss=1.6173, Acc=0.1\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 4 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 136.6s\n",
      "Training   - Loss: 1.609851 | Accuracy: 0.213722\n",
      "Validation - Loss: 1.617338 | Accuracy: 0.112004\n",
      "Overall    - F1: 0.040289 | Precision: 0.022401 | Recall: 0.200000\n",
      "Metrics    - MAE: 2.221253 | Adjacent Acc: 0.322011 | QWK: 0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.201445   0.112004     1.000000   1.000000  \n",
      "Negative             0.000000   0.000000     0.000000   0.000000  \n",
      "Neutral              0.000000   0.000000     0.000000   0.000000  \n",
      "Positive             0.000000   0.000000     0.000000   0.000000  \n",
      "Extremely Positive   0.000000   0.000000     0.000000   0.000000  \n",
      "========================================================================================================================\n",
      "QWK: 0.0000 (best: 0.7978 at epoch 1)\n",
      "📉 QWK: 0.0000 (best: 0.7978 at epoch 1)\n",
      "\n",
      "⏹️  EARLY STOPPING triggered at epoch 4\n",
      "    No improvement in QWK for 2 epochs\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best QWK: 0.7978 (Epoch 1)\n",
      "Total Epochs: 4\n",
      "Model Size: 475.51 MB\n",
      "Parameters: 124,649,477 total, 124,649,477 trainable\n",
      "Inference Time: 389.05 ms per sample\n",
      "Efficiency: 124.6M params, 389.1ms\n",
      "============================================================\n",
      "✅ Trial checkpoint saved: ./checkpoints_roberta/trial_3/final_epoch_13.ckpt\n",
      "✅ Model files saved in: ./checkpoints_roberta/trial_3\n",
      "📊 Trial 3 score: 0.7978 (Best: 0.9342)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▆█</td></tr><tr><td>Epoch_Time</td><td>█▅▃▁</td></tr><tr><td>Inference Time (ms)</td><td>▁▁▁▁</td></tr><tr><td>Learning_Rate</td><td>▁██▇</td></tr><tr><td>Model Size (MB)</td><td>▁▁▁▁</td></tr><tr><td>Parameters per MB</td><td>▁▁▁▁</td></tr><tr><td>Total Parameters</td><td>▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>█▇▁▁</td></tr><tr><td>Train Loss</td><td>▁▂██</td></tr><tr><td>Trainable Parameters</td><td>▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>█▃▂▁</td></tr><tr><td>Validation Adjacent Accuracy</td><td>█▅▆▁</td></tr><tr><td>Validation F1</td><td>█▂▁▁</td></tr><tr><td>Validation Loss</td><td>▁███</td></tr><tr><td>Validation MAE</td><td>▁▄▄█</td></tr><tr><td>Validation Precision</td><td>█▁▁▁</td></tr><tr><td>Validation QWK</td><td>█▁▁▁</td></tr><tr><td>Validation Recall</td><td>█▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>4</td></tr><tr><td>Epoch_Time</td><td>136.61208</td></tr><tr><td>Inference Time (ms)</td><td>389.05056</td></tr><tr><td>Learning_Rate</td><td>0.00036</td></tr><tr><td>Model Size (MB)</td><td>475.50786</td></tr><tr><td>Parameters per MB</td><td>262139.6762</td></tr><tr><td>Total Parameters</td><td>124649477</td></tr><tr><td>Train Accuracy</td><td>0.21372</td></tr><tr><td>Train Loss</td><td>1.60985</td></tr><tr><td>Trainable Parameters</td><td>124649477</td></tr><tr><td>Validation Accuracy</td><td>0.112</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.32201</td></tr><tr><td>Validation F1</td><td>0.04029</td></tr><tr><td>Validation Loss</td><td>1.61734</td></tr><tr><td>Validation MAE</td><td>2.22125</td></tr><tr><td>Validation Precision</td><td>0.0224</td></tr><tr><td>Validation QWK</td><td>0</td></tr><tr><td>Validation Recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twitter-roberta-base_trial_3</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/938jyy85' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/938jyy85</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_163210-938jyy85/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 16:41:49,444] Trial 3 finished with value: 0.7977773894333053 and parameters: {'learning_rate': 0.000392178906505487, 'batch_size': 64, 'label_smoothing': 0.129563078735166, 'epochs': 13, 'warmup_ratio': 0.143642606203714, 'weight_decay': 0.14736768977176035, 'attention_dropout': 0.33166550090146024, 'hidden_dropout': 0.37397707659785673}. Best is trial 2 with value: 0.9342031958692001.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_164150-zkkcrxmm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/zkkcrxmm' target=\"_blank\">twitter-roberta-base_trial_4</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/zkkcrxmm' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/zkkcrxmm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training Configuration:\n",
      "   • Device: cuda\n",
      "   • Model Size: 475.51 MB\n",
      "   • Total Parameters: 124,649,477\n",
      "   • Trainable Parameters: 124,649,477\n",
      "   • Avg Inference Time: 185.63 ms\n",
      "   • Total Epochs: 12\n",
      "   • Train Batches: 782\n",
      "   • Validation Batches: 137\n",
      "   • Learning Rate: 0.00e+00\n",
      "   • Batch Size: 32\n",
      "============================================================\n",
      "\n",
      "EPOCH 1/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:37<00:00,  4.96it/s, Loss=1.1310, Acc=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.40it/s, Loss=0.8414, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 164.7s\n",
      "Training   - Loss: 1.131004 | Accuracy: 0.557571\n",
      "Validation - Loss: 0.841359 | Accuracy: 0.733303\n",
      "Overall    - F1: 0.739027 | Precision: 0.740575 | Recall: 0.743439\n",
      "Metrics    - MAE: 0.334634 | Adjacent Acc: 0.938031 | QWK: 0.852292\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.752089   0.687606     0.829918   0.829918  \n",
      "Negative             0.661564   0.661926     0.661202   0.661202  \n",
      "Neutral              0.806685   0.799117     0.814398   0.814398  \n",
      "Positive             0.704924   0.697086     0.712941   0.712941  \n",
      "Extremely Positive   0.769874   0.857143     0.698734   0.698734  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8523 (saved model)\n",
      "🌟 New best QWK: 0.8523 (saved model)\n",
      "\n",
      "EPOCH 2/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:37<00:00,  4.98it/s, Loss=0.8028, Acc=0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.15it/s, Loss=0.7971, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 164.4s\n",
      "Training   - Loss: 0.802824 | Accuracy: 0.755521\n",
      "Validation - Loss: 0.797124 | Accuracy: 0.777599\n",
      "Overall    - F1: 0.779333 | Precision: 0.771444 | Recall: 0.795587\n",
      "Metrics    - MAE: 0.279550 | Adjacent Acc: 0.949277 | QWK: 0.881167\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.767201   0.710297     0.834016   0.834016  \n",
      "Negative             0.707438   0.713333     0.701639   0.701639  \n",
      "Neutral              0.850755   0.845556     0.856018   0.856018  \n",
      "Positive             0.740998   0.829126     0.669804   0.669804  \n",
      "Extremely Positive   0.830275   0.758910     0.916456   0.916456  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8812 (saved model)\n",
      "🌟 New best QWK: 0.8812 (saved model)\n",
      "\n",
      "EPOCH 3/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:37<00:00,  4.98it/s, Loss=0.6888, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.24it/s, Loss=0.7385, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 3 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 164.3s\n",
      "Training   - Loss: 0.688834 | Accuracy: 0.812303\n",
      "Validation - Loss: 0.738477 | Accuracy: 0.803076\n",
      "Overall    - F1: 0.806976 | Precision: 0.804513 | Recall: 0.818762\n",
      "Metrics    - MAE: 0.247418 | Adjacent Acc: 0.953867 | QWK: 0.894928\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.822050   0.778388     0.870902   0.870902  \n",
      "Negative             0.744578   0.829530     0.675410   0.675410  \n",
      "Neutral              0.870748   0.877714     0.863892   0.863892  \n",
      "Positive             0.763799   0.791421     0.738039   0.738039  \n",
      "Extremely Positive   0.833705   0.745509     0.945570   0.945570  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8949 (saved model)\n",
      "🌟 New best QWK: 0.8949 (saved model)\n",
      "\n",
      "EPOCH 4/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  4.98it/s, Loss=0.6040, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:06<00:00, 19.61it/s, Loss=0.7581, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 4 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 164.0s\n",
      "Training   - Loss: 0.604024 | Accuracy: 0.856467\n",
      "Validation - Loss: 0.758145 | Accuracy: 0.806518\n",
      "Overall    - F1: 0.808365 | Precision: 0.799813 | Recall: 0.825632\n",
      "Metrics    - MAE: 0.238237 | Adjacent Acc: 0.960294 | QWK: 0.901999\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.797166   0.702028     0.922131   0.922131  \n",
      "Negative             0.741189   0.746948     0.735519   0.735519  \n",
      "Neutral              0.868208   0.892985     0.844769   0.844769  \n",
      "Positive             0.786375   0.847688     0.733333   0.733333  \n",
      "Extremely Positive   0.848886   0.809414     0.892405   0.892405  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9020 (saved model)\n",
      "🌟 New best QWK: 0.9020 (saved model)\n",
      "\n",
      "EPOCH 5/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  4.99it/s, Loss=0.5408, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:06<00:00, 19.70it/s, Loss=0.7961, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 5 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 163.8s\n",
      "Training   - Loss: 0.540762 | Accuracy: 0.884858\n",
      "Validation - Loss: 0.796070 | Accuracy: 0.801010\n",
      "Overall    - F1: 0.806327 | Precision: 0.805166 | Recall: 0.820685\n",
      "Metrics    - MAE: 0.247877 | Adjacent Acc: 0.956622 | QWK: 0.896727\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.832090   0.763699     0.913934   0.913934  \n",
      "Negative             0.743932   0.836289     0.669945   0.669945  \n",
      "Neutral              0.866745   0.910781     0.826772   0.826772  \n",
      "Positive             0.760484   0.782573     0.739608   0.739608  \n",
      "Extremely Positive   0.828383   0.732490     0.953165   0.953165  \n",
      "========================================================================================================================\n",
      "QWK: 0.8967 (best: 0.9020 at epoch 4)\n",
      "📉 QWK: 0.8967 (best: 0.9020 at epoch 4)\n",
      "\n",
      "EPOCH 6/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  4.99it/s, Loss=0.4513, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.28it/s, Loss=0.8596, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 6 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 163.9s\n",
      "Training   - Loss: 0.451276 | Accuracy: 0.925079\n",
      "Validation - Loss: 0.859619 | Accuracy: 0.802157\n",
      "Overall    - F1: 0.810806 | Precision: 0.810737 | Recall: 0.825593\n",
      "Metrics    - MAE: 0.238008 | Adjacent Acc: 0.964196 | QWK: 0.905370\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.849505   0.821839     0.879098   0.879098  \n",
      "Negative             0.797681   0.849383     0.751913   0.751913  \n",
      "Neutral              0.881087   0.887115     0.875141   0.875141  \n",
      "Positive             0.727431   0.814383     0.657255   0.657255  \n",
      "Extremely Positive   0.798324   0.680965     0.964557   0.964557  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9054 (saved model)\n",
      "🌟 New best QWK: 0.9054 (saved model)\n",
      "\n",
      "EPOCH 7/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  4.99it/s, Loss=0.4197, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.00it/s, Loss=0.7309, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 7 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 164.0s\n",
      "Training   - Loss: 0.419724 | Accuracy: 0.940063\n",
      "Validation - Loss: 0.730871 | Accuracy: 0.828552\n",
      "Overall    - F1: 0.830056 | Precision: 0.822459 | Recall: 0.843657\n",
      "Metrics    - MAE: 0.199679 | Adjacent Acc: 0.974524 | QWK: 0.922460\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.838235   0.760000     0.934426   0.934426  \n",
      "Negative             0.767347   0.822500     0.719126   0.719126  \n",
      "Neutral              0.839602   0.825898     0.853768   0.853768  \n",
      "Positive             0.823243   0.854132     0.794510   0.794510  \n",
      "Extremely Positive   0.881851   0.849765     0.916456   0.916456  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9225 (saved model)\n",
      "🌟 New best QWK: 0.9225 (saved model)\n",
      "\n",
      "EPOCH 8/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  5.00it/s, Loss=0.3729, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.55it/s, Loss=0.8449, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 8 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 163.5s\n",
      "Training   - Loss: 0.372914 | Accuracy: 0.957808\n",
      "Validation - Loss: 0.844856 | Accuracy: 0.818224\n",
      "Overall    - F1: 0.822983 | Precision: 0.819190 | Recall: 0.835207\n",
      "Metrics    - MAE: 0.210007 | Adjacent Acc: 0.974753 | QWK: 0.919333\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.857143   0.825427     0.891393   0.891393  \n",
      "Negative             0.781488   0.842172     0.728962   0.728962  \n",
      "Neutral              0.840834   0.821008     0.861642   0.861642  \n",
      "Positive             0.791107   0.850316     0.739608   0.739608  \n",
      "Extremely Positive   0.844345   0.757028     0.954430   0.954430  \n",
      "========================================================================================================================\n",
      "QWK: 0.9193 (best: 0.9225 at epoch 7)\n",
      "📉 QWK: 0.9193 (best: 0.9225 at epoch 7)\n",
      "\n",
      "EPOCH 9/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  5.00it/s, Loss=0.3401, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:06<00:00, 19.89it/s, Loss=0.7628, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 9 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 163.4s\n",
      "Training   - Loss: 0.340056 | Accuracy: 0.972397\n",
      "Validation - Loss: 0.762784 | Accuracy: 0.844159\n",
      "Overall    - F1: 0.847759 | Precision: 0.845966 | Recall: 0.851115\n",
      "Metrics    - MAE: 0.175580 | Adjacent Acc: 0.982557 | QWK: 0.932585\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.866803   0.866803     0.866803   0.866803  \n",
      "Negative             0.815022   0.836594     0.794536   0.794536  \n",
      "Neutral              0.835389   0.798156     0.876265   0.876265  \n",
      "Positive             0.835022   0.864094     0.807843   0.807843  \n",
      "Extremely Positive   0.886560   0.864183     0.910127   0.910127  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9326 (saved model)\n",
      "🌟 New best QWK: 0.9326 (saved model)\n",
      "\n",
      "EPOCH 10/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  5.01it/s, Loss=0.3326, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 18.92it/s, Loss=0.7961, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 10 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 163.3s\n",
      "Training   - Loss: 0.332566 | Accuracy: 0.980284\n",
      "Validation - Loss: 0.796053 | Accuracy: 0.848979\n",
      "Overall    - F1: 0.852582 | Precision: 0.851164 | Recall: 0.857674\n",
      "Metrics    - MAE: 0.170760 | Adjacent Acc: 0.982557 | QWK: 0.934151\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.878695   0.874239     0.883197   0.883197  \n",
      "Negative             0.811310   0.859413     0.768306   0.768306  \n",
      "Neutral              0.842437   0.790148     0.902137   0.902137  \n",
      "Positive             0.842793   0.879046     0.809412   0.809412  \n",
      "Extremely Positive   0.887675   0.852975     0.925316   0.925316  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9342 (saved model)\n",
      "🌟 New best QWK: 0.9342 (saved model)\n",
      "\n",
      "EPOCH 11/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  5.00it/s, Loss=0.3118, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.21it/s, Loss=0.8091, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 11 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 163.4s\n",
      "Training   - Loss: 0.311780 | Accuracy: 0.986199\n",
      "Validation - Loss: 0.809072 | Accuracy: 0.845995\n",
      "Overall    - F1: 0.850155 | Precision: 0.847934 | Recall: 0.856044\n",
      "Metrics    - MAE: 0.172366 | Adjacent Acc: 0.983934 | QWK: 0.934488\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.875380   0.865731     0.885246   0.885246  \n",
      "Negative             0.814562   0.849348     0.782514   0.782514  \n",
      "Neutral              0.835245   0.778426     0.901012   0.901012  \n",
      "Positive             0.836018   0.883072     0.793725   0.793725  \n",
      "Extremely Positive   0.889571   0.863095     0.917722   0.917722  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9345 (saved model)\n",
      "🌟 New best QWK: 0.9345 (saved model)\n",
      "\n",
      "EPOCH 12/12\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  5.00it/s, Loss=0.3086, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.49it/s, Loss=0.8219, Acc=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 12 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 163.6s\n",
      "Training   - Loss: 0.308559 | Accuracy: 0.987776\n",
      "Validation - Loss: 0.821868 | Accuracy: 0.841864\n",
      "Overall    - F1: 0.845994 | Precision: 0.843698 | Recall: 0.852088\n",
      "Metrics    - MAE: 0.176268 | Adjacent Acc: 0.984393 | QWK: 0.933486\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.873354   0.863727     0.883197   0.883197  \n",
      "Negative             0.809851   0.850782     0.772678   0.772678  \n",
      "Neutral              0.830624   0.777996     0.890889   0.890889  \n",
      "Positive             0.832646   0.877498     0.792157   0.792157  \n",
      "Extremely Positive   0.883495   0.848485     0.921519   0.921519  \n",
      "========================================================================================================================\n",
      "QWK: 0.9335 (best: 0.9345 at epoch 11)\n",
      "📉 QWK: 0.9335 (best: 0.9345 at epoch 11)\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best QWK: 0.9345 (Epoch 11)\n",
      "Total Epochs: 12\n",
      "Model Size: 475.51 MB\n",
      "Parameters: 124,649,477 total, 124,649,477 trainable\n",
      "Inference Time: 185.63 ms per sample\n",
      "Efficiency: 124.6M params, 185.6ms\n",
      "============================================================\n",
      "✅ Trial checkpoint saved: ./checkpoints_roberta/trial_4/final_epoch_12.ckpt\n",
      "✅ Model files saved in: ./checkpoints_roberta/trial_4\n",
      "🏆 New best model saved! Score: 0.9345 (Trial 4)\n",
      "🏆 Best model files saved in: ./best_roberta_model_so_far\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>Epoch_Time</td><td>█▆▆▄▃▄▄▂▁▁▂▂</td></tr><tr><td>Inference Time (ms)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Learning_Rate</td><td>▅██▇▆▅▄▃▂▂▁▁</td></tr><tr><td>Model Size (MB)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters per MB</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Total Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▆▇▇█████</td></tr><tr><td>Train Loss</td><td>█▅▄▄▃▂▂▂▁▁▁▁</td></tr><tr><td>Trainable Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▅▅▅▇▆████</td></tr><tr><td>Validation Adjacent Accuracy</td><td>▁▃▃▄▄▅▇▇████</td></tr><tr><td>Validation F1</td><td>▁▃▅▅▅▅▇▆████</td></tr><tr><td>Validation Loss</td><td>▇▅▁▂▅█▁▇▃▅▅▆</td></tr><tr><td>Validation MAE</td><td>█▆▄▄▄▄▂▃▁▁▁▁</td></tr><tr><td>Validation Precision</td><td>▁▃▅▅▅▅▆▆████</td></tr><tr><td>Validation QWK</td><td>▁▃▅▅▅▆▇▇████</td></tr><tr><td>Validation Recall</td><td>▁▄▆▆▆▆▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>12</td></tr><tr><td>Epoch_Time</td><td>163.56965</td></tr><tr><td>Inference Time (ms)</td><td>185.62725</td></tr><tr><td>Learning_Rate</td><td>0</td></tr><tr><td>Model Size (MB)</td><td>475.50786</td></tr><tr><td>Parameters per MB</td><td>262139.6762</td></tr><tr><td>Total Parameters</td><td>124649477</td></tr><tr><td>Train Accuracy</td><td>0.98778</td></tr><tr><td>Train Loss</td><td>0.30856</td></tr><tr><td>Trainable Parameters</td><td>124649477</td></tr><tr><td>Validation Accuracy</td><td>0.84186</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.98439</td></tr><tr><td>Validation F1</td><td>0.84599</td></tr><tr><td>Validation Loss</td><td>0.82187</td></tr><tr><td>Validation MAE</td><td>0.17627</td></tr><tr><td>Validation Precision</td><td>0.8437</td></tr><tr><td>Validation QWK</td><td>0.93349</td></tr><tr><td>Validation Recall</td><td>0.85209</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twitter-roberta-base_trial_4</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/zkkcrxmm' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/zkkcrxmm</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_164150-zkkcrxmm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 17:15:04,721] Trial 4 finished with value: 0.9344879741478308 and parameters: {'learning_rate': 0.0001011659992836504, 'batch_size': 32, 'label_smoothing': 0.06379175537830997, 'epochs': 12, 'warmup_ratio': 0.13249816729375002, 'weight_decay': 0.11338314667246435, 'attention_dropout': 0.3884054851219189, 'hidden_dropout': 0.3087398338921572}. Best is trial 4 with value: 0.9344879741478308.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_171506-f2dwjylr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/f2dwjylr' target=\"_blank\">twitter-roberta-base_trial_5</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/f2dwjylr' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/f2dwjylr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training Configuration:\n",
      "   • Device: cuda\n",
      "   • Model Size: 475.51 MB\n",
      "   • Total Parameters: 124,649,477\n",
      "   • Trainable Parameters: 124,649,477\n",
      "   • Avg Inference Time: 186.17 ms\n",
      "   • Total Epochs: 15\n",
      "   • Train Batches: 782\n",
      "   • Validation Batches: 137\n",
      "   • Learning Rate: 0.00e+00\n",
      "   • Batch Size: 32\n",
      "============================================================\n",
      "\n",
      "EPOCH 1/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:37<00:00,  4.96it/s, Loss=1.1478, Acc=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.38it/s, Loss=1.0799, Acc=0\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 164.7s\n",
      "Training   - Loss: 1.147835 | Accuracy: 0.583202\n",
      "Validation - Loss: 1.079863 | Accuracy: 0.613266\n",
      "Overall    - F1: 0.606551 | Precision: 0.690357 | Recall: 0.586805\n",
      "Metrics    - MAE: 0.502639 | Adjacent Acc: 0.904980 | QWK: 0.744622\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.535411   0.866972     0.387295   0.387295  \n",
      "Negative             0.621013   0.543961     0.723497   0.723497  \n",
      "Neutral              0.596017   0.810445     0.471316   0.471316  \n",
      "Positive             0.598909   0.529873     0.688627   0.688627  \n",
      "Extremely Positive   0.681404   0.700535     0.663291   0.663291  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.7446 (saved model)\n",
      "🌟 New best QWK: 0.7446 (saved model)\n",
      "\n",
      "EPOCH 2/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:36<00:00,  5.00it/s, Loss=1.2709, Acc=0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:06<00:00, 19.60it/s, Loss=1.5939, Acc=0\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 163.4s\n",
      "Training   - Loss: 1.270937 | Accuracy: 0.493297\n",
      "Validation - Loss: 1.593869 | Accuracy: 0.204039\n",
      "Overall    - F1: 0.067785 | Precision: 0.040808 | Recall: 0.200000\n",
      "Metrics    - MAE: 1.089282 | Adjacent Acc: 0.706679 | QWK: 0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.000000   0.000000     0.000000   0.000000  \n",
      "Negative             0.000000   0.000000     0.000000   0.000000  \n",
      "Neutral              0.338925   0.204039     1.000000   1.000000  \n",
      "Positive             0.000000   0.000000     0.000000   0.000000  \n",
      "Extremely Positive   0.000000   0.000000     0.000000   0.000000  \n",
      "========================================================================================================================\n",
      "QWK: 0.0000 (best: 0.7446 at epoch 1)\n",
      "📉 QWK: 0.0000 (best: 0.7446 at epoch 1)\n",
      "\n",
      "EPOCH 3/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:35<00:00,  5.03it/s, Loss=1.6133, Acc=0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:07<00:00, 19.30it/s, Loss=1.6082, Acc=0\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 3 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 162.5s\n",
      "Training   - Loss: 1.613277 | Accuracy: 0.203076\n",
      "Validation - Loss: 1.608220 | Accuracy: 0.181317\n",
      "Overall    - F1: 0.061395 | Precision: 0.036263 | Recall: 0.200000\n",
      "Metrics    - MAE: 1.778747 | Adjacent Acc: 0.473950 | QWK: 0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.000000   0.000000     0.000000   0.000000  \n",
      "Negative             0.000000   0.000000     0.000000   0.000000  \n",
      "Neutral              0.000000   0.000000     0.000000   0.000000  \n",
      "Positive             0.000000   0.000000     0.000000   0.000000  \n",
      "Extremely Positive   0.306975   0.181317     1.000000   1.000000  \n",
      "========================================================================================================================\n",
      "QWK: 0.0000 (best: 0.7446 at epoch 1)\n",
      "📉 QWK: 0.0000 (best: 0.7446 at epoch 1)\n",
      "\n",
      "EPOCH 4/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/782 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 782/782 [02:34<00:00,  5.05it/s, Loss=1.6092, Acc=0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/137 [00:00<?, ?it/s]                             /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 137/137 [00:06<00:00, 20.20it/s, Loss=1.6091, Acc=0\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 4 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 161.6s\n",
      "Training   - Loss: 1.609223 | Accuracy: 0.213328\n",
      "Validation - Loss: 1.609069 | Accuracy: 0.210007\n",
      "Overall    - F1: 0.069423 | Precision: 0.042001 | Recall: 0.200000\n",
      "Metrics    - MAE: 1.445261 | Adjacent Acc: 0.526050 | QWK: 0.000000\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.000000   0.000000     0.000000   0.000000  \n",
      "Negative             0.347117   0.210007     1.000000   1.000000  \n",
      "Neutral              0.000000   0.000000     0.000000   0.000000  \n",
      "Positive             0.000000   0.000000     0.000000   0.000000  \n",
      "Extremely Positive   0.000000   0.000000     0.000000   0.000000  \n",
      "========================================================================================================================\n",
      "QWK: 0.0000 (best: 0.7446 at epoch 1)\n",
      "📉 QWK: 0.0000 (best: 0.7446 at epoch 1)\n",
      "\n",
      "⏹️  EARLY STOPPING triggered at epoch 4\n",
      "    No improvement in QWK for 2 epochs\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best QWK: 0.7446 (Epoch 1)\n",
      "Total Epochs: 4\n",
      "Model Size: 475.51 MB\n",
      "Parameters: 124,649,477 total, 124,649,477 trainable\n",
      "Inference Time: 186.17 ms per sample\n",
      "Efficiency: 124.6M params, 186.2ms\n",
      "============================================================\n",
      "✅ Trial checkpoint saved: ./checkpoints_roberta/trial_5/final_epoch_15.ckpt\n",
      "✅ Model files saved in: ./checkpoints_roberta/trial_5\n",
      "📊 Trial 5 score: 0.7446 (Best: 0.9345)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▆█</td></tr><tr><td>Epoch_Time</td><td>█▅▃▁</td></tr><tr><td>Inference Time (ms)</td><td>▁▁▁▁</td></tr><tr><td>Learning_Rate</td><td>▁██▇</td></tr><tr><td>Model Size (MB)</td><td>▁▁▁▁</td></tr><tr><td>Parameters per MB</td><td>▁▁▁▁</td></tr><tr><td>Total Parameters</td><td>▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>█▆▁▁</td></tr><tr><td>Train Loss</td><td>▁▃██</td></tr><tr><td>Trainable Parameters</td><td>▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>█▁▁▁</td></tr><tr><td>Validation Adjacent Accuracy</td><td>█▅▁▂</td></tr><tr><td>Validation F1</td><td>█▁▁▁</td></tr><tr><td>Validation Loss</td><td>▁███</td></tr><tr><td>Validation MAE</td><td>▁▄█▆</td></tr><tr><td>Validation Precision</td><td>█▁▁▁</td></tr><tr><td>Validation QWK</td><td>█▁▁▁</td></tr><tr><td>Validation Recall</td><td>█▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>4</td></tr><tr><td>Epoch_Time</td><td>161.58705</td></tr><tr><td>Inference Time (ms)</td><td>186.16903</td></tr><tr><td>Learning_Rate</td><td>0.00029</td></tr><tr><td>Model Size (MB)</td><td>475.50786</td></tr><tr><td>Parameters per MB</td><td>262139.6762</td></tr><tr><td>Total Parameters</td><td>124649477</td></tr><tr><td>Train Accuracy</td><td>0.21333</td></tr><tr><td>Train Loss</td><td>1.60922</td></tr><tr><td>Trainable Parameters</td><td>124649477</td></tr><tr><td>Validation Accuracy</td><td>0.21001</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.52605</td></tr><tr><td>Validation F1</td><td>0.06942</td></tr><tr><td>Validation Loss</td><td>1.60907</td></tr><tr><td>Validation MAE</td><td>1.44526</td></tr><tr><td>Validation Precision</td><td>0.042</td></tr><tr><td>Validation QWK</td><td>0</td></tr><tr><td>Validation Recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twitter-roberta-base_trial_5</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/f2dwjylr' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/f2dwjylr</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_171506-f2dwjylr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 17:26:24,223] Trial 5 finished with value: 0.744621817190995 and parameters: {'learning_rate': 0.00031292994357053257, 'batch_size': 32, 'label_smoothing': 0.09141192067162016, 'epochs': 15, 'warmup_ratio': 0.10965139892518147, 'weight_decay': 0.14480225082316792, 'attention_dropout': 0.3551371177370714, 'hidden_dropout': 0.31202842942848197}. Best is trial 4 with value: 0.9344879741478308.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_172625-s6e590c6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/s6e590c6' target=\"_blank\">twitter-roberta-base_trial_6</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/s6e590c6' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/s6e590c6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training Configuration:\n",
      "   • Device: cuda\n",
      "   • Model Size: 475.51 MB\n",
      "   • Total Parameters: 124,649,477\n",
      "   • Trainable Parameters: 124,649,477\n",
      "   • Avg Inference Time: 389.70 ms\n",
      "   • Total Epochs: 11\n",
      "   • Train Batches: 391\n",
      "   • Validation Batches: 69\n",
      "   • Learning Rate: 0.00e+00\n",
      "   • Batch Size: 64\n",
      "============================================================\n",
      "\n",
      "EPOCH 1/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:13<00:00,  2.93it/s, Loss=1.2458, Acc=0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00,  9.98it/s, Loss=0.9475, Acc=0.6\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 140.3s\n",
      "Training   - Loss: 1.245839 | Accuracy: 0.477524\n",
      "Validation - Loss: 0.947480 | Accuracy: 0.661923\n",
      "Overall    - F1: 0.668000 | Precision: 0.671593 | Recall: 0.682204\n",
      "Metrics    - MAE: 0.431949 | Adjacent Acc: 0.918522 | QWK: 0.801656\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.739008   0.679862     0.809426   0.809426  \n",
      "Negative             0.511533   0.674419     0.412022   0.412022  \n",
      "Neutral              0.721545   0.658017     0.798650   0.798650  \n",
      "Positive             0.613077   0.601509     0.625098   0.625098  \n",
      "Extremely Positive   0.754835   0.744157     0.765823   0.765823  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8017 (saved model)\n",
      "🌟 New best QWK: 0.8017 (saved model)\n",
      "\n",
      "EPOCH 2/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.7781, Acc=0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.34it/s, Loss=0.8931, Acc=0.7\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.4s\n",
      "Training   - Loss: 0.778077 | Accuracy: 0.750000\n",
      "Validation - Loss: 0.893077 | Accuracy: 0.713794\n",
      "Overall    - F1: 0.725810 | Precision: 0.731798 | Recall: 0.753600\n",
      "Metrics    - MAE: 0.345880 | Adjacent Acc: 0.948129 | QWK: 0.868034\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.785064   0.706557     0.883197   0.883197  \n",
      "Negative             0.702736   0.791781     0.631694   0.631694  \n",
      "Neutral              0.834711   0.878261     0.795276   0.795276  \n",
      "Positive             0.583868   0.702315     0.499608   0.499608  \n",
      "Extremely Positive   0.722673   0.580077     0.958228   0.958228  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8680 (saved model)\n",
      "🌟 New best QWK: 0.8680 (saved model)\n",
      "\n",
      "EPOCH 3/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.6077, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.33it/s, Loss=0.6697, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 3 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.4s\n",
      "Training   - Loss: 0.607749 | Accuracy: 0.832413\n",
      "Validation - Loss: 0.669724 | Accuracy: 0.815699\n",
      "Overall    - F1: 0.820662 | Precision: 0.835634 | Recall: 0.811519\n",
      "Metrics    - MAE: 0.225384 | Adjacent Acc: 0.960753 | QWK: 0.901579\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.842105   0.848233     0.836066   0.836066  \n",
      "Negative             0.777162   0.788526     0.766120   0.766120  \n",
      "Neutral              0.854155   0.883413     0.826772   0.826772  \n",
      "Positive             0.800719   0.739535     0.872941   0.872941  \n",
      "Extremely Positive   0.829167   0.918462     0.755696   0.755696  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9016 (saved model)\n",
      "🌟 New best QWK: 0.9016 (saved model)\n",
      "\n",
      "EPOCH 4/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.94it/s, Loss=0.5009, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.41it/s, Loss=0.6176, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 4 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.4s\n",
      "Training   - Loss: 0.500918 | Accuracy: 0.882492\n",
      "Validation - Loss: 0.617621 | Accuracy: 0.839109\n",
      "Overall    - F1: 0.843767 | Precision: 0.850869 | Recall: 0.838217\n",
      "Metrics    - MAE: 0.200367 | Adjacent Acc: 0.962589 | QWK: 0.911956\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.851380   0.883260     0.821721   0.821721  \n",
      "Negative             0.801897   0.774161     0.831694   0.831694  \n",
      "Neutral              0.875437   0.907117     0.845894   0.845894  \n",
      "Positive             0.818426   0.801504     0.836078   0.836078  \n",
      "Extremely Positive   0.871696   0.888305     0.855696   0.855696  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9120 (saved model)\n",
      "🌟 New best QWK: 0.9120 (saved model)\n",
      "\n",
      "EPOCH 5/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.4328, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.24it/s, Loss=0.6763, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 5 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.5s\n",
      "Training   - Loss: 0.432836 | Accuracy: 0.916404\n",
      "Validation - Loss: 0.676338 | Accuracy: 0.821437\n",
      "Overall    - F1: 0.825656 | Precision: 0.823925 | Recall: 0.828665\n",
      "Metrics    - MAE: 0.209777 | Adjacent Acc: 0.971311 | QWK: 0.915814\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.840041   0.846154     0.834016   0.834016  \n",
      "Negative             0.792536   0.796031     0.789071   0.789071  \n",
      "Neutral              0.842640   0.845023     0.840270   0.840270  \n",
      "Positive             0.796774   0.819917     0.774902   0.774902  \n",
      "Extremely Positive   0.856287   0.812500     0.905063   0.905063  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9158 (saved model)\n",
      "🌟 New best QWK: 0.9158 (saved model)\n",
      "\n",
      "EPOCH 6/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.3658, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.16it/s, Loss=0.7222, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 6 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.3s\n",
      "Training   - Loss: 0.365795 | Accuracy: 0.945189\n",
      "Validation - Loss: 0.722195 | Accuracy: 0.819830\n",
      "Overall    - F1: 0.820076 | Precision: 0.814370 | Recall: 0.832379\n",
      "Metrics    - MAE: 0.212532 | Adjacent Acc: 0.969933 | QWK: 0.916187\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.805679   0.710485     0.930328   0.930328  \n",
      "Negative             0.744501   0.769231     0.721311   0.721311  \n",
      "Neutral              0.847019   0.847019     0.847019   0.847019  \n",
      "Positive             0.823388   0.841244     0.806275   0.806275  \n",
      "Extremely Positive   0.879792   0.903872     0.856962   0.856962  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9162 (saved model)\n",
      "🌟 New best QWK: 0.9162 (saved model)\n",
      "\n",
      "EPOCH 7/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.3621, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.08it/s, Loss=0.7401, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 7 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.4s\n",
      "Training   - Loss: 0.362091 | Accuracy: 0.947950\n",
      "Validation - Loss: 0.740078 | Accuracy: 0.828322\n",
      "Overall    - F1: 0.831974 | Precision: 0.829953 | Recall: 0.835767\n",
      "Metrics    - MAE: 0.193941 | Adjacent Acc: 0.979573 | QWK: 0.925938\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.853955   0.845382     0.862705   0.862705  \n",
      "Negative             0.788087   0.827918     0.751913   0.751913  \n",
      "Neutral              0.823148   0.792708     0.856018   0.856018  \n",
      "Positive             0.819528   0.836601     0.803137   0.803137  \n",
      "Extremely Positive   0.875153   0.847156     0.905063   0.905063  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9259 (saved model)\n",
      "🌟 New best QWK: 0.9259 (saved model)\n",
      "\n",
      "EPOCH 8/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.3222, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.11it/s, Loss=0.7708, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 8 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.2s\n",
      "Training   - Loss: 0.322159 | Accuracy: 0.963722\n",
      "Validation - Loss: 0.770843 | Accuracy: 0.823502\n",
      "Overall    - F1: 0.826605 | Precision: 0.820587 | Recall: 0.836651\n",
      "Metrics    - MAE: 0.200138 | Adjacent Acc: 0.978885 | QWK: 0.924210\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.841699   0.795620     0.893443   0.893443  \n",
      "Negative             0.787984   0.835784     0.745355   0.745355  \n",
      "Neutral              0.833787   0.808668     0.860517   0.860517  \n",
      "Positive             0.805726   0.841880     0.772549   0.772549  \n",
      "Extremely Positive   0.863827   0.820981     0.911392   0.911392  \n",
      "========================================================================================================================\n",
      "QWK: 0.9242 (best: 0.9259 at epoch 7)\n",
      "📉 QWK: 0.9242 (best: 0.9259 at epoch 7)\n",
      "\n",
      "EPOCH 9/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.2922, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.26it/s, Loss=0.7939, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 9 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.1s\n",
      "Training   - Loss: 0.292244 | Accuracy: 0.977524\n",
      "Validation - Loss: 0.793877 | Accuracy: 0.830847\n",
      "Overall    - F1: 0.833369 | Precision: 0.827985 | Recall: 0.840930\n",
      "Metrics    - MAE: 0.193711 | Adjacent Acc: 0.977507 | QWK: 0.925602\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.843629   0.797445     0.895492   0.895492  \n",
      "Negative             0.790194   0.825983     0.757377   0.757377  \n",
      "Neutral              0.838137   0.826230     0.850394   0.850394  \n",
      "Positive             0.820657   0.838103     0.803922   0.803922  \n",
      "Extremely Positive   0.874229   0.852163     0.897468   0.897468  \n",
      "========================================================================================================================\n",
      "QWK: 0.9256 (best: 0.9259 at epoch 7)\n",
      "📉 QWK: 0.9256 (best: 0.9259 at epoch 7)\n",
      "\n",
      "EPOCH 10/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.2811, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.32it/s, Loss=0.7987, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 10 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.2s\n",
      "Training   - Loss: 0.281069 | Accuracy: 0.981073\n",
      "Validation - Loss: 0.798715 | Accuracy: 0.833372\n",
      "Overall    - F1: 0.836982 | Precision: 0.832772 | Recall: 0.842871\n",
      "Metrics    - MAE: 0.189350 | Adjacent Acc: 0.979114 | QWK: 0.927599\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.854022   0.828516     0.881148   0.881148  \n",
      "Negative             0.803399   0.834118     0.774863   0.774863  \n",
      "Neutral              0.834242   0.809524     0.860517   0.860517  \n",
      "Positive             0.818402   0.842893     0.795294   0.795294  \n",
      "Extremely Positive   0.874847   0.848810     0.902532   0.902532  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9276 (saved model)\n",
      "🌟 New best QWK: 0.9276 (saved model)\n",
      "\n",
      "EPOCH 11/11\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.2759, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.40it/s, Loss=0.8102, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 11 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.0s\n",
      "Training   - Loss: 0.275875 | Accuracy: 0.981861\n",
      "Validation - Loss: 0.810221 | Accuracy: 0.830617\n",
      "Overall    - F1: 0.834566 | Precision: 0.830518 | Recall: 0.840592\n",
      "Metrics    - MAE: 0.192105 | Adjacent Acc: 0.979114 | QWK: 0.926643\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.856287   0.834630     0.879098   0.879098  \n",
      "Negative             0.797727   0.830769     0.767213   0.767213  \n",
      "Neutral              0.831522   0.804416     0.860517   0.860517  \n",
      "Positive             0.815715   0.843384     0.789804   0.789804  \n",
      "Extremely Positive   0.871576   0.839390     0.906329   0.906329  \n",
      "========================================================================================================================\n",
      "QWK: 0.9266 (best: 0.9276 at epoch 10)\n",
      "📉 QWK: 0.9266 (best: 0.9276 at epoch 10)\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best QWK: 0.9276 (Epoch 10)\n",
      "Total Epochs: 11\n",
      "Model Size: 475.51 MB\n",
      "Parameters: 124,649,477 total, 124,649,477 trainable\n",
      "Inference Time: 389.70 ms per sample\n",
      "Efficiency: 124.6M params, 389.7ms\n",
      "============================================================\n",
      "✅ Trial checkpoint saved: ./checkpoints_roberta/trial_6/final_epoch_11.ckpt\n",
      "✅ Model files saved in: ./checkpoints_roberta/trial_6\n",
      "📊 Trial 6 score: 0.9276 (Best: 0.9345)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>Epoch_Time</td><td>█▃▃▃▄▂▃▂▂▂▁</td></tr><tr><td>Inference Time (ms)</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Learning_Rate</td><td>██▇▇▆▅▃▂▂▁▁</td></tr><tr><td>Model Size (MB)</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters per MB</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Total Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▅▆▇▇▇█████</td></tr><tr><td>Train Loss</td><td>█▅▃▃▂▂▂▁▁▁▁</td></tr><tr><td>Trainable Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▃▇█▇▇█▇███</td></tr><tr><td>Validation Adjacent Accuracy</td><td>▁▄▆▆▇▇█████</td></tr><tr><td>Validation F1</td><td>▁▃▇█▇▇█▇███</td></tr><tr><td>Validation Loss</td><td>█▇▂▁▂▃▄▄▅▅▅</td></tr><tr><td>Validation MAE</td><td>█▆▂▁▂▂▁▁▁▁▁</td></tr><tr><td>Validation Precision</td><td>▁▃▇█▇▇▇▇▇▇▇</td></tr><tr><td>Validation QWK</td><td>▁▅▇▇▇▇█████</td></tr><tr><td>Validation Recall</td><td>▁▄▇█▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>11</td></tr><tr><td>Epoch_Time</td><td>139.02291</td></tr><tr><td>Inference Time (ms)</td><td>389.7007</td></tr><tr><td>Learning_Rate</td><td>0</td></tr><tr><td>Model Size (MB)</td><td>475.50786</td></tr><tr><td>Parameters per MB</td><td>262139.6762</td></tr><tr><td>Total Parameters</td><td>124649477</td></tr><tr><td>Train Accuracy</td><td>0.98186</td></tr><tr><td>Train Loss</td><td>0.27587</td></tr><tr><td>Trainable Parameters</td><td>124649477</td></tr><tr><td>Validation Accuracy</td><td>0.83062</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.97911</td></tr><tr><td>Validation F1</td><td>0.83457</td></tr><tr><td>Validation Loss</td><td>0.81022</td></tr><tr><td>Validation MAE</td><td>0.1921</td></tr><tr><td>Validation Precision</td><td>0.83052</td></tr><tr><td>Validation QWK</td><td>0.92664</td></tr><tr><td>Validation Recall</td><td>0.84059</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twitter-roberta-base_trial_6</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/s6e590c6' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/s6e590c6</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_172625-s6e590c6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 17:52:25,143] Trial 6 finished with value: 0.9275993819341823 and parameters: {'learning_rate': 3.486332463650596e-05, 'batch_size': 64, 'label_smoothing': 0.05198841871440321, 'epochs': 11, 'warmup_ratio': 0.09195415917472691, 'weight_decay': 0.14211805930196114, 'attention_dropout': 0.302422228310401, 'hidden_dropout': 0.3283387336293008}. Best is trial 4 with value: 0.9344879741478308.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_175226-gc9rczws</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/gc9rczws' target=\"_blank\">twitter-roberta-base_trial_7</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/gc9rczws' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/gc9rczws</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Training Configuration:\n",
      "   • Device: cuda\n",
      "   • Model Size: 475.51 MB\n",
      "   • Total Parameters: 124,649,477\n",
      "   • Trainable Parameters: 124,649,477\n",
      "   • Avg Inference Time: 387.37 ms\n",
      "   • Total Epochs: 15\n",
      "   • Train Batches: 391\n",
      "   • Validation Batches: 69\n",
      "   • Learning Rate: 0.00e+00\n",
      "   • Batch Size: 64\n",
      "============================================================\n",
      "\n",
      "EPOCH 1/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:13<00:00,  2.93it/s, Loss=1.2640, Acc=0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.05it/s, Loss=0.9892, Acc=0.6\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 140.3s\n",
      "Training   - Loss: 1.263951 | Accuracy: 0.494479\n",
      "Validation - Loss: 0.989156 | Accuracy: 0.697039\n",
      "Overall    - F1: 0.704308 | Precision: 0.699614 | Recall: 0.719404\n",
      "Metrics    - MAE: 0.369520 | Adjacent Acc: 0.941244 | QWK: 0.840867\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.746641   0.702166     0.797131   0.797131  \n",
      "Negative             0.616382   0.668798     0.571585   0.571585  \n",
      "Neutral              0.756598   0.668971     0.870641   0.870641  \n",
      "Positive             0.629136   0.695817     0.574118   0.574118  \n",
      "Extremely Positive   0.772784   0.762315     0.783544   0.783544  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8409 (saved model)\n",
      "🌟 New best QWK: 0.8409 (saved model)\n",
      "\n",
      "EPOCH 2/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.8997, Acc=0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.35it/s, Loss=0.8999, Acc=0.7\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.4s\n",
      "Training   - Loss: 0.899747 | Accuracy: 0.754338\n",
      "Validation - Loss: 0.899935 | Accuracy: 0.763599\n",
      "Overall    - F1: 0.765488 | Precision: 0.761976 | Recall: 0.780082\n",
      "Metrics    - MAE: 0.291944 | Adjacent Acc: 0.948359 | QWK: 0.881940\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.757386   0.672496     0.866803   0.866803  \n",
      "Negative             0.672065   0.713759     0.634973   0.634973  \n",
      "Neutral              0.829982   0.905585     0.766029   0.766029  \n",
      "Positive             0.744074   0.762768     0.726275   0.726275  \n",
      "Extremely Positive   0.823936   0.755274     0.906329   0.906329  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.8819 (saved model)\n",
      "🌟 New best QWK: 0.8819 (saved model)\n",
      "\n",
      "EPOCH 3/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.7949, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.35it/s, Loss=0.8525, Acc=0.7\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 3 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.3s\n",
      "Training   - Loss: 0.794935 | Accuracy: 0.810331\n",
      "Validation - Loss: 0.852509 | Accuracy: 0.798256\n",
      "Overall    - F1: 0.802363 | Precision: 0.793767 | Recall: 0.822530\n",
      "Metrics    - MAE: 0.237778 | Adjacent Acc: 0.966491 | QWK: 0.908196\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.822222   0.750000     0.909836   0.909836  \n",
      "Negative             0.753394   0.780774     0.727869   0.727869  \n",
      "Neutral              0.845777   0.820296     0.872891   0.872891  \n",
      "Positive             0.753954   0.857143     0.672941   0.672941  \n",
      "Extremely Positive   0.836467   0.760622     0.929114   0.929114  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9082 (saved model)\n",
      "🌟 New best QWK: 0.9082 (saved model)\n",
      "\n",
      "EPOCH 4/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.6948, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.10it/s, Loss=0.8211, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 4 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.6s\n",
      "Training   - Loss: 0.694774 | Accuracy: 0.867902\n",
      "Validation - Loss: 0.821062 | Accuracy: 0.810190\n",
      "Overall    - F1: 0.809779 | Precision: 0.804189 | Recall: 0.826064\n",
      "Metrics    - MAE: 0.232958 | Adjacent Acc: 0.960753 | QWK: 0.904866\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.792586   0.696124     0.920082   0.920082  \n",
      "Negative             0.732579   0.804974     0.672131   0.672131  \n",
      "Neutral              0.860465   0.890493     0.832396   0.832396  \n",
      "Positive             0.799197   0.818930     0.780392   0.780392  \n",
      "Extremely Positive   0.864066   0.810421     0.925316   0.925316  \n",
      "========================================================================================================================\n",
      "QWK: 0.9049 (best: 0.9082 at epoch 3)\n",
      "📉 QWK: 0.9049 (best: 0.9082 at epoch 3)\n",
      "\n",
      "EPOCH 5/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.6682, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:07<00:00,  9.82it/s, Loss=0.8797, Acc=0.7\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 5 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.7s\n",
      "Training   - Loss: 0.668216 | Accuracy: 0.874211\n",
      "Validation - Loss: 0.879658 | Accuracy: 0.788157\n",
      "Overall    - F1: 0.793673 | Precision: 0.809494 | Recall: 0.798649\n",
      "Metrics    - MAE: 0.251320 | Adjacent Acc: 0.963277 | QWK: 0.900088\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.798619   0.910761     0.711066   0.711066  \n",
      "Negative             0.788687   0.770594     0.807650   0.807650  \n",
      "Neutral              0.881952   0.890034     0.874016   0.874016  \n",
      "Positive             0.710445   0.810865     0.632157   0.632157  \n",
      "Extremely Positive   0.788660   0.665217     0.968354   0.968354  \n",
      "========================================================================================================================\n",
      "QWK: 0.9001 (best: 0.9082 at epoch 3)\n",
      "📉 QWK: 0.9001 (best: 0.9082 at epoch 3)\n",
      "\n",
      "EPOCH 6/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.94it/s, Loss=0.5839, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.25it/s, Loss=0.7801, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 6 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.6s\n",
      "Training   - Loss: 0.583938 | Accuracy: 0.923896\n",
      "Validation - Loss: 0.780106 | Accuracy: 0.832224\n",
      "Overall    - F1: 0.835107 | Precision: 0.832172 | Recall: 0.839964\n",
      "Metrics    - MAE: 0.200826 | Adjacent Acc: 0.968786 | QWK: 0.917501\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.836923   0.837782     0.836066   0.836066  \n",
      "Negative             0.792132   0.771222     0.814208   0.814208  \n",
      "Neutral              0.866228   0.844920     0.888639   0.888639  \n",
      "Positive             0.810474   0.862069     0.764706   0.764706  \n",
      "Extremely Positive   0.869779   0.844869     0.896203   0.896203  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9175 (saved model)\n",
      "🌟 New best QWK: 0.9175 (saved model)\n",
      "\n",
      "EPOCH 7/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.5679, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.21it/s, Loss=0.8272, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 7 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.5s\n",
      "Training   - Loss: 0.567883 | Accuracy: 0.938486\n",
      "Validation - Loss: 0.827156 | Accuracy: 0.838191\n",
      "Overall    - F1: 0.840027 | Precision: 0.835633 | Recall: 0.845835\n",
      "Metrics    - MAE: 0.195777 | Adjacent Acc: 0.970163 | QWK: 0.917923\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.841487   0.805243     0.881148   0.881148  \n",
      "Negative             0.789622   0.815851     0.765027   0.765027  \n",
      "Neutral              0.856335   0.861206     0.851519   0.851519  \n",
      "Positive             0.828571   0.838554     0.818824   0.818824  \n",
      "Extremely Positive   0.884120   0.857313     0.912658   0.912658  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9179 (saved model)\n",
      "🌟 New best QWK: 0.9179 (saved model)\n",
      "\n",
      "EPOCH 8/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.5383, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.26it/s, Loss=0.8324, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 8 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.4s\n",
      "Training   - Loss: 0.538261 | Accuracy: 0.949921\n",
      "Validation - Loss: 0.832434 | Accuracy: 0.824420\n",
      "Overall    - F1: 0.831322 | Precision: 0.832662 | Recall: 0.839277\n",
      "Metrics    - MAE: 0.199220 | Adjacent Acc: 0.978885 | QWK: 0.924125\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.869565   0.901099     0.840164   0.840164  \n",
      "Negative             0.826327   0.836506     0.816393   0.816393  \n",
      "Neutral              0.847349   0.808793     0.889764   0.889764  \n",
      "Positive             0.775563   0.866409     0.701961   0.701961  \n",
      "Extremely Positive   0.837808   0.750501     0.948101   0.948101  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9241 (saved model)\n",
      "🌟 New best QWK: 0.9241 (saved model)\n",
      "\n",
      "EPOCH 9/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.5170, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.35it/s, Loss=0.7998, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 9 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.1s\n",
      "Training   - Loss: 0.516990 | Accuracy: 0.962145\n",
      "Validation - Loss: 0.799761 | Accuracy: 0.843241\n",
      "Overall    - F1: 0.848135 | Precision: 0.850047 | Recall: 0.849761\n",
      "Metrics    - MAE: 0.176727 | Adjacent Acc: 0.982786 | QWK: 0.930363\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.871102   0.883966     0.858607   0.858607  \n",
      "Negative             0.826259   0.856808     0.797814   0.797814  \n",
      "Neutral              0.834274   0.766981     0.914511   0.914511  \n",
      "Positive             0.826352   0.858108     0.796863   0.796863  \n",
      "Extremely Positive   0.882689   0.884371     0.881013   0.881013  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9304 (saved model)\n",
      "🌟 New best QWK: 0.9304 (saved model)\n",
      "\n",
      "EPOCH 10/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.95it/s, Loss=0.5046, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.21it/s, Loss=0.8358, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 10 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.3s\n",
      "Training   - Loss: 0.504592 | Accuracy: 0.965694\n",
      "Validation - Loss: 0.835806 | Accuracy: 0.839569\n",
      "Overall    - F1: 0.840780 | Precision: 0.833934 | Recall: 0.852095\n",
      "Metrics    - MAE: 0.183842 | Adjacent Acc: 0.979344 | QWK: 0.929164\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.842006   0.782074     0.911885   0.911885  \n",
      "Negative             0.791691   0.838631     0.749727   0.749727  \n",
      "Neutral              0.852334   0.823715     0.883015   0.883015  \n",
      "Positive             0.831424   0.869119     0.796863   0.796863  \n",
      "Extremely Positive   0.886447   0.856132     0.918987   0.918987  \n",
      "========================================================================================================================\n",
      "QWK: 0.9292 (best: 0.9304 at epoch 9)\n",
      "📉 QWK: 0.9292 (best: 0.9304 at epoch 9)\n",
      "\n",
      "EPOCH 11/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.96it/s, Loss=0.4771, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.00it/s, Loss=0.8251, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 11 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.1s\n",
      "Training   - Loss: 0.477078 | Accuracy: 0.978312\n",
      "Validation - Loss: 0.825147 | Accuracy: 0.847602\n",
      "Overall    - F1: 0.851103 | Precision: 0.847316 | Recall: 0.858546\n",
      "Metrics    - MAE: 0.173514 | Adjacent Acc: 0.980950 | QWK: 0.933136\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.870518   0.846899     0.895492   0.895492  \n",
      "Negative             0.819388   0.867971     0.775956   0.775956  \n",
      "Neutral              0.852878   0.810537     0.899888   0.899888  \n",
      "Positive             0.830946   0.869007     0.796078   0.796078  \n",
      "Extremely Positive   0.881785   0.842166     0.925316   0.925316  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9331 (saved model)\n",
      "🌟 New best QWK: 0.9331 (saved model)\n",
      "\n",
      "EPOCH 12/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.96it/s, Loss=0.4580, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:07<00:00,  9.79it/s, Loss=0.8380, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 12 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 139.3s\n",
      "Training   - Loss: 0.457990 | Accuracy: 0.989353\n",
      "Validation - Loss: 0.838046 | Accuracy: 0.849667\n",
      "Overall    - F1: 0.853591 | Precision: 0.854458 | Recall: 0.855389\n",
      "Metrics    - MAE: 0.173055 | Adjacent Acc: 0.980262 | QWK: 0.930608\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.873684   0.898268     0.850410   0.850410  \n",
      "Negative             0.826531   0.858657     0.796721   0.796721  \n",
      "Neutral              0.850882   0.810591     0.895388   0.895388  \n",
      "Positive             0.833333   0.860485     0.807843   0.807843  \n",
      "Extremely Positive   0.883524   0.844291     0.926582   0.926582  \n",
      "========================================================================================================================\n",
      "QWK: 0.9306 (best: 0.9331 at epoch 11)\n",
      "📉 QWK: 0.9306 (best: 0.9331 at epoch 11)\n",
      "\n",
      "EPOCH 13/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:11<00:00,  2.96it/s, Loss=0.4491, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.37it/s, Loss=0.8653, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 13 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 138.7s\n",
      "Training   - Loss: 0.449136 | Accuracy: 0.992508\n",
      "Validation - Loss: 0.865318 | Accuracy: 0.846224\n",
      "Overall    - F1: 0.847964 | Precision: 0.843925 | Recall: 0.856335\n",
      "Metrics    - MAE: 0.175350 | Adjacent Acc: 0.981180 | QWK: 0.931437\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.853540   0.810313     0.901639   0.901639  \n",
      "Negative             0.806957   0.859259     0.760656   0.760656  \n",
      "Neutral              0.853249   0.798822     0.915636   0.915636  \n",
      "Positive             0.838157   0.872666     0.806275   0.806275  \n",
      "Extremely Positive   0.887915   0.878563     0.897468   0.897468  \n",
      "========================================================================================================================\n",
      "QWK: 0.9314 (best: 0.9331 at epoch 11)\n",
      "📉 QWK: 0.9314 (best: 0.9331 at epoch 11)\n",
      "\n",
      "EPOCH 14/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:12<00:00,  2.96it/s, Loss=0.4475, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.18it/s, Loss=0.8520, Acc=0.8\n",
      "/tmp/ipython-input-3737844875.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 14 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 138.8s\n",
      "Training   - Loss: 0.447500 | Accuracy: 0.992114\n",
      "Validation - Loss: 0.851979 | Accuracy: 0.854717\n",
      "Overall    - F1: 0.857486 | Precision: 0.858000 | Recall: 0.859664\n",
      "Metrics    - MAE: 0.165022 | Adjacent Acc: 0.983016 | QWK: 0.934534\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.871001   0.877339     0.864754   0.864754  \n",
      "Negative             0.823864   0.857988     0.792350   0.792350  \n",
      "Neutral              0.848801   0.791059     0.915636   0.915636  \n",
      "Positive             0.849293   0.875833     0.824314   0.824314  \n",
      "Extremely Positive   0.894472   0.887781     0.901266   0.901266  \n",
      "========================================================================================================================\n",
      "New best QWK: 0.9345 (saved model)\n",
      "🌟 New best QWK: 0.9345 (saved model)\n",
      "\n",
      "EPOCH 15/15\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|                              | 0/391 [00:00<?, ?it/s]                               /tmp/ipython-input-3737844875.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training: 100%|██████████████████████████████| 391/391 [02:11<00:00,  2.96it/s, Loss=0.4397, Acc=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Validation Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                              | 0/69 [00:00<?, ?it/s]                              /tmp/ipython-input-3737844875.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████████████████████████| 69/69 [00:06<00:00, 10.21it/s, Loss=0.8618, Acc=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 15 DETAILED SUMMARY\n",
      "========================================================================================================================\n",
      "Epoch Time: 138.7s\n",
      "Training   - Loss: 0.439659 | Accuracy: 0.996451\n",
      "Validation - Loss: 0.861794 | Accuracy: 0.853110\n",
      "Overall    - F1: 0.856094 | Precision: 0.853624 | Recall: 0.861415\n",
      "Metrics    - MAE: 0.168235 | Adjacent Acc: 0.981409 | QWK: 0.933376\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "PER-CLASS BREAKDOWN:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Class                F1         Precision    Recall     Accuracy  \n",
      "─────────────────────────────────────────────────────────────────\n",
      "Extremely Negative   0.870968   0.857143     0.885246   0.885246  \n",
      "Negative             0.824471   0.864508     0.787978   0.787978  \n",
      "Neutral              0.855485   0.805362     0.912261   0.912261  \n",
      "Positive             0.841205   0.874682     0.810196   0.810196  \n",
      "Extremely Positive   0.888341   0.866426     0.911392   0.911392  \n",
      "========================================================================================================================\n",
      "QWK: 0.9334 (best: 0.9345 at epoch 14)\n",
      "📉 QWK: 0.9334 (best: 0.9345 at epoch 14)\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Best QWK: 0.9345 (Epoch 14)\n",
      "Total Epochs: 15\n",
      "Model Size: 475.51 MB\n",
      "Parameters: 124,649,477 total, 124,649,477 trainable\n",
      "Inference Time: 387.37 ms per sample\n",
      "Efficiency: 124.6M params, 387.4ms\n",
      "============================================================\n",
      "✅ Trial checkpoint saved: ./checkpoints_roberta/trial_7/final_epoch_15.ckpt\n",
      "✅ Model files saved in: ./checkpoints_roberta/trial_7\n",
      "🏆 New best model saved! Score: 0.9345 (Trial 7)\n",
      "🏆 Best model files saved in: ./best_roberta_model_so_far\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>Epoch_Time</td><td>█▄▄▅▅▅▄▄▃▄▃▃▁▂▁</td></tr><tr><td>Inference Time (ms)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Learning_Rate</td><td>▄███▇▇▆▅▄▃▃▂▁▁▁</td></tr><tr><td>Model Size (MB)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters per MB</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Total Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▅▅▆▆▇▇▇███████</td></tr><tr><td>Train Loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Trainable Parameters</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▆▅▇▇▇▇▇█████</td></tr><tr><td>Validation Adjacent Accuracy</td><td>▁▂▅▄▅▆▆▇█▇█████</td></tr><tr><td>Validation F1</td><td>▁▄▅▆▅▇▇▇█▇█████</td></tr><tr><td>Validation Loss</td><td>█▅▃▂▄▁▃▃▂▃▃▃▄▃▄</td></tr><tr><td>Validation MAE</td><td>█▅▃▃▄▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>Validation Precision</td><td>▁▄▅▆▆▇▇▇█▇██▇██</td></tr><tr><td>Validation QWK</td><td>▁▄▆▆▅▇▇▇███████</td></tr><tr><td>Validation Recall</td><td>▁▄▆▆▅▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>15</td></tr><tr><td>Epoch_Time</td><td>138.70596</td></tr><tr><td>Inference Time (ms)</td><td>387.36584</td></tr><tr><td>Learning_Rate</td><td>0</td></tr><tr><td>Model Size (MB)</td><td>475.50786</td></tr><tr><td>Parameters per MB</td><td>262139.6762</td></tr><tr><td>Total Parameters</td><td>124649477</td></tr><tr><td>Train Accuracy</td><td>0.99645</td></tr><tr><td>Train Loss</td><td>0.43966</td></tr><tr><td>Trainable Parameters</td><td>124649477</td></tr><tr><td>Validation Accuracy</td><td>0.85311</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.98141</td></tr><tr><td>Validation F1</td><td>0.85609</td></tr><tr><td>Validation Loss</td><td>0.86179</td></tr><tr><td>Validation MAE</td><td>0.16824</td></tr><tr><td>Validation Precision</td><td>0.85362</td></tr><tr><td>Validation QWK</td><td>0.93338</td></tr><tr><td>Validation Recall</td><td>0.86141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twitter-roberta-base_trial_7</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/gc9rczws' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta/runs/gc9rczws</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/sentiment-full-roberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_175226-gc9rczws/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 18:28:02,547] Trial 7 finished with value: 0.9345340002246791 and parameters: {'learning_rate': 0.00011137674835204488, 'batch_size': 64, 'label_smoothing': 0.113394801714666, 'epochs': 15, 'warmup_ratio': 0.14077254623725632, 'weight_decay': 0.1359792823200739, 'attention_dropout': 0.385660939987262, 'hidden_dropout': 0.35719138097972813}. Best is trial 7 with value: 0.9345340002246791.\n"
     ]
    }
   ],
   "source": [
    "global_best_qwk = 0.0\n",
    "global_best_model_state = None\n",
    "\n",
    "print(f\"Initialized global_best_qwk: {global_best_qwk}\")\n",
    "print(f\"Initialized global_best_model_state: {global_best_model_state}\")\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720d326",
   "metadata": {
    "id": "e720d326"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e6d36f",
   "metadata": {
    "id": "95e6d36f"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device=None):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            device = next(model.parameters()).device  # Get model's device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'macro_f1': f1_score(y_true, y_pred, average='macro'),\n",
    "        'precision': precision_score(y_true, y_pred, average='macro'),\n",
    "        'recall': recall_score(y_true, y_pred, average='macro'),\n",
    "        'weighted_f1': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'adjacent_accuracy': np.sum(np.abs(y_pred - y_true) <= 1) / len(y_true),\n",
    "        'qwk': cohen_kappa_score(y_true, y_pred, weights='quadratic'),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'classification_report': classification_report(y_true, y_pred,\n",
    "                                                     target_names=list(ordinal_id2label.values()),\n",
    "                                                     labels=list(ordinal_id2label.keys()))\n",
    "    }\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"Macro F1: {results['macro_f1']:.4f}\")\n",
    "    print(f\"precision: {results['precision']:.4f}\")\n",
    "    print(f\"recall: {results['recall']:.4f}\")\n",
    "    print(f\"weighted_f1: {results['weighted_f1']:.4f}\")\n",
    "    print(f\"MAE: {results['mae']:.4f}\")\n",
    "    print(f\"Adjacent Accuracy: {results['adjacent_accuracy']:.4f}\")\n",
    "    print(f\"qwk: {results['qwk']:.4f}\")\n",
    "    print(f\"\\nClassification Report:\\n{results['classification_report']}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dbd13ee",
   "metadata": {
    "id": "7dbd13ee"
   },
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # existing confusion matrix\n",
    "    sns.heatmap(results['confusion_matrix'], annot=True, fmt='d',\n",
    "                xticklabels=list(ordinal_id2label.values()),  # use ordinal_id2label\n",
    "                yticklabels=list(ordinal_id2label.values()), ax=ax1)  # use ordinal_id2label\n",
    "    ax1.set_title('Confusion Matrix')\n",
    "\n",
    "    # existing metrics bar chart\n",
    "    metrics = ['Accuracy', 'Macro F1', 'Precision', 'Recall', 'Adjacent Accuracy', 'QWK', 'MAE']\n",
    "    values = [results['accuracy'], results['macro_f1'], results['precision'], results['recall'],\n",
    "              results['adjacent_accuracy'], results['qwk'], results['mae']]\n",
    "    ax2.bar(metrics, values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax2.set_title('Key Metrics')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4ce0aa2",
   "metadata": {
    "id": "f4ce0aa2"
   },
   "outputs": [],
   "source": [
    "from torch.serialization import safe_globals\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "def quick_eval_manual(model_path, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if model_path == 'best_model_optuna.pt':\n",
    "        model_path = './best_roberta_model_so_far/model_roberta.pt'\n",
    "\n",
    "    # Use the safe_globals context manager\n",
    "    with safe_globals({RobertaForSequenceClassification}):\n",
    "        model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "    model.to(device)\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    plot_results(results)\n",
    "    return results\n",
    "\n",
    "def quick_eval_auto(model_path, study, formatted_test, tokenizer, id2label):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimal_batch_size = study.best_params['batch_size']\n",
    "    max_length = study.best_params.get('max_length', 128)  # Get from study if available\n",
    "    test_dataset = TweetDataset(formatted_test, tokenizer, max_length=max_length)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=optimal_batch_size, shuffle=False)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base\", num_labels=5, id2label=ordinal_id2label, label2id=ordinal_label2id)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    results = evaluate_model(model, test_loader,  device)\n",
    "    plot_results(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c32989-6657-4ab7-a142-7410e0c1d224",
   "metadata": {},
   "source": [
    "### Final Testing with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc934864",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "id": "cc934864",
    "outputId": "e4630db5-27fe-4f0c-ed32-15e611406429"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8402\n",
      "Macro F1: 0.8445\n",
      "precision: 0.8389\n",
      "recall: 0.8561\n",
      "weighted_f1: 0.8392\n",
      "MAE: 0.1846\n",
      "Adjacent Accuracy: 0.9781\n",
      "qwk: 0.9286\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative       0.82      0.91      0.86       410\n",
      "          Negative       0.87      0.77      0.82       813\n",
      "           Neutral       0.78      0.92      0.84       659\n",
      "          Positive       0.87      0.78      0.82       954\n",
      "Extremely Positive       0.85      0.91      0.88       588\n",
      "\n",
      "          accuracy                           0.84      3424\n",
      "         macro avg       0.84      0.86      0.84      3424\n",
      "      weighted avg       0.84      0.84      0.84      3424\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the best model that was saved\n",
    "optimal_batch_size = study.best_params['batch_size']\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=optimal_batch_size, shuffle=False)\n",
    "\n",
    "# Use the actual best model now\n",
    "results = quick_eval_manual('best_model_optuna.pt', test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c78876a-5e8c-47be-80ee-5771f24507d2",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e65c4ae4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e65c4ae4",
    "outputId": "c3c2bfee-533f-4098-b5cf-26e1ffa31ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STUDY RESULTS ===\n",
      "Best trial: 7\n",
      "Best QWK: 0.9345\n",
      "Best params: {'learning_rate': 0.00011137674835204488, 'batch_size': 64, 'label_smoothing': 0.113394801714666, 'epochs': 15, 'warmup_ratio': 0.14077254623725632, 'weight_decay': 0.1359792823200739, 'attention_dropout': 0.385660939987262, 'hidden_dropout': 0.35719138097972813}\n",
      "\n",
      "=== ALL TRIALS ===\n",
      "Trial 0: QWK = 0.9288\n",
      "Trial 1: QWK = 0.7711\n",
      "Trial 2: QWK = 0.9342\n",
      "Trial 3: QWK = 0.7978\n",
      "Trial 4: QWK = 0.9345\n",
      "Trial 5: QWK = 0.7446\n",
      "Trial 6: QWK = 0.9276\n",
      "Trial 7: QWK = 0.9345\n"
     ]
    }
   ],
   "source": [
    "# Check study results\n",
    "print(\"=== STUDY RESULTS ===\")\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"Best QWK: {study.best_value:.4f}\")\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "\n",
    "# Compare all trials\n",
    "print(\"\\n=== ALL TRIALS ===\")\n",
    "for trial in study.trials:\n",
    "    print(f\"Trial {trial.number}: QWK = {trial.value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd261e7-c797-42b5-a92e-9b4e11d47c4e",
   "metadata": {},
   "source": [
    "### Model Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a772083",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a772083",
    "outputId": "b6dbd0ac-af9c-4813-ba5c-cd6202dadf1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEST MODEL ANALYSIS ===\n",
      "============================================================\n",
      "MODEL SPECIFICATIONS:\n",
      "   • Model Size: 475.51 MB\n",
      "   • Total Parameters: 124,649,477\n",
      "   • Trainable Parameters: 124,649,477\n",
      "   • Inference Time: 202.14 ms per request\n",
      "============================================================\n",
      "PREDICTION ANALYSIS:\n",
      "   • True label distribution: {np.int64(2): 889, np.int64(1): 915, np.int64(3): 1275, np.int64(0): 488, np.int64(4): 790}\n",
      "   • Predicted label distribution: {np.int64(2): 1007, np.int64(3): 1181, np.int64(0): 504, np.int64(1): 834, np.int64(4): 831}\n",
      "   • Unique predictions: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "   • Number of classes predicted: 5\n",
      "============================================================\n",
      "✅ Model is predicting all 5 classes!\n",
      "📊 Efficiency Metrics:\n",
      "   • Parameters per MB: 262,140\n"
     ]
    }
   ],
   "source": [
    "# Analyze what the best model is predicting\n",
    "import torch\n",
    "from torch.serialization import safe_globals\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the entire model directly\n",
    "with safe_globals({RobertaForSequenceClassification}):\n",
    "    model = torch.load('./best_roberta_model_so_far/model_roberta.pt', map_location=device, weights_only=False)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Calculate model metrics\n",
    "model_size_mb = get_model_size(model)\n",
    "total_params, trainable_params = get_model_parameters(model)\n",
    "\n",
    "# Get a sample batch for inference timing\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "sample_batch = next(iter(val_loader))\n",
    "inference_time_ms = measure_inference_time(model, sample_batch, device)\n",
    "\n",
    "# Check predictions on validation data\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Analyze predictions\n",
    "from collections import Counter\n",
    "pred_dist = Counter(all_predictions)\n",
    "true_dist = Counter(all_true_labels)\n",
    "\n",
    "print(\"=== BEST MODEL ANALYSIS ===\")\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL SPECIFICATIONS:\")\n",
    "print(f\"   • Model Size: {model_size_mb:.2f} MB\")\n",
    "print(f\"   • Total Parameters: {total_params:,}\")\n",
    "print(f\"   • Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"   • Inference Time: {inference_time_ms:.2f} ms per request\")\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICTION ANALYSIS:\")\n",
    "print(f\"   • True label distribution: {dict(true_dist)}\")\n",
    "print(f\"   • Predicted label distribution: {dict(pred_dist)}\")\n",
    "print(f\"   • Unique predictions: {sorted(set(all_predictions))}\")\n",
    "print(f\"   • Number of classes predicted: {len(set(all_predictions))}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(set(all_predictions)) == 5:\n",
    "    print(\"✅ Model is predicting all 5 classes!\")\n",
    "elif len(set(all_predictions)) > 1:\n",
    "    print(f\"✅ Model is predicting {len(set(all_predictions))} classes (better than before!)\")\n",
    "else:\n",
    "    print(\"❌ Model still predicting only 1 class\")\n",
    "\n",
    "print(f\"📊 Efficiency Metrics:\")\n",
    "print(f\"   • Parameters per MB: {total_params/model_size_mb:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "853444bd",
   "metadata": {
    "id": "853444bd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a928b79-5dad-46c0-9697-c39d12d05ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00454005c106401db1769318286f9169": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "011db3b3e5cb4013a5202bba057dc835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb11b30150f1404083362c2bbc4f4e5e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36f534bdf49f45b9b5914f7cdff7b3c1",
      "value": 1
     }
    },
    "02a9e4626a0a469aae2fcfc553bb03ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "03f794890d994ee69bc6df6b5b492b1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "048a5c8978964d3289578377d4fcddcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3083e2000af402ab7327b13ad30d106",
      "placeholder": "​",
      "style": "IPY_MODEL_e5322c88c7d041d1879e98dabf67d132",
      "value": "merges.txt: "
     }
    },
    "0d2877ba0b5249c496cde99c510fa020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_048a5c8978964d3289578377d4fcddcd",
       "IPY_MODEL_87b88b35894f4eb5b6b8674e8771fcab",
       "IPY_MODEL_ad134be56c8a4a83987df79a0d14512b"
      ],
      "layout": "IPY_MODEL_52708a194fac47928203f2a4cbef8532"
     }
    },
    "197b4da506aa47db9988d891d3179ad6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e75c53bd40f49ef95f915c82565b7dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2017ae54d2e7498fb88b17e8f4cda26a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20f02aae3cdd469ba7473fc747596080",
       "IPY_MODEL_37cadb113e2d44a7ae91bdaf0c221721",
       "IPY_MODEL_7f243a55281648d88f60755db1a2cd3a"
      ],
      "layout": "IPY_MODEL_290119f0f97f4208bed233eccd9ceed9"
     }
    },
    "20f02aae3cdd469ba7473fc747596080": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdf64b1aeb444fc68e6f2cdf6d6a7694",
      "placeholder": "​",
      "style": "IPY_MODEL_7e0656e70b5a40fcb901c07ea31e630a",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "290119f0f97f4208bed233eccd9ceed9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31773b5049214b048b2c88356c0232cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b2e6b89978e2421a9b4c6e897b10fc6f",
       "IPY_MODEL_011db3b3e5cb4013a5202bba057dc835",
       "IPY_MODEL_e79a1fb2e2354f78abdd3f177106959f"
      ],
      "layout": "IPY_MODEL_aff0faf084d1475db3856dfac9137af3"
     }
    },
    "36f534bdf49f45b9b5914f7cdff7b3c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37cadb113e2d44a7ae91bdaf0c221721": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9795a32d4ec44407a8fc1f9da3bbfec4",
      "max": 501204462,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8292b8a468264145af673d554715b0ff",
      "value": 501204462
     }
    },
    "3ef0b4dd65874af5aba7cfaca1a38b94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52708a194fac47928203f2a4cbef8532": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5458de6e8c7f4a80b0de294e277c4c1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5de3987322414ecaa140416ed53fc9a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "5f024dfb446c4bc29743b0dced15b6e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7403a722bd6c4c7eb1116a60f1869fea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77b8b95191d0448b8aad3516fb4df48c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7daeada907644944a32f7abda1ca1294": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77b8b95191d0448b8aad3516fb4df48c",
      "placeholder": "​",
      "style": "IPY_MODEL_02a9e4626a0a469aae2fcfc553bb03ab",
      "value": " 501M/501M [00:03&lt;00:00, 230MB/s]"
     }
    },
    "7e0656e70b5a40fcb901c07ea31e630a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f243a55281648d88f60755db1a2cd3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aced22573e8a4e5b8e183db46077ccd5",
      "placeholder": "​",
      "style": "IPY_MODEL_3ef0b4dd65874af5aba7cfaca1a38b94",
      "value": " 501M/501M [00:02&lt;00:00, 173MB/s]"
     }
    },
    "8292b8a468264145af673d554715b0ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "866b4656d2614d5eac38e2f77bd31aa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db331bea80d4490f9d31b408d3607333",
      "placeholder": "​",
      "style": "IPY_MODEL_ed281cccf0d841b0b20698e4393552de",
      "value": "config.json: 100%"
     }
    },
    "87b88b35894f4eb5b6b8674e8771fcab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5de3987322414ecaa140416ed53fc9a4",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88d25d74174b4ef1bb5524382df3073d",
      "value": 1
     }
    },
    "88d25d74174b4ef1bb5524382df3073d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9795a32d4ec44407a8fc1f9da3bbfec4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a64b2549b7e4ebbab9144e39579ab45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5458de6e8c7f4a80b0de294e277c4c1c",
      "placeholder": "​",
      "style": "IPY_MODEL_ea4a2856f5794a82b4715594b4ff4fb2",
      "value": "model.safetensors: 100%"
     }
    },
    "9c3b34c425c244cb9529da29a1e8cbcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa22225a8eb7475d83dd64b16cec9e6a",
      "max": 501176510,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9ec59c5fbf124d1baa0934733a2e5410",
      "value": 501176510
     }
    },
    "9ec59c5fbf124d1baa0934733a2e5410": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a3083e2000af402ab7327b13ad30d106": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa22225a8eb7475d83dd64b16cec9e6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac6735ce42894521a0f92cf15cb9889c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aced22573e8a4e5b8e183db46077ccd5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad134be56c8a4a83987df79a0d14512b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7e394de208e46b5bdc63405b8219d26",
      "placeholder": "​",
      "style": "IPY_MODEL_efbd5a1ef7224d5c81aaf3e38f37966b",
      "value": " 456k/? [00:00&lt;00:00, 14.6MB/s]"
     }
    },
    "aff0faf084d1475db3856dfac9137af3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2e6b89978e2421a9b4c6e897b10fc6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac6735ce42894521a0f92cf15cb9889c",
      "placeholder": "​",
      "style": "IPY_MODEL_1e75c53bd40f49ef95f915c82565b7dc",
      "value": "vocab.json: "
     }
    },
    "bdf64b1aeb444fc68e6f2cdf6d6a7694": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c17dd0277609411d94672d5d928d8c36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_866b4656d2614d5eac38e2f77bd31aa5",
       "IPY_MODEL_d5acd8753d084191a201335153c3c885",
       "IPY_MODEL_f82f72f6f1e9489ca0970782019c164d"
      ],
      "layout": "IPY_MODEL_5f024dfb446c4bc29743b0dced15b6e2"
     }
    },
    "c7e394de208e46b5bdc63405b8219d26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cff436214ce34ad581af2a1ba0e51986": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a64b2549b7e4ebbab9144e39579ab45",
       "IPY_MODEL_9c3b34c425c244cb9529da29a1e8cbcf",
       "IPY_MODEL_7daeada907644944a32f7abda1ca1294"
      ],
      "layout": "IPY_MODEL_197b4da506aa47db9988d891d3179ad6"
     }
    },
    "d08ab1ed029440c9ae851f6f9105d848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5acd8753d084191a201335153c3c885": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7403a722bd6c4c7eb1116a60f1869fea",
      "max": 565,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_03f794890d994ee69bc6df6b5b492b1a",
      "value": 565
     }
    },
    "db331bea80d4490f9d31b408d3607333": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5322c88c7d041d1879e98dabf67d132": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e79a1fb2e2354f78abdd3f177106959f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff4d9b0d08004c0bbebbf9b7219caffb",
      "placeholder": "​",
      "style": "IPY_MODEL_d08ab1ed029440c9ae851f6f9105d848",
      "value": " 899k/? [00:00&lt;00:00, 13.7MB/s]"
     }
    },
    "e9d726b492c941fc89a82194b31ee5bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea4a2856f5794a82b4715594b4ff4fb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed281cccf0d841b0b20698e4393552de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efbd5a1ef7224d5c81aaf3e38f37966b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f82f72f6f1e9489ca0970782019c164d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9d726b492c941fc89a82194b31ee5bf",
      "placeholder": "​",
      "style": "IPY_MODEL_00454005c106401db1769318286f9169",
      "value": " 565/565 [00:00&lt;00:00, 32.5kB/s]"
     }
    },
    "fb11b30150f1404083362c2bbc4f4e5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "ff4d9b0d08004c0bbebbf9b7219caffb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
