{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc5af20-8025-457e-a584-4ff17b96b9b3",
   "metadata": {},
   "source": [
    "## BERT fine-tuning + hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c79c7-c075-479e-a748-32489c4caa32",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fDUrIIgpHU",
   "metadata": {
    "id": "76fDUrIIgpHU"
   },
   "outputs": [],
   "source": [
    "!pip install -q optuna\n",
    "!pip install -q evaluate\n",
    "!pip install -q emoji==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8caf727",
   "metadata": {
    "id": "d8caf727"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41bbac45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41bbac45",
    "outputId": "40cd524a-e104-43e6-d6ea-560a85247782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244d7e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5244d7e5",
    "outputId": "9b5e08e6-1e08-4f33-fd08-ca3740d8dc6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmayachn3\u001b[0m (\u001b[33mmayachn3-maya-bondar\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"<wandb key>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ff6c8-e202-48dc-a25b-5209af2fbc30",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a0a73c",
   "metadata": {
    "id": "d3a0a73c"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"OOT_train.csv\", encoding='latin-1')\n",
    "val = pd.read_csv(\"OOT_val.csv\", encoding='latin-1')\n",
    "test = pd.read_csv(\"OOT_test.csv\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38cadc1a",
   "metadata": {
    "id": "38cadc1a"
   },
   "outputs": [],
   "source": [
    "# train = train.head(1000)\n",
    "# val = val.head(1000)\n",
    "# test = test.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687deaa",
   "metadata": {
    "id": "4687deaa"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9966ecc5",
   "metadata": {
    "id": "9966ecc5"
   },
   "outputs": [],
   "source": [
    "#encoding the labels numerically from Sentiment\n",
    "\n",
    "ordinal_mapping = {\n",
    "    'Extremely Negative': 0,\n",
    "    'Negative': 1,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 3,\n",
    "    'Extremely Positive': 4\n",
    "}\n",
    "\n",
    "# map to ordinal labels\n",
    "train[\"ordinal_label_id\"] = train[\"Sentiment\"].map(ordinal_mapping)\n",
    "val[\"ordinal_label_id\"] = val[\"Sentiment\"].map(ordinal_mapping)\n",
    "test[\"ordinal_label_id\"] = test[\"Sentiment\"].map(ordinal_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b78e181",
   "metadata": {
    "id": "2b78e181"
   },
   "outputs": [],
   "source": [
    "# Concat the relevant columns into one string with seperation.\n",
    "# for example: \"Tweet: my food stock is low | Location: Canada | Date: 2020-03-17 | URL: https://t.co/abcd\"\n",
    "\n",
    "# Function to build the input string from multiple columns\n",
    "def build_augmented_input(row):\n",
    "    parts = []\n",
    "\n",
    "    if pd.notna(row.get('clean_tweet')):\n",
    "        parts.append(f\"{row['clean_tweet']}\")\n",
    "\n",
    "    if pd.notna(row.get('Location_standardized')) and row['Location_standardized'].lower() != 'unknown':\n",
    "        parts.append(f\"{row['Location_standardized']}\")\n",
    "\n",
    "    if pd.notna(row.get('TweetAt')):\n",
    "        parts.append(f\"{row['TweetAt']}\")\n",
    "\n",
    "\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "# Apply to the DataFrames\n",
    "train['model_input'] = train.apply(build_augmented_input, axis=1)\n",
    "val['model_input'] = val.apply(build_augmented_input, axis=1)\n",
    "test['model_input'] = test.apply(build_augmented_input, axis=1)\n",
    "\n",
    "# Create  new DataFrames with only what's needed for modeling\n",
    "formatted_train = train[['model_input', 'ordinal_label_id']].copy()\n",
    "formatted_val = val[['model_input', 'ordinal_label_id']].copy()\n",
    "formatted_test = test[['model_input', 'ordinal_label_id']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9uFy-91QYhCT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uFy-91QYhCT",
    "outputId": "ae0e5cd4-157b-43e7-eb0a-ebd3ae4543f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "ordinal_label_id\n",
      "0     5175\n",
      "1     9230\n",
      "2     6784\n",
      "3    10140\n",
      "4     5845\n",
      "Name: count, dtype: int64\n",
      "Class 0: 5000 samples (undersampled)\n",
      "Class 1: 5000 samples (undersampled)\n",
      "Class 2: 5000 samples (undersampled)\n",
      "Class 3: 5000 samples (undersampled)\n",
      "Class 4: 5000 samples (undersampled)\n",
      "Balanced dataset: 25000 total samples\n",
      "New distribution:\n",
      "ordinal_label_id\n",
      "0    5000\n",
      "1    5000\n",
      "2    5000\n",
      "3    5000\n",
      "4    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def balance_dataset(df, target_samples_per_class=5000):\n",
    "    \"\"\"Balance dataset by undersampling\"\"\"\n",
    "    balanced_dfs = []\n",
    "\n",
    "    print(\"Original class distribution:\")\n",
    "    print(df['ordinal_label_id'].value_counts().sort_index())\n",
    "\n",
    "    for class_id in range(5):\n",
    "        class_data = df[df['ordinal_label_id'] == class_id]\n",
    "\n",
    "        if len(class_data) > target_samples_per_class:\n",
    "            class_data = class_data.sample(n=target_samples_per_class, random_state=42)\n",
    "            print(f\"Class {class_id}: {len(class_data)} samples (undersampled)\")\n",
    "        else:\n",
    "            print(f\"Class {class_id}: {len(class_data)} samples (kept all)\")\n",
    "\n",
    "        balanced_dfs.append(class_data)\n",
    "\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "    print(f\"Balanced dataset: {len(balanced_df)} total samples\")\n",
    "    print(\"New distribution:\")\n",
    "    print(balanced_df['ordinal_label_id'].value_counts().sort_index())\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "# Apply balancing to training data\n",
    "formatted_train = balance_dataset(formatted_train, target_samples_per_class=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13dad1",
   "metadata": {
    "id": "6c13dad1"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d0d69-3648-4840-a6a4-30de7d3f59a3",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "641d56a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "641d56a1",
    "outputId": "828e6ed0-a3d0-460e-d95c-dde4e5771c6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_data(data, max_length=128):\n",
    "    return tokenizer(\n",
    "        data['model_input'].tolist(),\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(formatted_train)\n",
    "val_encodings = tokenize_data(formatted_val)\n",
    "test_encodings = tokenize_data(formatted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "022a3ee0",
   "metadata": {
    "id": "022a3ee0"
   },
   "outputs": [],
   "source": [
    "## define a PyTorch Dataset\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels  # Should be integers\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])  # For training\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert labels to integers if not already\n",
    "train_labels = formatted_train['ordinal_label_id'].tolist()\n",
    "val_labels = formatted_val['ordinal_label_id'].tolist()\n",
    "test_labels = formatted_test['ordinal_label_id'].tolist()\n",
    "\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset = TweetDataset(val_encodings, val_labels)\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a74d9d",
   "metadata": {
    "id": "27a74d9d"
   },
   "outputs": [],
   "source": [
    "# define mapping between label id and sentiment for later use and conveniency\n",
    "ordinal_label2id = ordinal_mapping\n",
    "ordinal_id2label = {v: k for k, v in ordinal_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c673006c",
   "metadata": {
    "id": "c673006c"
   },
   "outputs": [],
   "source": [
    "def compute_detailed_metrics(eval_pred):\n",
    "    \"\"\"Enhanced metrics using HuggingFace Evaluate library\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Load HuggingFace metrics (cached after first load)\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "    # Compute standard classification metrics\n",
    "    results = {}\n",
    "\n",
    "    # Basic metrics\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average='macro'))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average='weighted'))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average='macro'))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average='macro'))\n",
    "\n",
    "    # Per-class F1 scores (HF doesn't have this built-in, so keep custom)\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "    for i, class_name in enumerate(['extremely_negative', 'negative', 'neutral', 'positive', 'extremely_positive']):\n",
    "        results[f'f1_{class_name}'] = f1_per_class[i]\n",
    "\n",
    "        # Per-class precision and recall\n",
    "        precision_per_class = precision_score(labels, predictions, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(labels, predictions, average=None, zero_division=0)\n",
    "        results[f'precision_{class_name}'] = precision_per_class[i]\n",
    "        results[f'recall_{class_name}'] = recall_per_class[i]\n",
    "\n",
    "        # Per-class accuracy\n",
    "        class_mask = (labels == i)\n",
    "        if class_mask.sum() > 0:\n",
    "            results[f'accuracy_{class_name}'] = accuracy_score(labels[class_mask], predictions[class_mask])\n",
    "        else:\n",
    "            results[f'accuracy_{class_name}'] = 0.0\n",
    "\n",
    "    # Custom ordinal metrics (HF doesn't have these)\n",
    "    results['mae'] = np.mean(np.abs(predictions - labels))\n",
    "    results['adjacent_accuracy'] = np.sum(np.abs(predictions - labels) <= 1) / len(labels)\n",
    "\n",
    "    # Quadratic Weighted Kappa (custom)\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    try:\n",
    "        qwk = cohen_kappa_score(labels, predictions, weights='quadratic')\n",
    "        results['quadratic_weighted_kappa'] = qwk\n",
    "    except:\n",
    "        results['quadratic_weighted_kappa'] = 0.0\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b58598b",
   "metadata": {
    "id": "9b58598b"
   },
   "outputs": [],
   "source": [
    "def find_optimal_batch_size(base_batch_size):\n",
    "    \"\"\"Find the largest batch size that fits in GPU memory\"\"\"\n",
    "    if device.type == \"cpu\":\n",
    "        return base_batch_size\n",
    "\n",
    "    # Try larger batch sizes for GPU\n",
    "    for multiplier in [4, 3, 2, 1]:\n",
    "        try_batch_size = base_batch_size * multiplier\n",
    "        try:\n",
    "            # Test if this batch size fits\n",
    "            dummy_input = torch.randn(try_batch_size, 128, 768, device=device)\n",
    "            dummy_output = torch.randn(try_batch_size, 5, device=device)\n",
    "            del dummy_input, dummy_output\n",
    "            torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
    "            return try_batch_size\n",
    "        except RuntimeError:  # Out of memory\n",
    "            continue\n",
    "    return base_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "Td0xg4gjBac4",
   "metadata": {
    "id": "Td0xg4gjBac4"
   },
   "outputs": [],
   "source": [
    "class SimpleMetricsLogger(TrainerCallback):\n",
    "    \"\"\"Simple callback to log detailed metrics every epoch\"\"\"\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None or not wandb.run:\n",
    "            return\n",
    "\n",
    "        # Only log when we have evaluation metrics (after each epoch)\n",
    "        if 'eval_loss' in logs:\n",
    "            current_epoch = int(state.epoch)\n",
    "\n",
    "            # Get current learning rate\n",
    "            current_lr = args.learning_rate\n",
    "            if 'learning_rate' in logs:\n",
    "                current_lr = logs['learning_rate']\n",
    "\n",
    "            #Get training loss from state history\n",
    "            train_loss = 0\n",
    "            if state.log_history:\n",
    "                # Find the most recent training loss\n",
    "                for log_entry in reversed(state.log_history):\n",
    "                    if 'train_loss' in log_entry:\n",
    "                        train_loss = log_entry['train_loss']\n",
    "                        break\n",
    "\n",
    "            detailed_metrics = {\n",
    "                \"Epoch\": current_epoch,\n",
    "                \"Stage\": 1,\n",
    "                \"Unfrozen_Layers\": 12,\n",
    "                \"Train Loss\": train_loss,\n",
    "                \"Train Accuracy\": 0,  # Usually not computed during training\n",
    "                \"Validation Loss\": logs.get('eval_loss', 0),\n",
    "                \"Validation Accuracy\": logs.get('eval_accuracy', 0),\n",
    "                \"Validation Precision\": logs.get('eval_precision_macro', 0),\n",
    "                \"Validation Recall\": logs.get('eval_recall_macro', 0),\n",
    "                \"Validation F1\": logs.get('eval_f1_macro', 0),\n",
    "                \"Validation MAE\": logs.get('eval_mae', 0),\n",
    "                \"Validation Adjacent Accuracy\": logs.get('eval_adjacent_accuracy', 0),\n",
    "                \"Validation QWK\": logs.get('eval_quadratic_weighted_kappa', 0),\n",
    "                \"Learning_Rate\": current_lr,\n",
    "            }\n",
    "\n",
    "            # Log to WandB\n",
    "            wandb.log(detailed_metrics)\n",
    "\n",
    "            #Print progress to console\n",
    "            print(f\"Epoch {current_epoch}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, \"  # ‚Üê Now shows real values\n",
    "                  f\"Val Loss: {logs.get('eval_loss', 0):.4f}, \"\n",
    "                  f\"Val F1: {logs.get('eval_f1_macro', 0):.4f}, \"\n",
    "                  f\"QWK: {logs.get('eval_quadratic_weighted_kappa', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "qHQHdNiVKd4P",
   "metadata": {
    "id": "qHQHdNiVKd4P"
   },
   "outputs": [],
   "source": [
    "def save_training_checkpoint(model, optimizer, epoch, loss, trial_params, filepath, trial_number, current_score, trainer):\n",
    "    \"\"\"Save complete training checkpoint and handle best model updates\"\"\"\n",
    "    global best_score, best_model_path\n",
    "\n",
    "    # Get the trial directory from filepath\n",
    "    trial_dir = os.path.dirname(filepath)\n",
    "\n",
    "    # Save trial checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        # 'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'trial_params': trial_params,\n",
    "        'model_config': model.config.to_dict(),\n",
    "        'tokenizer_name': model_name,\n",
    "        'current_score': current_score,  # Add score to checkpoint\n",
    "        'trial_number': trial_number,\n",
    "    }\n",
    "\n",
    "    # Save all trial files in the same directory\n",
    "    torch.save(checkpoint, filepath)\n",
    "    torch.save(model.state_dict(), os.path.join(trial_dir, 'model_bert_weights.pt'))\n",
    "    torch.save(model, os.path.join(trial_dir, 'model_bert.pt'))\n",
    "\n",
    "    print(f\"‚úÖ Trial checkpoint saved: {filepath}\")\n",
    "    print(f\"‚úÖ Model files saved in: {trial_dir}\")\n",
    "\n",
    "    # Update best model if needed\n",
    "    if current_score > best_score:\n",
    "        best_score = current_score\n",
    "\n",
    "        # Save HuggingFace format to best model directory\n",
    "        trainer.save_model(best_model_path)\n",
    "\n",
    "        # Also save our custom format in best model directory\n",
    "        os.makedirs(best_model_path, exist_ok=True)\n",
    "        best_checkpoint_path = os.path.join(best_model_path, 'best_checkpoint.ckpt')\n",
    "        best_weights_path = os.path.join(best_model_path, 'model_bert_weights.pt')\n",
    "        best_model_file_path = os.path.join(best_model_path, 'model_bert.pt')\n",
    "\n",
    "        torch.save(checkpoint, best_checkpoint_path)\n",
    "        torch.save(model.state_dict(), best_weights_path)\n",
    "        torch.save(model, best_model_file_path)\n",
    "\n",
    "        print(f\"üèÜ New best model saved! Score: {current_score:.4f} (Trial {trial_number})\")\n",
    "        print(f\"üèÜ Best model files saved in: {best_model_path}\")\n",
    "\n",
    "        # Optional: Log to W&B\n",
    "        # wandb.log({\n",
    "        #     \"best_score_so_far\": current_score,\n",
    "        #     \"best_trial_number\": trial_number,\n",
    "        # })\n",
    "    else:\n",
    "        print(f\"üìä Trial {trial_number} score: {current_score:.4f} (Best: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68de4939",
   "metadata": {
    "id": "68de4939"
   },
   "outputs": [],
   "source": [
    "# Global variables to track best model\n",
    "best_score = 0.0\n",
    "best_model_path = \"./best_bert_model_so_far\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563898a1-8c7f-4db8-a17c-eaab596442ea",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a177f371",
   "metadata": {
    "id": "a177f371"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Clean, organized objective function for Optuna hyperparameter optimization\"\"\"\n",
    "    global best_score, best_model_path\n",
    "\n",
    "    # === GPU MEMORY CLEANUP ===\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # === HYPERPARAMETER SAMPLING ===\n",
    "    # Core training parameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 3e-5, 5e-4, log=True)\n",
    "    base_batch_size = trial.suggest_categorical(\"batch_size\", [16,32,64])\n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.05, 0.15)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 10, 15)\n",
    "\n",
    "    # Advanced optimization parameters\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.15)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.05, 0.15)\n",
    "\n",
    "    # Model architecture parameters\n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0.3, 0.4)\n",
    "    hidden_dropout = trial.suggest_float(\"hidden_dropout\", 0.3, 0.4)\n",
    "\n",
    "    #R-drop parameter\n",
    "    # rdrop_alpha = trial.suggest_float(\"rdrop_alpha\", 0.0, 1.0)\n",
    "\n",
    "    # PRINT CHOSEN PARAMETERS\n",
    "    print(f\"TRIAL {trial.number} - TESTING THESE PARAMETERS:\")\n",
    "    print(f\"Learning Rate:      {learning_rate:.2e}\")\n",
    "    print(f\"Epochs:             {num_epochs}\")\n",
    "    print(f\"Warmup Ratio:       {warmup_ratio:.3f}\")\n",
    "    print(f\"Weight Decay:       {weight_decay:.3f}\")\n",
    "    print(f\"Attention Dropout:  {attention_dropout:.3f}\")\n",
    "    print(f\"Hidden Dropout:     {hidden_dropout:.3f}\")\n",
    "\n",
    "\n",
    "    # Optimize batch size for available hardware\n",
    "    batch_size = find_optimal_batch_size(base_batch_size)\n",
    "\n",
    "\n",
    "    # === EXPERIMENT TRACKING SETUP ===\n",
    "    wandb.init(\n",
    "        project=\"covid-tweet-sentiment-hf-bert-regularloss\",\n",
    "        name=f\"trial_{trial.number}\",\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"attention_dropout\": attention_dropout,\n",
    "            \"hidden_dropout\": hidden_dropout\n",
    "            # \"rdrop_alpha\": rdrop_alpha ,\n",
    "        },\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # === MODEL SETUP ===\n",
    "        model = _setup_model(attention_dropout, hidden_dropout)\n",
    "\n",
    "        # === TRAINING CONFIGURATION ===\n",
    "        training_args = _create_training_args(\n",
    "            trial_number=trial.number,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            label_smoothing_factor=label_smoothing\n",
    "        )\n",
    "\n",
    "        trial_params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"attention_dropout\": attention_dropout,\n",
    "            \"hidden_dropout\": hidden_dropout\n",
    "        }\n",
    "\n",
    "        trainer = _create_trainer(model, training_args, trial.number, trial_params)\n",
    "        trainer.train()\n",
    "\n",
    "        # Set Checkpoint per trial\n",
    "        checkpoint_dir = f\"./checkpoints_bert/trial_{trial.number}\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        final_epoch = int(trainer.state.epoch)\n",
    "        checkpoint_path = f\"{checkpoint_dir}/final_epoch_{final_epoch}.ckpt\"\n",
    "\n",
    "        # Get final training loss\n",
    "        final_loss = 0\n",
    "        if trainer.state.log_history:\n",
    "            for log_entry in reversed(trainer.state.log_history):\n",
    "                if 'train_loss' in log_entry:\n",
    "                    final_loss = log_entry['train_loss']\n",
    "                    break\n",
    "\n",
    "        eval_results = trainer.evaluate()\n",
    "        current_score = eval_results[\"eval_quadratic_weighted_kappa\"]\n",
    "\n",
    "        save_training_checkpoint(\n",
    "            model=trainer.model,\n",
    "            optimizer=None,\n",
    "            epoch=final_epoch,\n",
    "            loss=final_loss,\n",
    "            trial_params=trial_params,\n",
    "            filepath=checkpoint_path,\n",
    "            trial_number=trial.number,\n",
    "            current_score=current_score,\n",
    "            trainer=trainer\n",
    "        )\n",
    "\n",
    "        # Log GPU usage if available\n",
    "        if device.type == \"cuda\":\n",
    "            print(f\"GPU Memory Used: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "        return current_score\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    finally:\n",
    "        # === CLEANUP ===\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "def _setup_model(attention_dropout, hidden_dropout):\n",
    "    \"\"\"Setup and configure the model with dropout and freezing\"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "        num_labels=5,\n",
    "        id2label=ordinal_id2label,\n",
    "        label2id=ordinal_label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    # Apply dropout configuration\n",
    "    model.config.attention_probs_dropout_prob = attention_dropout\n",
    "    model.config.hidden_dropout_prob = hidden_dropout\n",
    "\n",
    "    # GPU optimizations\n",
    "    if device.type == \"cuda\":\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _create_training_args(trial_number, learning_rate, batch_size, num_epochs, warmup_ratio, weight_decay, label_smoothing_factor=0.1):\n",
    "    \"\"\"Create optimized training arguments\"\"\"\n",
    "    return TrainingArguments(\n",
    "        # output_dir=f\"./results/trial_{trial_number}\",\n",
    "\n",
    "        # Core training parameters\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size * 2,  # Larger eval batch\n",
    "        learning_rate=learning_rate,\n",
    "        label_smoothing_factor=label_smoothing_factor,\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=warmup_ratio,\n",
    "\n",
    "        # Optimization\n",
    "        optim=\"adamw_torch\",\n",
    "        weight_decay=weight_decay,\n",
    "        max_grad_norm=1.0,\n",
    "\n",
    "        # Evaluation and saving\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        save_total_limit=1,\n",
    "        # load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_quadratic_weighted_kappa\",\n",
    "        greater_is_better=True,\n",
    "\n",
    "        # Performance optimizations\n",
    "        fp16=device.type == \"cuda\",\n",
    "        # tf32=device.type == \"cuda\",\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_persistent_workers=device.type == \"cuda\",\n",
    "        dataloader_num_workers=2 if device.type == \"cuda\" else 0,\n",
    "        dataloader_drop_last=False,\n",
    "        group_by_length=True,\n",
    "        gradient_accumulation_steps=1,\n",
    "        dataloader_prefetch_factor=2 if device.type == \"cuda\" else None,\n",
    "\n",
    "        # Logging\n",
    "        logging_steps=100,\n",
    "        report_to=\"wandb\",\n",
    "        remove_unused_columns=False,\n",
    "\n",
    "        # Evaluation optimizations\n",
    "        eval_accumulation_steps=None,\n",
    "        prediction_loss_only=False,\n",
    "\n",
    "    )\n",
    "\n",
    "def _create_trainer(model, training_args, trial_number, trial_params):\n",
    "    \"\"\"ordinal loss trainer\"\"\"\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStoppingCallback(early_stopping_patience=2),\n",
    "        SimpleMetricsLogger(),\n",
    "        ]\n",
    "\n",
    "    return Trainer(  # Uses ordinal loss always\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_detailed_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=callbacks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3_Vh9-hdl-HE",
   "metadata": {
    "id": "3_Vh9-hdl-HE"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def save_best_hyperparameters(study, model_name=\"bertweet_base\"):\n",
    "    \"\"\"Save the best hyperparameters found by Optuna\"\"\"\n",
    "\n",
    "    # Check if any trial completed successfully\n",
    "    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    if not completed_trials:\n",
    "        print(\"No completed trials found. Skipping saving best hyperparameters.\")\n",
    "        return None\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value\n",
    "\n",
    "    # Save to JSON file\n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"best_score\": best_score,\n",
    "        \"best_params\": best_params,\n",
    "        \"timestamp\": str(datetime.now()),\n",
    "        \"total_trials\": len(study.trials),\n",
    "        \"completed_trials\": len(completed_trials)\n",
    "    }\n",
    "\n",
    "    filename = f\"best_params_{model_name}.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"Best hyperparameters saved to {filename}\")\n",
    "    print(f\"Best score: {best_score:.4f}\")\n",
    "    print(f\"Best params: {best_params}\")\n",
    "    print(f\"Completed {len(completed_trials)}/{len(study.trials)} trials\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L_B3GAlccXSV",
   "metadata": {
    "id": "L_B3GAlccXSV"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07eb100c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "07eb100c",
    "outputId": "ca222b45-d2aa-482c-d022-74df2ed53156"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 07:39:41,030] A new study created in memory with name: no-name-13c69f66-8008-465f-bdf4-85ddf2eaf009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 0 - TESTING THESE PARAMETERS:\n",
      "Learning Rate:      3.36e-05\n",
      "Epochs:             12\n",
      "Warmup Ratio:       0.079\n",
      "Weight Decay:       0.077\n",
      "Attention Dropout:  0.301\n",
      "Hidden Dropout:     0.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_073941-239xw040</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/239xw040' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/239xw040' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/239xw040</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1318007737.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(  # Uses ordinal loss always\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3128' max='4692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3128/4692 12:02 < 06:01, 4.33 it/s, Epoch 8/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Extremely Negative</th>\n",
       "      <th>Precision Extremely Negative</th>\n",
       "      <th>Recall Extremely Negative</th>\n",
       "      <th>Accuracy Extremely Negative</th>\n",
       "      <th>F1 Negative</th>\n",
       "      <th>Precision Negative</th>\n",
       "      <th>Recall Negative</th>\n",
       "      <th>Accuracy Negative</th>\n",
       "      <th>F1 Neutral</th>\n",
       "      <th>Precision Neutral</th>\n",
       "      <th>Recall Neutral</th>\n",
       "      <th>Accuracy Neutral</th>\n",
       "      <th>F1 Positive</th>\n",
       "      <th>Precision Positive</th>\n",
       "      <th>Recall Positive</th>\n",
       "      <th>Accuracy Positive</th>\n",
       "      <th>F1 Extremely Positive</th>\n",
       "      <th>Precision Extremely Positive</th>\n",
       "      <th>Recall Extremely Positive</th>\n",
       "      <th>Accuracy Extremely Positive</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Adjacent Accuracy</th>\n",
       "      <th>Quadratic Weighted Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.128500</td>\n",
       "      <td>1.017624</td>\n",
       "      <td>0.711269</td>\n",
       "      <td>0.711390</td>\n",
       "      <td>0.743204</td>\n",
       "      <td>0.712159</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.666316</td>\n",
       "      <td>0.641700</td>\n",
       "      <td>0.692896</td>\n",
       "      <td>0.692896</td>\n",
       "      <td>0.771612</td>\n",
       "      <td>0.734010</td>\n",
       "      <td>0.813273</td>\n",
       "      <td>0.813273</td>\n",
       "      <td>0.683327</td>\n",
       "      <td>0.651494</td>\n",
       "      <td>0.718431</td>\n",
       "      <td>0.718431</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.575949</td>\n",
       "      <td>0.575949</td>\n",
       "      <td>0.359422</td>\n",
       "      <td>0.934588</td>\n",
       "      <td>0.833697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.911400</td>\n",
       "      <td>0.890793</td>\n",
       "      <td>0.798715</td>\n",
       "      <td>0.798142</td>\n",
       "      <td>0.795248</td>\n",
       "      <td>0.815284</td>\n",
       "      <td>0.795160</td>\n",
       "      <td>0.687593</td>\n",
       "      <td>0.942623</td>\n",
       "      <td>0.942623</td>\n",
       "      <td>0.719491</td>\n",
       "      <td>0.764128</td>\n",
       "      <td>0.679781</td>\n",
       "      <td>0.679781</td>\n",
       "      <td>0.851088</td>\n",
       "      <td>0.866978</td>\n",
       "      <td>0.835771</td>\n",
       "      <td>0.835771</td>\n",
       "      <td>0.785091</td>\n",
       "      <td>0.793905</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.841772</td>\n",
       "      <td>0.841772</td>\n",
       "      <td>0.249943</td>\n",
       "      <td>0.956162</td>\n",
       "      <td>0.894932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.778300</td>\n",
       "      <td>0.852766</td>\n",
       "      <td>0.817535</td>\n",
       "      <td>0.818170</td>\n",
       "      <td>0.839895</td>\n",
       "      <td>0.803928</td>\n",
       "      <td>0.788372</td>\n",
       "      <td>0.911290</td>\n",
       "      <td>0.694672</td>\n",
       "      <td>0.694672</td>\n",
       "      <td>0.772679</td>\n",
       "      <td>0.754953</td>\n",
       "      <td>0.791257</td>\n",
       "      <td>0.791257</td>\n",
       "      <td>0.867991</td>\n",
       "      <td>0.902795</td>\n",
       "      <td>0.835771</td>\n",
       "      <td>0.835771</td>\n",
       "      <td>0.802651</td>\n",
       "      <td>0.756419</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.858247</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.843038</td>\n",
       "      <td>0.843038</td>\n",
       "      <td>0.226303</td>\n",
       "      <td>0.958228</td>\n",
       "      <td>0.898681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.725200</td>\n",
       "      <td>0.895241</td>\n",
       "      <td>0.810879</td>\n",
       "      <td>0.810341</td>\n",
       "      <td>0.816922</td>\n",
       "      <td>0.824504</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.860656</td>\n",
       "      <td>0.860656</td>\n",
       "      <td>0.775185</td>\n",
       "      <td>0.808789</td>\n",
       "      <td>0.744262</td>\n",
       "      <td>0.744262</td>\n",
       "      <td>0.872226</td>\n",
       "      <td>0.934447</td>\n",
       "      <td>0.817773</td>\n",
       "      <td>0.817773</td>\n",
       "      <td>0.769604</td>\n",
       "      <td>0.793995</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.832965</td>\n",
       "      <td>0.739686</td>\n",
       "      <td>0.953165</td>\n",
       "      <td>0.953165</td>\n",
       "      <td>0.230663</td>\n",
       "      <td>0.960982</td>\n",
       "      <td>0.906755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>0.938338</td>\n",
       "      <td>0.796420</td>\n",
       "      <td>0.795675</td>\n",
       "      <td>0.805892</td>\n",
       "      <td>0.815249</td>\n",
       "      <td>0.836257</td>\n",
       "      <td>0.797398</td>\n",
       "      <td>0.879098</td>\n",
       "      <td>0.879098</td>\n",
       "      <td>0.775652</td>\n",
       "      <td>0.825926</td>\n",
       "      <td>0.731148</td>\n",
       "      <td>0.731148</td>\n",
       "      <td>0.856624</td>\n",
       "      <td>0.926702</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.742834</td>\n",
       "      <td>0.777207</td>\n",
       "      <td>0.711373</td>\n",
       "      <td>0.711373</td>\n",
       "      <td>0.810493</td>\n",
       "      <td>0.702226</td>\n",
       "      <td>0.958228</td>\n",
       "      <td>0.958228</td>\n",
       "      <td>0.243516</td>\n",
       "      <td>0.963277</td>\n",
       "      <td>0.904482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.640100</td>\n",
       "      <td>0.867103</td>\n",
       "      <td>0.835437</td>\n",
       "      <td>0.835753</td>\n",
       "      <td>0.833765</td>\n",
       "      <td>0.842702</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.780919</td>\n",
       "      <td>0.905738</td>\n",
       "      <td>0.905738</td>\n",
       "      <td>0.776087</td>\n",
       "      <td>0.771892</td>\n",
       "      <td>0.780328</td>\n",
       "      <td>0.780328</td>\n",
       "      <td>0.854848</td>\n",
       "      <td>0.872365</td>\n",
       "      <td>0.838020</td>\n",
       "      <td>0.838020</td>\n",
       "      <td>0.837998</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>0.833725</td>\n",
       "      <td>0.833725</td>\n",
       "      <td>0.877922</td>\n",
       "      <td>0.901333</td>\n",
       "      <td>0.855696</td>\n",
       "      <td>0.855696</td>\n",
       "      <td>0.193023</td>\n",
       "      <td>0.972688</td>\n",
       "      <td>0.923897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.823502</td>\n",
       "      <td>0.822853</td>\n",
       "      <td>0.815695</td>\n",
       "      <td>0.837953</td>\n",
       "      <td>0.804233</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.934426</td>\n",
       "      <td>0.934426</td>\n",
       "      <td>0.745958</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.706011</td>\n",
       "      <td>0.706011</td>\n",
       "      <td>0.861853</td>\n",
       "      <td>0.871264</td>\n",
       "      <td>0.852643</td>\n",
       "      <td>0.852643</td>\n",
       "      <td>0.822437</td>\n",
       "      <td>0.847049</td>\n",
       "      <td>0.799216</td>\n",
       "      <td>0.799216</td>\n",
       "      <td>0.880199</td>\n",
       "      <td>0.863581</td>\n",
       "      <td>0.897468</td>\n",
       "      <td>0.897468</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.970163</td>\n",
       "      <td>0.918732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.904674</td>\n",
       "      <td>0.826027</td>\n",
       "      <td>0.825156</td>\n",
       "      <td>0.830274</td>\n",
       "      <td>0.827575</td>\n",
       "      <td>0.830472</td>\n",
       "      <td>0.871622</td>\n",
       "      <td>0.793033</td>\n",
       "      <td>0.793033</td>\n",
       "      <td>0.781232</td>\n",
       "      <td>0.809133</td>\n",
       "      <td>0.755191</td>\n",
       "      <td>0.755191</td>\n",
       "      <td>0.840737</td>\n",
       "      <td>0.810867</td>\n",
       "      <td>0.872891</td>\n",
       "      <td>0.872891</td>\n",
       "      <td>0.816294</td>\n",
       "      <td>0.831570</td>\n",
       "      <td>0.801569</td>\n",
       "      <td>0.801569</td>\n",
       "      <td>0.869513</td>\n",
       "      <td>0.828179</td>\n",
       "      <td>0.915190</td>\n",
       "      <td>0.915190</td>\n",
       "      <td>0.200367</td>\n",
       "      <td>0.975442</td>\n",
       "      <td>0.920538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0000, Val Loss: 1.0176, Val F1: 0.0000, QWK: 0.8337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0000, Val Loss: 0.8908, Val F1: 0.0000, QWK: 0.8949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0000, Val Loss: 0.8528, Val F1: 0.0000, QWK: 0.8987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0000, Val Loss: 0.8952, Val F1: 0.0000, QWK: 0.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0000, Val Loss: 0.9383, Val F1: 0.0000, QWK: 0.9045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0000, Val Loss: 0.8671, Val F1: 0.0000, QWK: 0.9239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0000, Val Loss: 0.9175, Val F1: 0.0000, QWK: 0.9187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0000, Val Loss: 0.9047, Val F1: 0.0000, QWK: 0.9205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.7764, Val Loss: 0.9047, Val F1: 0.0000, QWK: 0.9205\n",
      "‚úÖ Trial checkpoint saved: ./checkpoints_bert/trial_0/final_epoch_8.ckpt\n",
      "‚úÖ Model files saved in: ./checkpoints_bert/trial_0\n",
      "üèÜ New best model saved! Score: 0.9205 (Trial 0)\n",
      "üèÜ Best model files saved in: ./best_bert_model_so_far\n",
      "GPU Memory Used: 2.57 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>Learning_Rate</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Stage</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>Unfrozen_Layers</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>Validation Adjacent Accuracy</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÉ</td></tr><tr><td>Validation MAE</td><td>‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation QWK</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>‚ñÉ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÑ‚ñÑ</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>eval/accuracy_negative</td><td>‚ñÇ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÜ</td></tr><tr><td>eval/accuracy_neutral</td><td>‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/accuracy_positive</td><td>‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>eval/adjacent_accuracy</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/f1_extremely_negative</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñÖ‚ñá‚ñá</td></tr><tr><td>eval/f1_extremely_positive</td><td>‚ñÅ‚ñá‚ñá‚ñÜ‚ñÖ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_negative</td><td>‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/f1_neutral</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ</td></tr><tr><td>eval/f1_positive</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÉ</td></tr><tr><td>eval/mae</td><td>‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/precision_extremely_negative</td><td>‚ñÉ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñá‚ñá</td></tr><tr><td>eval/precision_extremely_positive</td><td>‚ñà‚ñÜ‚ñÜ‚ñÇ‚ñÅ‚ñá‚ñÜ‚ñÖ‚ñÖ</td></tr><tr><td>eval/precision_negative</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>eval/precision_neutral</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ</td></tr><tr><td>eval/precision_positive</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>eval/recall_extremely_negative</td><td>‚ñÉ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÑ‚ñÑ</td></tr><tr><td>eval/recall_extremely_positive</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>eval/recall_negative</td><td>‚ñÇ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÜ</td></tr><tr><td>eval/recall_neutral</td><td>‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/recall_positive</td><td>‚ñÅ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>eval/runtime</td><td>‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñÅ‚ñÉ‚ñÉ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÑ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÉ‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÑ‚ñÅ‚ñá‚ñÖ‚ñá‚ñÉ‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñÉ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>8</td></tr><tr><td>Learning_Rate</td><td>3e-05</td></tr><tr><td>Stage</td><td>1</td></tr><tr><td>Train Accuracy</td><td>0</td></tr><tr><td>Train Loss</td><td>0.77637</td></tr><tr><td>Unfrozen_Layers</td><td>12</td></tr><tr><td>Validation Accuracy</td><td>0.82603</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.97544</td></tr><tr><td>Validation F1</td><td>0</td></tr><tr><td>Validation Loss</td><td>0.90467</td></tr><tr><td>Validation MAE</td><td>0.20037</td></tr><tr><td>Validation Precision</td><td>0</td></tr><tr><td>Validation QWK</td><td>0.92054</td></tr><tr><td>Validation Recall</td><td>0</td></tr><tr><td>eval/accuracy</td><td>0.82603</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>0.79303</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>0.91519</td></tr><tr><td>eval/accuracy_negative</td><td>0.75519</td></tr><tr><td>eval/accuracy_neutral</td><td>0.87289</td></tr><tr><td>eval/accuracy_positive</td><td>0.80157</td></tr><tr><td>eval/adjacent_accuracy</td><td>0.97544</td></tr><tr><td>eval/f1</td><td>0.82516</td></tr><tr><td>eval/f1_extremely_negative</td><td>0.83047</td></tr><tr><td>eval/f1_extremely_positive</td><td>0.86951</td></tr><tr><td>eval/f1_negative</td><td>0.78123</td></tr><tr><td>eval/f1_neutral</td><td>0.84074</td></tr><tr><td>eval/f1_positive</td><td>0.81629</td></tr><tr><td>eval/loss</td><td>0.90467</td></tr><tr><td>eval/mae</td><td>0.20037</td></tr><tr><td>eval/precision</td><td>0.83027</td></tr><tr><td>eval/precision_extremely_negative</td><td>0.87162</td></tr><tr><td>eval/precision_extremely_positive</td><td>0.82818</td></tr><tr><td>eval/precision_negative</td><td>0.80913</td></tr><tr><td>eval/precision_neutral</td><td>0.81087</td></tr><tr><td>eval/precision_positive</td><td>0.83157</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>0.92054</td></tr><tr><td>eval/recall</td><td>0.82757</td></tr><tr><td>eval/recall_extremely_negative</td><td>0.79303</td></tr><tr><td>eval/recall_extremely_positive</td><td>0.91519</td></tr><tr><td>eval/recall_negative</td><td>0.75519</td></tr><tr><td>eval/recall_neutral</td><td>0.87289</td></tr><tr><td>eval/recall_positive</td><td>0.80157</td></tr><tr><td>eval/runtime</td><td>5.1429</td></tr><tr><td>eval/samples_per_second</td><td>847.189</td></tr><tr><td>eval/steps_per_second</td><td>6.806</td></tr><tr><td>total_flos</td><td>5085855799117392.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>3128</td></tr><tr><td>train/grad_norm</td><td>4.86894</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.5929</td></tr><tr><td>train_loss</td><td>0.77637</td></tr><tr><td>train_runtime</td><td>724.2429</td></tr><tr><td>train_samples_per_second</td><td>414.226</td></tr><tr><td>train_steps_per_second</td><td>6.478</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/239xw040' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/239xw040</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_073941-239xw040/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 07:52:49,980] Trial 0 finished with value: 0.9205380993626433 and parameters: {'learning_rate': 3.36347924961925e-05, 'batch_size': 16, 'label_smoothing': 0.1428269059035347, 'num_epochs': 12, 'warmup_ratio': 0.07863577327467691, 'weight_decay': 0.07710265336920982, 'attention_dropout': 0.301254181394481, 'hidden_dropout': 0.3268135201604385}. Best is trial 0 with value: 0.9205380993626433.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 1 - TESTING THESE PARAMETERS:\n",
      "Learning Rate:      2.53e-04\n",
      "Epochs:             12\n",
      "Warmup Ratio:       0.090\n",
      "Weight Decay:       0.080\n",
      "Attention Dropout:  0.398\n",
      "Hidden Dropout:     0.310\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_075249-wi5egoaa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/wi5egoaa' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/wi5egoaa' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/wi5egoaa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1318007737.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(  # Uses ordinal loss always\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4692' max='4692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4692/4692 17:44, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Extremely Negative</th>\n",
       "      <th>Precision Extremely Negative</th>\n",
       "      <th>Recall Extremely Negative</th>\n",
       "      <th>Accuracy Extremely Negative</th>\n",
       "      <th>F1 Negative</th>\n",
       "      <th>Precision Negative</th>\n",
       "      <th>Recall Negative</th>\n",
       "      <th>Accuracy Negative</th>\n",
       "      <th>F1 Neutral</th>\n",
       "      <th>Precision Neutral</th>\n",
       "      <th>Recall Neutral</th>\n",
       "      <th>Accuracy Neutral</th>\n",
       "      <th>F1 Positive</th>\n",
       "      <th>Precision Positive</th>\n",
       "      <th>Recall Positive</th>\n",
       "      <th>Accuracy Positive</th>\n",
       "      <th>F1 Extremely Positive</th>\n",
       "      <th>Precision Extremely Positive</th>\n",
       "      <th>Recall Extremely Positive</th>\n",
       "      <th>Accuracy Extremely Positive</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Adjacent Accuracy</th>\n",
       "      <th>Quadratic Weighted Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.051700</td>\n",
       "      <td>1.452927</td>\n",
       "      <td>0.522607</td>\n",
       "      <td>0.466102</td>\n",
       "      <td>0.694353</td>\n",
       "      <td>0.462909</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.079918</td>\n",
       "      <td>0.079918</td>\n",
       "      <td>0.528808</td>\n",
       "      <td>0.447130</td>\n",
       "      <td>0.646995</td>\n",
       "      <td>0.646995</td>\n",
       "      <td>0.694457</td>\n",
       "      <td>0.585781</td>\n",
       "      <td>0.852643</td>\n",
       "      <td>0.852643</td>\n",
       "      <td>0.558976</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.633725</td>\n",
       "      <td>0.633725</td>\n",
       "      <td>0.183276</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.570576</td>\n",
       "      <td>0.915538</td>\n",
       "      <td>0.680696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.991900</td>\n",
       "      <td>1.035872</td>\n",
       "      <td>0.677989</td>\n",
       "      <td>0.671620</td>\n",
       "      <td>0.754896</td>\n",
       "      <td>0.667787</td>\n",
       "      <td>0.737557</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.668033</td>\n",
       "      <td>0.668033</td>\n",
       "      <td>0.658150</td>\n",
       "      <td>0.551292</td>\n",
       "      <td>0.816393</td>\n",
       "      <td>0.816393</td>\n",
       "      <td>0.808631</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.651694</td>\n",
       "      <td>0.614157</td>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.524469</td>\n",
       "      <td>0.969283</td>\n",
       "      <td>0.359494</td>\n",
       "      <td>0.359494</td>\n",
       "      <td>0.413128</td>\n",
       "      <td>0.916456</td>\n",
       "      <td>0.793912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.865700</td>\n",
       "      <td>1.033858</td>\n",
       "      <td>0.720220</td>\n",
       "      <td>0.718611</td>\n",
       "      <td>0.757345</td>\n",
       "      <td>0.731358</td>\n",
       "      <td>0.765550</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.694396</td>\n",
       "      <td>0.736520</td>\n",
       "      <td>0.656831</td>\n",
       "      <td>0.656831</td>\n",
       "      <td>0.840530</td>\n",
       "      <td>0.860849</td>\n",
       "      <td>0.821147</td>\n",
       "      <td>0.821147</td>\n",
       "      <td>0.619601</td>\n",
       "      <td>0.658429</td>\n",
       "      <td>0.585098</td>\n",
       "      <td>0.585098</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.611386</td>\n",
       "      <td>0.937975</td>\n",
       "      <td>0.937975</td>\n",
       "      <td>0.363782</td>\n",
       "      <td>0.928391</td>\n",
       "      <td>0.832923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.797200</td>\n",
       "      <td>0.926490</td>\n",
       "      <td>0.757861</td>\n",
       "      <td>0.757056</td>\n",
       "      <td>0.789624</td>\n",
       "      <td>0.755557</td>\n",
       "      <td>0.804284</td>\n",
       "      <td>0.766234</td>\n",
       "      <td>0.846311</td>\n",
       "      <td>0.846311</td>\n",
       "      <td>0.722133</td>\n",
       "      <td>0.705208</td>\n",
       "      <td>0.739891</td>\n",
       "      <td>0.739891</td>\n",
       "      <td>0.836578</td>\n",
       "      <td>0.879653</td>\n",
       "      <td>0.797525</td>\n",
       "      <td>0.797525</td>\n",
       "      <td>0.742877</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.828235</td>\n",
       "      <td>0.828235</td>\n",
       "      <td>0.701727</td>\n",
       "      <td>0.923554</td>\n",
       "      <td>0.565823</td>\n",
       "      <td>0.565823</td>\n",
       "      <td>0.301125</td>\n",
       "      <td>0.946523</td>\n",
       "      <td>0.863228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.889307</td>\n",
       "      <td>0.781501</td>\n",
       "      <td>0.782069</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.765390</td>\n",
       "      <td>0.774120</td>\n",
       "      <td>0.867684</td>\n",
       "      <td>0.698770</td>\n",
       "      <td>0.698770</td>\n",
       "      <td>0.739342</td>\n",
       "      <td>0.730277</td>\n",
       "      <td>0.748634</td>\n",
       "      <td>0.748634</td>\n",
       "      <td>0.849456</td>\n",
       "      <td>0.864802</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.770855</td>\n",
       "      <td>0.699234</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.778736</td>\n",
       "      <td>0.900332</td>\n",
       "      <td>0.686076</td>\n",
       "      <td>0.686076</td>\n",
       "      <td>0.269451</td>\n",
       "      <td>0.952031</td>\n",
       "      <td>0.876606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.899815</td>\n",
       "      <td>0.794354</td>\n",
       "      <td>0.793604</td>\n",
       "      <td>0.800174</td>\n",
       "      <td>0.801333</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>0.803245</td>\n",
       "      <td>0.811475</td>\n",
       "      <td>0.811475</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.763251</td>\n",
       "      <td>0.708197</td>\n",
       "      <td>0.708197</td>\n",
       "      <td>0.839667</td>\n",
       "      <td>0.889308</td>\n",
       "      <td>0.795276</td>\n",
       "      <td>0.795276</td>\n",
       "      <td>0.764500</td>\n",
       "      <td>0.758887</td>\n",
       "      <td>0.770196</td>\n",
       "      <td>0.770196</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.786177</td>\n",
       "      <td>0.921519</td>\n",
       "      <td>0.921519</td>\n",
       "      <td>0.262336</td>\n",
       "      <td>0.948818</td>\n",
       "      <td>0.883845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.576100</td>\n",
       "      <td>0.898636</td>\n",
       "      <td>0.790682</td>\n",
       "      <td>0.790145</td>\n",
       "      <td>0.789533</td>\n",
       "      <td>0.803444</td>\n",
       "      <td>0.802734</td>\n",
       "      <td>0.766791</td>\n",
       "      <td>0.842213</td>\n",
       "      <td>0.842213</td>\n",
       "      <td>0.738527</td>\n",
       "      <td>0.721585</td>\n",
       "      <td>0.756284</td>\n",
       "      <td>0.756284</td>\n",
       "      <td>0.830595</td>\n",
       "      <td>0.836758</td>\n",
       "      <td>0.824522</td>\n",
       "      <td>0.824522</td>\n",
       "      <td>0.760132</td>\n",
       "      <td>0.804024</td>\n",
       "      <td>0.720784</td>\n",
       "      <td>0.720784</td>\n",
       "      <td>0.845070</td>\n",
       "      <td>0.818505</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.256140</td>\n",
       "      <td>0.956162</td>\n",
       "      <td>0.893594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.538700</td>\n",
       "      <td>0.953030</td>\n",
       "      <td>0.786550</td>\n",
       "      <td>0.784954</td>\n",
       "      <td>0.781477</td>\n",
       "      <td>0.803222</td>\n",
       "      <td>0.791394</td>\n",
       "      <td>0.728055</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>0.718925</td>\n",
       "      <td>0.737084</td>\n",
       "      <td>0.701639</td>\n",
       "      <td>0.701639</td>\n",
       "      <td>0.820432</td>\n",
       "      <td>0.771287</td>\n",
       "      <td>0.876265</td>\n",
       "      <td>0.876265</td>\n",
       "      <td>0.763930</td>\n",
       "      <td>0.834572</td>\n",
       "      <td>0.704314</td>\n",
       "      <td>0.704314</td>\n",
       "      <td>0.851461</td>\n",
       "      <td>0.836386</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>0.253615</td>\n",
       "      <td>0.962818</td>\n",
       "      <td>0.898197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.955846</td>\n",
       "      <td>0.788846</td>\n",
       "      <td>0.787406</td>\n",
       "      <td>0.800293</td>\n",
       "      <td>0.789315</td>\n",
       "      <td>0.791334</td>\n",
       "      <td>0.892031</td>\n",
       "      <td>0.711066</td>\n",
       "      <td>0.711066</td>\n",
       "      <td>0.742535</td>\n",
       "      <td>0.766279</td>\n",
       "      <td>0.720219</td>\n",
       "      <td>0.720219</td>\n",
       "      <td>0.801245</td>\n",
       "      <td>0.743738</td>\n",
       "      <td>0.868391</td>\n",
       "      <td>0.868391</td>\n",
       "      <td>0.774486</td>\n",
       "      <td>0.814719</td>\n",
       "      <td>0.738039</td>\n",
       "      <td>0.738039</td>\n",
       "      <td>0.842229</td>\n",
       "      <td>0.784699</td>\n",
       "      <td>0.908861</td>\n",
       "      <td>0.908861</td>\n",
       "      <td>0.243516</td>\n",
       "      <td>0.971081</td>\n",
       "      <td>0.901198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.462200</td>\n",
       "      <td>0.966085</td>\n",
       "      <td>0.803076</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>0.810721</td>\n",
       "      <td>0.802958</td>\n",
       "      <td>0.804420</td>\n",
       "      <td>0.872902</td>\n",
       "      <td>0.745902</td>\n",
       "      <td>0.745902</td>\n",
       "      <td>0.744400</td>\n",
       "      <td>0.784504</td>\n",
       "      <td>0.708197</td>\n",
       "      <td>0.708197</td>\n",
       "      <td>0.812890</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.879640</td>\n",
       "      <td>0.879640</td>\n",
       "      <td>0.797586</td>\n",
       "      <td>0.819008</td>\n",
       "      <td>0.777255</td>\n",
       "      <td>0.777255</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.821634</td>\n",
       "      <td>0.903797</td>\n",
       "      <td>0.903797</td>\n",
       "      <td>0.228139</td>\n",
       "      <td>0.971311</td>\n",
       "      <td>0.907275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.980226</td>\n",
       "      <td>0.802616</td>\n",
       "      <td>0.801656</td>\n",
       "      <td>0.807555</td>\n",
       "      <td>0.805635</td>\n",
       "      <td>0.810180</td>\n",
       "      <td>0.839560</td>\n",
       "      <td>0.782787</td>\n",
       "      <td>0.782787</td>\n",
       "      <td>0.747545</td>\n",
       "      <td>0.792892</td>\n",
       "      <td>0.707104</td>\n",
       "      <td>0.707104</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.741722</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.797254</td>\n",
       "      <td>0.821815</td>\n",
       "      <td>0.774118</td>\n",
       "      <td>0.774118</td>\n",
       "      <td>0.861557</td>\n",
       "      <td>0.841787</td>\n",
       "      <td>0.882278</td>\n",
       "      <td>0.882278</td>\n",
       "      <td>0.223778</td>\n",
       "      <td>0.975671</td>\n",
       "      <td>0.912190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.805141</td>\n",
       "      <td>0.804537</td>\n",
       "      <td>0.808866</td>\n",
       "      <td>0.810150</td>\n",
       "      <td>0.815817</td>\n",
       "      <td>0.828753</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.759249</td>\n",
       "      <td>0.792162</td>\n",
       "      <td>0.728962</td>\n",
       "      <td>0.728962</td>\n",
       "      <td>0.806402</td>\n",
       "      <td>0.745229</td>\n",
       "      <td>0.878515</td>\n",
       "      <td>0.878515</td>\n",
       "      <td>0.795612</td>\n",
       "      <td>0.825464</td>\n",
       "      <td>0.767843</td>\n",
       "      <td>0.767843</td>\n",
       "      <td>0.862328</td>\n",
       "      <td>0.852723</td>\n",
       "      <td>0.872152</td>\n",
       "      <td>0.872152</td>\n",
       "      <td>0.219876</td>\n",
       "      <td>0.976819</td>\n",
       "      <td>0.914747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0000, Val Loss: 1.4529, Val F1: 0.0000, QWK: 0.6807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0000, Val Loss: 1.0359, Val F1: 0.0000, QWK: 0.7939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0000, Val Loss: 1.0339, Val F1: 0.0000, QWK: 0.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0000, Val Loss: 0.9265, Val F1: 0.0000, QWK: 0.8632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0000, Val Loss: 0.8893, Val F1: 0.0000, QWK: 0.8766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0000, Val Loss: 0.8998, Val F1: 0.0000, QWK: 0.8838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0000, Val Loss: 0.8986, Val F1: 0.0000, QWK: 0.8936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0000, Val Loss: 0.9530, Val F1: 0.0000, QWK: 0.8982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0000, Val Loss: 0.9558, Val F1: 0.0000, QWK: 0.9012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0000, Val Loss: 0.9661, Val F1: 0.0000, QWK: 0.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.0000, Val Loss: 0.9802, Val F1: 0.0000, QWK: 0.9122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.0000, Val Loss: 0.9773, Val F1: 0.0000, QWK: 0.9147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.6798, Val Loss: 0.9773, Val F1: 0.0000, QWK: 0.9147\n",
      "‚úÖ Trial checkpoint saved: ./checkpoints_bert/trial_1/final_epoch_12.ckpt\n",
      "‚úÖ Model files saved in: ./checkpoints_bert/trial_1\n",
      "üìä Trial 1 score: 0.9147 (Best: 0.9205)\n",
      "GPU Memory Used: 2.57 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Learning_Rate</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Stage</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>Unfrozen_Layers</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation Adjacent Accuracy</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>Validation MAE</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation QWK</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>eval/accuracy_negative</td><td>‚ñÅ‚ñà‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ</td></tr><tr><td>eval/accuracy_neutral</td><td>‚ñÜ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/accuracy_positive</td><td>‚ñÇ‚ñÑ‚ñÅ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>eval/adjacent_accuracy</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_extremely_negative</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_extremely_positive</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_negative</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_neutral</td><td>‚ñÅ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>eval/f1_positive</td><td>‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>eval/mae</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/precision_extremely_negative</td><td>‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>eval/precision_extremely_positive</td><td>‚ñà‚ñà‚ñÅ‚ñá‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>eval/precision_negative</td><td>‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/precision_neutral</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>eval/precision_positive</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall_extremely_negative</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>eval/recall_extremely_positive</td><td>‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>eval/recall_negative</td><td>‚ñÅ‚ñà‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ</td></tr><tr><td>eval/recall_neutral</td><td>‚ñÜ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall_positive</td><td>‚ñÇ‚ñÑ‚ñÅ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>12</td></tr><tr><td>Learning_Rate</td><td>0.00025</td></tr><tr><td>Stage</td><td>1</td></tr><tr><td>Train Accuracy</td><td>0</td></tr><tr><td>Train Loss</td><td>0.67985</td></tr><tr><td>Unfrozen_Layers</td><td>12</td></tr><tr><td>Validation Accuracy</td><td>0.80514</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.97682</td></tr><tr><td>Validation F1</td><td>0</td></tr><tr><td>Validation Loss</td><td>0.97727</td></tr><tr><td>Validation MAE</td><td>0.21988</td></tr><tr><td>Validation Precision</td><td>0</td></tr><tr><td>Validation QWK</td><td>0.91475</td></tr><tr><td>Validation Recall</td><td>0</td></tr><tr><td>eval/accuracy</td><td>0.80514</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>0.80328</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>0.87215</td></tr><tr><td>eval/accuracy_negative</td><td>0.72896</td></tr><tr><td>eval/accuracy_neutral</td><td>0.87852</td></tr><tr><td>eval/accuracy_positive</td><td>0.76784</td></tr><tr><td>eval/adjacent_accuracy</td><td>0.97682</td></tr><tr><td>eval/f1</td><td>0.80454</td></tr><tr><td>eval/f1_extremely_negative</td><td>0.81582</td></tr><tr><td>eval/f1_extremely_positive</td><td>0.86233</td></tr><tr><td>eval/f1_negative</td><td>0.75925</td></tr><tr><td>eval/f1_neutral</td><td>0.8064</td></tr><tr><td>eval/f1_positive</td><td>0.79561</td></tr><tr><td>eval/loss</td><td>0.97727</td></tr><tr><td>eval/mae</td><td>0.21988</td></tr><tr><td>eval/precision</td><td>0.80887</td></tr><tr><td>eval/precision_extremely_negative</td><td>0.82875</td></tr><tr><td>eval/precision_extremely_positive</td><td>0.85272</td></tr><tr><td>eval/precision_negative</td><td>0.79216</td></tr><tr><td>eval/precision_neutral</td><td>0.74523</td></tr><tr><td>eval/precision_positive</td><td>0.82546</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>0.91475</td></tr><tr><td>eval/recall</td><td>0.81015</td></tr><tr><td>eval/recall_extremely_negative</td><td>0.80328</td></tr><tr><td>eval/recall_extremely_positive</td><td>0.87215</td></tr><tr><td>eval/recall_negative</td><td>0.72896</td></tr><tr><td>eval/recall_neutral</td><td>0.87852</td></tr><tr><td>eval/recall_positive</td><td>0.76784</td></tr><tr><td>eval/runtime</td><td>5.5104</td></tr><tr><td>eval/samples_per_second</td><td>790.687</td></tr><tr><td>eval/steps_per_second</td><td>6.352</td></tr><tr><td>total_flos</td><td>7626372467573664.0</td></tr><tr><td>train/epoch</td><td>12</td></tr><tr><td>train/global_step</td><td>4692</td></tr><tr><td>train/grad_norm</td><td>0.07648</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4342</td></tr><tr><td>train_loss</td><td>0.67985</td></tr><tr><td>train_runtime</td><td>1065.3182</td></tr><tr><td>train_samples_per_second</td><td>281.606</td></tr><tr><td>train_steps_per_second</td><td>4.404</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/wi5egoaa' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/wi5egoaa</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_075249-wi5egoaa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 08:11:00,793] Trial 1 finished with value: 0.9147465517083717 and parameters: {'learning_rate': 0.00025347182708649116, 'batch_size': 16, 'label_smoothing': 0.1065678658979583, 'num_epochs': 12, 'warmup_ratio': 0.09017385634999035, 'weight_decay': 0.08017493106208261, 'attention_dropout': 0.3982050054406836, 'hidden_dropout': 0.31001996429156703}. Best is trial 0 with value: 0.9205380993626433.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 2 - TESTING THESE PARAMETERS:\n",
      "Learning Rate:      1.23e-04\n",
      "Epochs:             15\n",
      "Warmup Ratio:       0.060\n",
      "Weight Decay:       0.061\n",
      "Attention Dropout:  0.302\n",
      "Hidden Dropout:     0.313\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_081100-etq8gzsj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/etq8gzsj' target=\"_blank\">trial_2</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/etq8gzsj' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/etq8gzsj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1318007737.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(  # Uses ordinal loss always\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='588' max='1470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 588/1470 07:08 < 10:44, 1.37 it/s, Epoch 6/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Extremely Negative</th>\n",
       "      <th>Precision Extremely Negative</th>\n",
       "      <th>Recall Extremely Negative</th>\n",
       "      <th>Accuracy Extremely Negative</th>\n",
       "      <th>F1 Negative</th>\n",
       "      <th>Precision Negative</th>\n",
       "      <th>Recall Negative</th>\n",
       "      <th>Accuracy Negative</th>\n",
       "      <th>F1 Neutral</th>\n",
       "      <th>Precision Neutral</th>\n",
       "      <th>Recall Neutral</th>\n",
       "      <th>Accuracy Neutral</th>\n",
       "      <th>F1 Positive</th>\n",
       "      <th>Precision Positive</th>\n",
       "      <th>Recall Positive</th>\n",
       "      <th>Accuracy Positive</th>\n",
       "      <th>F1 Extremely Positive</th>\n",
       "      <th>Precision Extremely Positive</th>\n",
       "      <th>Recall Extremely Positive</th>\n",
       "      <th>Accuracy Extremely Positive</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Adjacent Accuracy</th>\n",
       "      <th>Quadratic Weighted Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.020771</td>\n",
       "      <td>0.653890</td>\n",
       "      <td>0.649736</td>\n",
       "      <td>0.660863</td>\n",
       "      <td>0.695859</td>\n",
       "      <td>0.672023</td>\n",
       "      <td>0.522134</td>\n",
       "      <td>0.942623</td>\n",
       "      <td>0.942623</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.544379</td>\n",
       "      <td>0.502732</td>\n",
       "      <td>0.502732</td>\n",
       "      <td>0.728792</td>\n",
       "      <td>0.850075</td>\n",
       "      <td>0.637795</td>\n",
       "      <td>0.637795</td>\n",
       "      <td>0.609669</td>\n",
       "      <td>0.710115</td>\n",
       "      <td>0.534118</td>\n",
       "      <td>0.534118</td>\n",
       "      <td>0.758774</td>\n",
       "      <td>0.677612</td>\n",
       "      <td>0.862025</td>\n",
       "      <td>0.862025</td>\n",
       "      <td>0.437457</td>\n",
       "      <td>0.921965</td>\n",
       "      <td>0.828379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.195800</td>\n",
       "      <td>0.792641</td>\n",
       "      <td>0.793895</td>\n",
       "      <td>0.793587</td>\n",
       "      <td>0.802849</td>\n",
       "      <td>0.806438</td>\n",
       "      <td>0.824742</td>\n",
       "      <td>0.829876</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.766756</td>\n",
       "      <td>0.752632</td>\n",
       "      <td>0.781421</td>\n",
       "      <td>0.781421</td>\n",
       "      <td>0.853541</td>\n",
       "      <td>0.915058</td>\n",
       "      <td>0.799775</td>\n",
       "      <td>0.799775</td>\n",
       "      <td>0.744856</td>\n",
       "      <td>0.783550</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.816601</td>\n",
       "      <td>0.733132</td>\n",
       "      <td>0.921519</td>\n",
       "      <td>0.921519</td>\n",
       "      <td>0.250861</td>\n",
       "      <td>0.958228</td>\n",
       "      <td>0.897792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.831600</td>\n",
       "      <td>0.788417</td>\n",
       "      <td>0.793895</td>\n",
       "      <td>0.793599</td>\n",
       "      <td>0.798298</td>\n",
       "      <td>0.803616</td>\n",
       "      <td>0.770903</td>\n",
       "      <td>0.651130</td>\n",
       "      <td>0.944672</td>\n",
       "      <td>0.944672</td>\n",
       "      <td>0.694825</td>\n",
       "      <td>0.762402</td>\n",
       "      <td>0.638251</td>\n",
       "      <td>0.638251</td>\n",
       "      <td>0.868164</td>\n",
       "      <td>0.889151</td>\n",
       "      <td>0.848144</td>\n",
       "      <td>0.848144</td>\n",
       "      <td>0.800600</td>\n",
       "      <td>0.766691</td>\n",
       "      <td>0.837647</td>\n",
       "      <td>0.837647</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.922118</td>\n",
       "      <td>0.749367</td>\n",
       "      <td>0.749367</td>\n",
       "      <td>0.249484</td>\n",
       "      <td>0.959146</td>\n",
       "      <td>0.898169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.741727</td>\n",
       "      <td>0.831765</td>\n",
       "      <td>0.830998</td>\n",
       "      <td>0.829397</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>0.838894</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.782959</td>\n",
       "      <td>0.827251</td>\n",
       "      <td>0.743169</td>\n",
       "      <td>0.743169</td>\n",
       "      <td>0.862876</td>\n",
       "      <td>0.855249</td>\n",
       "      <td>0.870641</td>\n",
       "      <td>0.870641</td>\n",
       "      <td>0.815956</td>\n",
       "      <td>0.821798</td>\n",
       "      <td>0.810196</td>\n",
       "      <td>0.810196</td>\n",
       "      <td>0.870162</td>\n",
       "      <td>0.858374</td>\n",
       "      <td>0.882278</td>\n",
       "      <td>0.882278</td>\n",
       "      <td>0.199449</td>\n",
       "      <td>0.970852</td>\n",
       "      <td>0.919701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.623400</td>\n",
       "      <td>0.818119</td>\n",
       "      <td>0.808354</td>\n",
       "      <td>0.807451</td>\n",
       "      <td>0.805865</td>\n",
       "      <td>0.818158</td>\n",
       "      <td>0.794667</td>\n",
       "      <td>0.701727</td>\n",
       "      <td>0.915984</td>\n",
       "      <td>0.915984</td>\n",
       "      <td>0.727814</td>\n",
       "      <td>0.799738</td>\n",
       "      <td>0.667760</td>\n",
       "      <td>0.667760</td>\n",
       "      <td>0.837900</td>\n",
       "      <td>0.850521</td>\n",
       "      <td>0.825647</td>\n",
       "      <td>0.825647</td>\n",
       "      <td>0.810749</td>\n",
       "      <td>0.793985</td>\n",
       "      <td>0.828235</td>\n",
       "      <td>0.828235</td>\n",
       "      <td>0.867997</td>\n",
       "      <td>0.883355</td>\n",
       "      <td>0.853165</td>\n",
       "      <td>0.853165</td>\n",
       "      <td>0.224466</td>\n",
       "      <td>0.969474</td>\n",
       "      <td>0.912123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.790186</td>\n",
       "      <td>0.821896</td>\n",
       "      <td>0.820529</td>\n",
       "      <td>0.816509</td>\n",
       "      <td>0.834186</td>\n",
       "      <td>0.822773</td>\n",
       "      <td>0.745424</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.750293</td>\n",
       "      <td>0.809102</td>\n",
       "      <td>0.699454</td>\n",
       "      <td>0.699454</td>\n",
       "      <td>0.844699</td>\n",
       "      <td>0.860981</td>\n",
       "      <td>0.829021</td>\n",
       "      <td>0.829021</td>\n",
       "      <td>0.820127</td>\n",
       "      <td>0.828663</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.873939</td>\n",
       "      <td>0.838372</td>\n",
       "      <td>0.912658</td>\n",
       "      <td>0.912658</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.972458</td>\n",
       "      <td>0.918802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0000, Val Loss: 1.0208, Val F1: 0.0000, QWK: 0.8284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0000, Val Loss: 0.7926, Val F1: 0.0000, QWK: 0.8978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0000, Val Loss: 0.7884, Val F1: 0.0000, QWK: 0.8982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0000, Val Loss: 0.7417, Val F1: 0.0000, QWK: 0.9197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0000, Val Loss: 0.8181, Val F1: 0.0000, QWK: 0.9121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0000, Val Loss: 0.7902, Val F1: 0.0000, QWK: 0.9188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.7382, Val Loss: 0.7902, Val F1: 0.0000, QWK: 0.9188\n",
      "‚úÖ Trial checkpoint saved: ./checkpoints_bert/trial_2/final_epoch_6.ckpt\n",
      "‚úÖ Model files saved in: ./checkpoints_bert/trial_2\n",
      "üìä Trial 2 score: 0.9188 (Best: 0.9205)\n",
      "GPU Memory Used: 4.13 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà</td></tr><tr><td>Learning_Rate</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Stage</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>Unfrozen_Layers</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>Validation Adjacent Accuracy</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>Validation MAE</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation QWK</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>‚ñà‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÖ‚ñà‚ñà</td></tr><tr><td>eval/accuracy_negative</td><td>‚ñÅ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÜ</td></tr><tr><td>eval/accuracy_neutral</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/accuracy_positive</td><td>‚ñÅ‚ñÖ‚ñà‚ñá‚ñà‚ñá‚ñá</td></tr><tr><td>eval/adjacent_accuracy</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/f1_extremely_negative</td><td>‚ñÅ‚ñá‚ñÖ‚ñà‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/f1_extremely_positive</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_negative</td><td>‚ñÅ‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/f1_neutral</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/f1_positive</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>eval/mae</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/precision_extremely_negative</td><td>‚ñÅ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÜ</td></tr><tr><td>eval/precision_extremely_positive</td><td>‚ñÅ‚ñÉ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ</td></tr><tr><td>eval/precision_negative</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/precision_neutral</td><td>‚ñÅ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>eval/precision_positive</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/recall_extremely_negative</td><td>‚ñà‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/recall_extremely_positive</td><td>‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÖ‚ñà‚ñà</td></tr><tr><td>eval/recall_negative</td><td>‚ñÅ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÜ</td></tr><tr><td>eval/recall_neutral</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/recall_positive</td><td>‚ñÅ‚ñÖ‚ñà‚ñá‚ñà‚ñá‚ñá</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÉ‚ñÅ‚ñà‚ñÜ‚ñÉ‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñá‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñá</td></tr><tr><td>eval/steps_per_second</td><td>‚ñá‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñá</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÉ‚ñÜ‚ñÅ‚ñà‚ñÉ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñá‚ñÜ‚ñÑ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>6</td></tr><tr><td>Learning_Rate</td><td>0.00012</td></tr><tr><td>Stage</td><td>1</td></tr><tr><td>Train Accuracy</td><td>0</td></tr><tr><td>Train Loss</td><td>0.73816</td></tr><tr><td>Unfrozen_Layers</td><td>12</td></tr><tr><td>Validation Accuracy</td><td>0.8219</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.97246</td></tr><tr><td>Validation F1</td><td>0</td></tr><tr><td>Validation Loss</td><td>0.79019</td></tr><tr><td>Validation MAE</td><td>0.2084</td></tr><tr><td>Validation Precision</td><td>0</td></tr><tr><td>Validation QWK</td><td>0.9188</td></tr><tr><td>Validation Recall</td><td>0</td></tr><tr><td>eval/accuracy</td><td>0.8219</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>0.91803</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>0.91266</td></tr><tr><td>eval/accuracy_negative</td><td>0.69945</td></tr><tr><td>eval/accuracy_neutral</td><td>0.82902</td></tr><tr><td>eval/accuracy_positive</td><td>0.81176</td></tr><tr><td>eval/adjacent_accuracy</td><td>0.97246</td></tr><tr><td>eval/f1</td><td>0.82053</td></tr><tr><td>eval/f1_extremely_negative</td><td>0.82277</td></tr><tr><td>eval/f1_extremely_positive</td><td>0.87394</td></tr><tr><td>eval/f1_negative</td><td>0.75029</td></tr><tr><td>eval/f1_neutral</td><td>0.8447</td></tr><tr><td>eval/f1_positive</td><td>0.82013</td></tr><tr><td>eval/loss</td><td>0.79019</td></tr><tr><td>eval/mae</td><td>0.2084</td></tr><tr><td>eval/precision</td><td>0.81651</td></tr><tr><td>eval/precision_extremely_negative</td><td>0.74542</td></tr><tr><td>eval/precision_extremely_positive</td><td>0.83837</td></tr><tr><td>eval/precision_negative</td><td>0.8091</td></tr><tr><td>eval/precision_neutral</td><td>0.86098</td></tr><tr><td>eval/precision_positive</td><td>0.82866</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>0.9188</td></tr><tr><td>eval/recall</td><td>0.83419</td></tr><tr><td>eval/recall_extremely_negative</td><td>0.91803</td></tr><tr><td>eval/recall_extremely_positive</td><td>0.91266</td></tr><tr><td>eval/recall_negative</td><td>0.69945</td></tr><tr><td>eval/recall_neutral</td><td>0.82902</td></tr><tr><td>eval/recall_positive</td><td>0.81176</td></tr><tr><td>eval/runtime</td><td>6.3258</td></tr><tr><td>eval/samples_per_second</td><td>688.771</td></tr><tr><td>eval/steps_per_second</td><td>1.423</td></tr><tr><td>total_flos</td><td>3979785241397280.0</td></tr><tr><td>train/epoch</td><td>6</td></tr><tr><td>train/global_step</td><td>588</td></tr><tr><td>train/grad_norm</td><td>2.51977</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.56</td></tr><tr><td>train_loss</td><td>0.73816</td></tr><tr><td>train_runtime</td><td>429.9735</td></tr><tr><td>train_samples_per_second</td><td>872.147</td></tr><tr><td>train_steps_per_second</td><td>3.419</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_2</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/etq8gzsj' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/etq8gzsj</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_081100-etq8gzsj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 08:19:30,841] Trial 2 finished with value: 0.9188023074101209 and parameters: {'learning_rate': 0.00012345780336645804, 'batch_size': 64, 'label_smoothing': 0.09171342217547615, 'num_epochs': 15, 'warmup_ratio': 0.06015041835053567, 'weight_decay': 0.06133991608882185, 'attention_dropout': 0.3019927202194576, 'hidden_dropout': 0.31261729684360023}. Best is trial 0 with value: 0.9205380993626433.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 3 - TESTING THESE PARAMETERS:\n",
      "Learning Rate:      4.74e-04\n",
      "Epochs:             10\n",
      "Warmup Ratio:       0.098\n",
      "Weight Decay:       0.134\n",
      "Attention Dropout:  0.331\n",
      "Hidden Dropout:     0.387\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_081930-0by8ytuj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/0by8ytuj' target=\"_blank\">trial_3</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/0by8ytuj' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/0by8ytuj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1318007737.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(  # Uses ordinal loss always\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1173' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1173/3910 04:20 < 10:09, 4.49 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Extremely Negative</th>\n",
       "      <th>Precision Extremely Negative</th>\n",
       "      <th>Recall Extremely Negative</th>\n",
       "      <th>Accuracy Extremely Negative</th>\n",
       "      <th>F1 Negative</th>\n",
       "      <th>Precision Negative</th>\n",
       "      <th>Recall Negative</th>\n",
       "      <th>Accuracy Negative</th>\n",
       "      <th>F1 Neutral</th>\n",
       "      <th>Precision Neutral</th>\n",
       "      <th>Recall Neutral</th>\n",
       "      <th>Accuracy Neutral</th>\n",
       "      <th>F1 Positive</th>\n",
       "      <th>Precision Positive</th>\n",
       "      <th>Recall Positive</th>\n",
       "      <th>Accuracy Positive</th>\n",
       "      <th>F1 Extremely Positive</th>\n",
       "      <th>Precision Extremely Positive</th>\n",
       "      <th>Recall Extremely Positive</th>\n",
       "      <th>Accuracy Extremely Positive</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Adjacent Accuracy</th>\n",
       "      <th>Quadratic Weighted Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.421400</td>\n",
       "      <td>1.742437</td>\n",
       "      <td>0.204039</td>\n",
       "      <td>0.069154</td>\n",
       "      <td>0.040808</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338925</td>\n",
       "      <td>0.204039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.089282</td>\n",
       "      <td>0.706679</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>1.599605</td>\n",
       "      <td>0.204039</td>\n",
       "      <td>0.069154</td>\n",
       "      <td>0.040808</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338925</td>\n",
       "      <td>0.204039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.089282</td>\n",
       "      <td>0.706679</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.611000</td>\n",
       "      <td>1.611399</td>\n",
       "      <td>0.210007</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.042001</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347117</td>\n",
       "      <td>0.210007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.445261</td>\n",
       "      <td>0.526050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0000, Val Loss: 1.7424, Val F1: 0.0000, QWK: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0000, Val Loss: 1.5996, Val F1: 0.0000, QWK: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0000, Val Loss: 1.6114, Val F1: 0.0000, QWK: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 1.5479, Val Loss: 1.6114, Val F1: 0.0000, QWK: 0.0000\n",
      "‚úÖ Trial checkpoint saved: ./checkpoints_bert/trial_3/final_epoch_3.ckpt\n",
      "‚úÖ Model files saved in: ./checkpoints_bert/trial_3\n",
      "üìä Trial 3 score: 0.0000 (Best: 0.9205)\n",
      "GPU Memory Used: 4.13 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÖ‚ñà‚ñà</td></tr><tr><td>Learning_Rate</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Stage</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>Unfrozen_Layers</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>Validation Adjacent Accuracy</td><td>‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>Validation MAE</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation QWK</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy_negative</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/accuracy_neutral</td><td>‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy_positive</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/adjacent_accuracy</td><td>‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/f1_extremely_negative</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1_extremely_positive</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1_negative</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/f1_neutral</td><td>‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1_positive</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>eval/mae</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/precision_extremely_negative</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_extremely_positive</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_negative</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/precision_neutral</td><td>‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_positive</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall_extremely_negative</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall_extremely_positive</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall_negative</td><td>‚ñÅ‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/recall_neutral</td><td>‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall_positive</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñà‚ñÅ‚ñá</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÅ‚ñà‚ñÇ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÅ‚ñà‚ñÇ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÜ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>3</td></tr><tr><td>Learning_Rate</td><td>0.00047</td></tr><tr><td>Stage</td><td>1</td></tr><tr><td>Train Accuracy</td><td>0</td></tr><tr><td>Train Loss</td><td>1.54792</td></tr><tr><td>Unfrozen_Layers</td><td>12</td></tr><tr><td>Validation Accuracy</td><td>0.21001</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.52605</td></tr><tr><td>Validation F1</td><td>0</td></tr><tr><td>Validation Loss</td><td>1.6114</td></tr><tr><td>Validation MAE</td><td>1.44526</td></tr><tr><td>Validation Precision</td><td>0</td></tr><tr><td>Validation QWK</td><td>0</td></tr><tr><td>Validation Recall</td><td>0</td></tr><tr><td>eval/accuracy</td><td>0.21001</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>0</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>0</td></tr><tr><td>eval/accuracy_negative</td><td>1</td></tr><tr><td>eval/accuracy_neutral</td><td>0</td></tr><tr><td>eval/accuracy_positive</td><td>0</td></tr><tr><td>eval/adjacent_accuracy</td><td>0.52605</td></tr><tr><td>eval/f1</td><td>0.0729</td></tr><tr><td>eval/f1_extremely_negative</td><td>0</td></tr><tr><td>eval/f1_extremely_positive</td><td>0</td></tr><tr><td>eval/f1_negative</td><td>0.34712</td></tr><tr><td>eval/f1_neutral</td><td>0</td></tr><tr><td>eval/f1_positive</td><td>0</td></tr><tr><td>eval/loss</td><td>1.6114</td></tr><tr><td>eval/mae</td><td>1.44526</td></tr><tr><td>eval/precision</td><td>0.042</td></tr><tr><td>eval/precision_extremely_negative</td><td>0</td></tr><tr><td>eval/precision_extremely_positive</td><td>0</td></tr><tr><td>eval/precision_negative</td><td>0.21001</td></tr><tr><td>eval/precision_neutral</td><td>0</td></tr><tr><td>eval/precision_positive</td><td>0</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>0</td></tr><tr><td>eval/recall</td><td>0.2</td></tr><tr><td>eval/recall_extremely_negative</td><td>0</td></tr><tr><td>eval/recall_extremely_positive</td><td>0</td></tr><tr><td>eval/recall_negative</td><td>1</td></tr><tr><td>eval/recall_neutral</td><td>0</td></tr><tr><td>eval/recall_positive</td><td>0</td></tr><tr><td>eval/runtime</td><td>5.0436</td></tr><tr><td>eval/samples_per_second</td><td>863.866</td></tr><tr><td>eval/steps_per_second</td><td>6.939</td></tr><tr><td>total_flos</td><td>1906216940174112.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>1173</td></tr><tr><td>train/grad_norm</td><td>0.18337</td></tr><tr><td>train/learning_rate</td><td>0.00043</td></tr><tr><td>train/loss</td><td>1.611</td></tr><tr><td>train_loss</td><td>1.54792</td></tr><tr><td>train_runtime</td><td>261.4677</td></tr><tr><td>train_samples_per_second</td><td>956.141</td></tr><tr><td>train_steps_per_second</td><td>14.954</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_3</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/0by8ytuj' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/0by8ytuj</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_081930-0by8ytuj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 08:24:52,114] Trial 3 finished with value: 0.0 and parameters: {'learning_rate': 0.000473801851608495, 'batch_size': 16, 'label_smoothing': 0.10161124967206855, 'num_epochs': 10, 'warmup_ratio': 0.09817653648196495, 'weight_decay': 0.13418021102228506, 'attention_dropout': 0.33100183170276276, 'hidden_dropout': 0.38740468538963047}. Best is trial 0 with value: 0.9205380993626433.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 4 - TESTING THESE PARAMETERS:\n",
      "Learning Rate:      2.12e-04\n",
      "Epochs:             11\n",
      "Warmup Ratio:       0.133\n",
      "Weight Decay:       0.112\n",
      "Attention Dropout:  0.331\n",
      "Hidden Dropout:     0.389\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_082452-5t95b92o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/5t95b92o' target=\"_blank\">trial_4</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/5t95b92o' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/5t95b92o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1318007737.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(  # Uses ordinal loss always\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2156' max='2156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2156/2156 13:40, Epoch 11/11]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Extremely Negative</th>\n",
       "      <th>Precision Extremely Negative</th>\n",
       "      <th>Recall Extremely Negative</th>\n",
       "      <th>Accuracy Extremely Negative</th>\n",
       "      <th>F1 Negative</th>\n",
       "      <th>Precision Negative</th>\n",
       "      <th>Recall Negative</th>\n",
       "      <th>Accuracy Negative</th>\n",
       "      <th>F1 Neutral</th>\n",
       "      <th>Precision Neutral</th>\n",
       "      <th>Recall Neutral</th>\n",
       "      <th>Accuracy Neutral</th>\n",
       "      <th>F1 Positive</th>\n",
       "      <th>Precision Positive</th>\n",
       "      <th>Recall Positive</th>\n",
       "      <th>Accuracy Positive</th>\n",
       "      <th>F1 Extremely Positive</th>\n",
       "      <th>Precision Extremely Positive</th>\n",
       "      <th>Recall Extremely Positive</th>\n",
       "      <th>Accuracy Extremely Positive</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Adjacent Accuracy</th>\n",
       "      <th>Quadratic Weighted Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.305200</td>\n",
       "      <td>0.950134</td>\n",
       "      <td>0.722515</td>\n",
       "      <td>0.721148</td>\n",
       "      <td>0.718319</td>\n",
       "      <td>0.749896</td>\n",
       "      <td>0.732303</td>\n",
       "      <td>0.607287</td>\n",
       "      <td>0.922131</td>\n",
       "      <td>0.922131</td>\n",
       "      <td>0.613739</td>\n",
       "      <td>0.632985</td>\n",
       "      <td>0.595628</td>\n",
       "      <td>0.595628</td>\n",
       "      <td>0.811527</td>\n",
       "      <td>0.832151</td>\n",
       "      <td>0.791901</td>\n",
       "      <td>0.791901</td>\n",
       "      <td>0.694362</td>\n",
       "      <td>0.755535</td>\n",
       "      <td>0.642353</td>\n",
       "      <td>0.642353</td>\n",
       "      <td>0.780186</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.355520</td>\n",
       "      <td>0.933211</td>\n",
       "      <td>0.847764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.890600</td>\n",
       "      <td>1.173132</td>\n",
       "      <td>0.659169</td>\n",
       "      <td>0.651481</td>\n",
       "      <td>0.690783</td>\n",
       "      <td>0.704905</td>\n",
       "      <td>0.771041</td>\n",
       "      <td>0.690438</td>\n",
       "      <td>0.872951</td>\n",
       "      <td>0.872951</td>\n",
       "      <td>0.622871</td>\n",
       "      <td>0.702332</td>\n",
       "      <td>0.559563</td>\n",
       "      <td>0.559563</td>\n",
       "      <td>0.777563</td>\n",
       "      <td>0.910876</td>\n",
       "      <td>0.678290</td>\n",
       "      <td>0.678290</td>\n",
       "      <td>0.513686</td>\n",
       "      <td>0.613959</td>\n",
       "      <td>0.441569</td>\n",
       "      <td>0.441569</td>\n",
       "      <td>0.691269</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.972152</td>\n",
       "      <td>0.972152</td>\n",
       "      <td>0.425063</td>\n",
       "      <td>0.925866</td>\n",
       "      <td>0.836103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.937596</td>\n",
       "      <td>0.766583</td>\n",
       "      <td>0.764217</td>\n",
       "      <td>0.775084</td>\n",
       "      <td>0.787600</td>\n",
       "      <td>0.805247</td>\n",
       "      <td>0.793241</td>\n",
       "      <td>0.817623</td>\n",
       "      <td>0.817623</td>\n",
       "      <td>0.734671</td>\n",
       "      <td>0.749716</td>\n",
       "      <td>0.720219</td>\n",
       "      <td>0.720219</td>\n",
       "      <td>0.849156</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.820022</td>\n",
       "      <td>0.820022</td>\n",
       "      <td>0.697128</td>\n",
       "      <td>0.782991</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>0.785789</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.951899</td>\n",
       "      <td>0.951899</td>\n",
       "      <td>0.286436</td>\n",
       "      <td>0.952261</td>\n",
       "      <td>0.883301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.716800</td>\n",
       "      <td>0.885185</td>\n",
       "      <td>0.790452</td>\n",
       "      <td>0.788794</td>\n",
       "      <td>0.785261</td>\n",
       "      <td>0.808644</td>\n",
       "      <td>0.787133</td>\n",
       "      <td>0.731107</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.730997</td>\n",
       "      <td>0.721277</td>\n",
       "      <td>0.740984</td>\n",
       "      <td>0.740984</td>\n",
       "      <td>0.859218</td>\n",
       "      <td>0.853496</td>\n",
       "      <td>0.865017</td>\n",
       "      <td>0.865017</td>\n",
       "      <td>0.753484</td>\n",
       "      <td>0.847209</td>\n",
       "      <td>0.678431</td>\n",
       "      <td>0.678431</td>\n",
       "      <td>0.834499</td>\n",
       "      <td>0.773218</td>\n",
       "      <td>0.906329</td>\n",
       "      <td>0.906329</td>\n",
       "      <td>0.260959</td>\n",
       "      <td>0.953867</td>\n",
       "      <td>0.890472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>0.879063</td>\n",
       "      <td>0.809961</td>\n",
       "      <td>0.808755</td>\n",
       "      <td>0.816861</td>\n",
       "      <td>0.812305</td>\n",
       "      <td>0.816594</td>\n",
       "      <td>0.873832</td>\n",
       "      <td>0.766393</td>\n",
       "      <td>0.766393</td>\n",
       "      <td>0.759551</td>\n",
       "      <td>0.781503</td>\n",
       "      <td>0.738798</td>\n",
       "      <td>0.738798</td>\n",
       "      <td>0.838816</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.860517</td>\n",
       "      <td>0.860517</td>\n",
       "      <td>0.787200</td>\n",
       "      <td>0.803265</td>\n",
       "      <td>0.771765</td>\n",
       "      <td>0.771765</td>\n",
       "      <td>0.861865</td>\n",
       "      <td>0.807522</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.225843</td>\n",
       "      <td>0.966032</td>\n",
       "      <td>0.907210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.609500</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.820748</td>\n",
       "      <td>0.820562</td>\n",
       "      <td>0.824030</td>\n",
       "      <td>0.827383</td>\n",
       "      <td>0.843177</td>\n",
       "      <td>0.838057</td>\n",
       "      <td>0.848361</td>\n",
       "      <td>0.848361</td>\n",
       "      <td>0.779493</td>\n",
       "      <td>0.786429</td>\n",
       "      <td>0.772678</td>\n",
       "      <td>0.772678</td>\n",
       "      <td>0.828877</td>\n",
       "      <td>0.790010</td>\n",
       "      <td>0.871766</td>\n",
       "      <td>0.871766</td>\n",
       "      <td>0.806114</td>\n",
       "      <td>0.827415</td>\n",
       "      <td>0.785882</td>\n",
       "      <td>0.785882</td>\n",
       "      <td>0.868118</td>\n",
       "      <td>0.878238</td>\n",
       "      <td>0.858228</td>\n",
       "      <td>0.858228</td>\n",
       "      <td>0.207482</td>\n",
       "      <td>0.973835</td>\n",
       "      <td>0.917116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>0.829073</td>\n",
       "      <td>0.824191</td>\n",
       "      <td>0.824108</td>\n",
       "      <td>0.826446</td>\n",
       "      <td>0.827517</td>\n",
       "      <td>0.830421</td>\n",
       "      <td>0.832990</td>\n",
       "      <td>0.827869</td>\n",
       "      <td>0.827869</td>\n",
       "      <td>0.782276</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.781421</td>\n",
       "      <td>0.781421</td>\n",
       "      <td>0.837644</td>\n",
       "      <td>0.820043</td>\n",
       "      <td>0.856018</td>\n",
       "      <td>0.856018</td>\n",
       "      <td>0.813492</td>\n",
       "      <td>0.823293</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.870558</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.868354</td>\n",
       "      <td>0.868354</td>\n",
       "      <td>0.204269</td>\n",
       "      <td>0.973835</td>\n",
       "      <td>0.917909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.540100</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.817535</td>\n",
       "      <td>0.816192</td>\n",
       "      <td>0.816559</td>\n",
       "      <td>0.829590</td>\n",
       "      <td>0.831143</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>0.842213</td>\n",
       "      <td>0.842213</td>\n",
       "      <td>0.787140</td>\n",
       "      <td>0.798650</td>\n",
       "      <td>0.775956</td>\n",
       "      <td>0.775956</td>\n",
       "      <td>0.842397</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.853768</td>\n",
       "      <td>0.853768</td>\n",
       "      <td>0.790424</td>\n",
       "      <td>0.850814</td>\n",
       "      <td>0.738039</td>\n",
       "      <td>0.738039</td>\n",
       "      <td>0.852704</td>\n",
       "      <td>0.781646</td>\n",
       "      <td>0.937975</td>\n",
       "      <td>0.937975</td>\n",
       "      <td>0.210007</td>\n",
       "      <td>0.974524</td>\n",
       "      <td>0.919528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.496800</td>\n",
       "      <td>0.848681</td>\n",
       "      <td>0.829011</td>\n",
       "      <td>0.828453</td>\n",
       "      <td>0.827360</td>\n",
       "      <td>0.837582</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.801869</td>\n",
       "      <td>0.879098</td>\n",
       "      <td>0.879098</td>\n",
       "      <td>0.779932</td>\n",
       "      <td>0.815256</td>\n",
       "      <td>0.747541</td>\n",
       "      <td>0.747541</td>\n",
       "      <td>0.834654</td>\n",
       "      <td>0.786853</td>\n",
       "      <td>0.888639</td>\n",
       "      <td>0.888639</td>\n",
       "      <td>0.825101</td>\n",
       "      <td>0.852720</td>\n",
       "      <td>0.799216</td>\n",
       "      <td>0.799216</td>\n",
       "      <td>0.876747</td>\n",
       "      <td>0.880102</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.191875</td>\n",
       "      <td>0.980491</td>\n",
       "      <td>0.927639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.470300</td>\n",
       "      <td>0.913533</td>\n",
       "      <td>0.819830</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.820078</td>\n",
       "      <td>0.832386</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.830078</td>\n",
       "      <td>0.870902</td>\n",
       "      <td>0.870902</td>\n",
       "      <td>0.767606</td>\n",
       "      <td>0.828897</td>\n",
       "      <td>0.714754</td>\n",
       "      <td>0.714754</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.773043</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.808793</td>\n",
       "      <td>0.858275</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.867725</td>\n",
       "      <td>0.810099</td>\n",
       "      <td>0.934177</td>\n",
       "      <td>0.934177</td>\n",
       "      <td>0.201744</td>\n",
       "      <td>0.980262</td>\n",
       "      <td>0.924717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.897964</td>\n",
       "      <td>0.828552</td>\n",
       "      <td>0.827678</td>\n",
       "      <td>0.829120</td>\n",
       "      <td>0.838525</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.841683</td>\n",
       "      <td>0.860656</td>\n",
       "      <td>0.860656</td>\n",
       "      <td>0.783153</td>\n",
       "      <td>0.817102</td>\n",
       "      <td>0.751913</td>\n",
       "      <td>0.751913</td>\n",
       "      <td>0.826472</td>\n",
       "      <td>0.769903</td>\n",
       "      <td>0.892013</td>\n",
       "      <td>0.892013</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.873451</td>\n",
       "      <td>0.774118</td>\n",
       "      <td>0.774118</td>\n",
       "      <td>0.877278</td>\n",
       "      <td>0.843458</td>\n",
       "      <td>0.913924</td>\n",
       "      <td>0.913924</td>\n",
       "      <td>0.190957</td>\n",
       "      <td>0.982327</td>\n",
       "      <td>0.928424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0000, Val Loss: 0.9501, Val F1: 0.0000, QWK: 0.8478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0000, Val Loss: 1.1731, Val F1: 0.0000, QWK: 0.8361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0000, Val Loss: 0.9376, Val F1: 0.0000, QWK: 0.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0000, Val Loss: 0.8852, Val F1: 0.0000, QWK: 0.8905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0000, Val Loss: 0.8791, Val F1: 0.0000, QWK: 0.9072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0000, Val Loss: 0.8244, Val F1: 0.0000, QWK: 0.9171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0000, Val Loss: 0.8291, Val F1: 0.0000, QWK: 0.9179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0000, Val Loss: 0.8675, Val F1: 0.0000, QWK: 0.9195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0000, Val Loss: 0.8487, Val F1: 0.0000, QWK: 0.9276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0000, Val Loss: 0.9135, Val F1: 0.0000, QWK: 0.9247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.0000, Val Loss: 0.8980, Val F1: 0.0000, QWK: 0.9284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.6679, Val Loss: 0.8980, Val F1: 0.0000, QWK: 0.9284\n",
      "‚úÖ Trial checkpoint saved: ./checkpoints_bert/trial_4/final_epoch_11.ckpt\n",
      "‚úÖ Model files saved in: ./checkpoints_bert/trial_4\n",
      "üèÜ New best model saved! Score: 0.9284 (Trial 4)\n",
      "üèÜ Best model files saved in: ./best_bert_model_so_far\n",
      "GPU Memory Used: 4.13 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Learning_Rate</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Stage</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>Unfrozen_Layers</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation Adjacent Accuracy</td><td>‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Loss</td><td>‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>Validation MAE</td><td>‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation QWK</td><td>‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>‚ñÅ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>eval/accuracy_negative</td><td>‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/accuracy_neutral</td><td>‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/accuracy_positive</td><td>‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/adjacent_accuracy</td><td>‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_extremely_negative</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_extremely_positive</td><td>‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_negative</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/f1_neutral</td><td>‚ñÑ‚ñÅ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>eval/f1_positive</td><td>‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>eval/mae</td><td>‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/precision_extremely_negative</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÑ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá</td></tr><tr><td>eval/precision_extremely_positive</td><td>‚ñÜ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/precision_negative</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/precision_neutral</td><td>‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_positive</td><td>‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall_extremely_negative</td><td>‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ</td></tr><tr><td>eval/recall_extremely_positive</td><td>‚ñÅ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>eval/recall_negative</td><td>‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/recall_neutral</td><td>‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall_positive</td><td>‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÅ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÅ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñá‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñÉ‚ñá‚ñá‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>11</td></tr><tr><td>Learning_Rate</td><td>0.00021</td></tr><tr><td>Stage</td><td>1</td></tr><tr><td>Train Accuracy</td><td>0</td></tr><tr><td>Train Loss</td><td>0.66787</td></tr><tr><td>Unfrozen_Layers</td><td>12</td></tr><tr><td>Validation Accuracy</td><td>0.82855</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.98233</td></tr><tr><td>Validation F1</td><td>0</td></tr><tr><td>Validation Loss</td><td>0.89796</td></tr><tr><td>Validation MAE</td><td>0.19096</td></tr><tr><td>Validation Precision</td><td>0</td></tr><tr><td>Validation QWK</td><td>0.92842</td></tr><tr><td>Validation Recall</td><td>0</td></tr><tr><td>eval/accuracy</td><td>0.82855</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>0.86066</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>0.91392</td></tr><tr><td>eval/accuracy_negative</td><td>0.75191</td></tr><tr><td>eval/accuracy_neutral</td><td>0.89201</td></tr><tr><td>eval/accuracy_positive</td><td>0.77412</td></tr><tr><td>eval/adjacent_accuracy</td><td>0.98233</td></tr><tr><td>eval/f1</td><td>0.82768</td></tr><tr><td>eval/f1_extremely_negative</td><td>0.85106</td></tr><tr><td>eval/f1_extremely_positive</td><td>0.87728</td></tr><tr><td>eval/f1_negative</td><td>0.78315</td></tr><tr><td>eval/f1_neutral</td><td>0.82647</td></tr><tr><td>eval/f1_positive</td><td>0.82079</td></tr><tr><td>eval/loss</td><td>0.89796</td></tr><tr><td>eval/mae</td><td>0.19096</td></tr><tr><td>eval/precision</td><td>0.82912</td></tr><tr><td>eval/precision_extremely_negative</td><td>0.84168</td></tr><tr><td>eval/precision_extremely_positive</td><td>0.84346</td></tr><tr><td>eval/precision_negative</td><td>0.8171</td></tr><tr><td>eval/precision_neutral</td><td>0.7699</td></tr><tr><td>eval/precision_positive</td><td>0.87345</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>0.92842</td></tr><tr><td>eval/recall</td><td>0.83852</td></tr><tr><td>eval/recall_extremely_negative</td><td>0.86066</td></tr><tr><td>eval/recall_extremely_positive</td><td>0.91392</td></tr><tr><td>eval/recall_negative</td><td>0.75191</td></tr><tr><td>eval/recall_neutral</td><td>0.89201</td></tr><tr><td>eval/recall_positive</td><td>0.77412</td></tr><tr><td>eval/runtime</td><td>5.6515</td></tr><tr><td>eval/samples_per_second</td><td>770.946</td></tr><tr><td>eval/steps_per_second</td><td>3.185</td></tr><tr><td>total_flos</td><td>7041826735543056.0</td></tr><tr><td>train/epoch</td><td>11</td></tr><tr><td>train/global_step</td><td>2156</td></tr><tr><td>train/grad_norm</td><td>0.27075</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4617</td></tr><tr><td>train_loss</td><td>0.66787</td></tr><tr><td>train_runtime</td><td>821.4902</td></tr><tr><td>train_samples_per_second</td><td>334.757</td></tr><tr><td>train_steps_per_second</td><td>2.624</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_4</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/5t95b92o' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/5t95b92o</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_082452-5t95b92o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 08:39:40,696] Trial 4 finished with value: 0.9284241099301461 and parameters: {'learning_rate': 0.00021229985961337253, 'batch_size': 32, 'label_smoothing': 0.11524364144454653, 'num_epochs': 11, 'warmup_ratio': 0.13273796418047804, 'weight_decay': 0.1121797776236029, 'attention_dropout': 0.33099760772482156, 'hidden_dropout': 0.38867822735790303}. Best is trial 4 with value: 0.9284241099301461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 5 - TESTING THESE PARAMETERS:\n",
      "Learning Rate:      1.34e-04\n",
      "Epochs:             15\n",
      "Warmup Ratio:       0.105\n",
      "Weight Decay:       0.057\n",
      "Attention Dropout:  0.313\n",
      "Hidden Dropout:     0.371\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_083940-4tbf43kz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/4tbf43kz' target=\"_blank\">trial_5</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/4tbf43kz' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/4tbf43kz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1318007737.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(  # Uses ordinal loss always\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3128' max='5865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3128/5865 11:52 < 10:23, 4.39 it/s, Epoch 8/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Extremely Negative</th>\n",
       "      <th>Precision Extremely Negative</th>\n",
       "      <th>Recall Extremely Negative</th>\n",
       "      <th>Accuracy Extremely Negative</th>\n",
       "      <th>F1 Negative</th>\n",
       "      <th>Precision Negative</th>\n",
       "      <th>Recall Negative</th>\n",
       "      <th>Accuracy Negative</th>\n",
       "      <th>F1 Neutral</th>\n",
       "      <th>Precision Neutral</th>\n",
       "      <th>Recall Neutral</th>\n",
       "      <th>Accuracy Neutral</th>\n",
       "      <th>F1 Positive</th>\n",
       "      <th>Precision Positive</th>\n",
       "      <th>Recall Positive</th>\n",
       "      <th>Accuracy Positive</th>\n",
       "      <th>F1 Extremely Positive</th>\n",
       "      <th>Precision Extremely Positive</th>\n",
       "      <th>Recall Extremely Positive</th>\n",
       "      <th>Accuracy Extremely Positive</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Adjacent Accuracy</th>\n",
       "      <th>Quadratic Weighted Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>0.860762</td>\n",
       "      <td>0.744779</td>\n",
       "      <td>0.744574</td>\n",
       "      <td>0.770725</td>\n",
       "      <td>0.729283</td>\n",
       "      <td>0.681704</td>\n",
       "      <td>0.877419</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.695565</td>\n",
       "      <td>0.645463</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.808774</td>\n",
       "      <td>0.808774</td>\n",
       "      <td>0.808774</td>\n",
       "      <td>0.808774</td>\n",
       "      <td>0.726291</td>\n",
       "      <td>0.714177</td>\n",
       "      <td>0.738824</td>\n",
       "      <td>0.738824</td>\n",
       "      <td>0.797436</td>\n",
       "      <td>0.807792</td>\n",
       "      <td>0.787342</td>\n",
       "      <td>0.787342</td>\n",
       "      <td>0.317650</td>\n",
       "      <td>0.943769</td>\n",
       "      <td>0.852656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.842400</td>\n",
       "      <td>0.843558</td>\n",
       "      <td>0.768878</td>\n",
       "      <td>0.764716</td>\n",
       "      <td>0.809417</td>\n",
       "      <td>0.731901</td>\n",
       "      <td>0.604716</td>\n",
       "      <td>0.935622</td>\n",
       "      <td>0.446721</td>\n",
       "      <td>0.446721</td>\n",
       "      <td>0.714961</td>\n",
       "      <td>0.687879</td>\n",
       "      <td>0.744262</td>\n",
       "      <td>0.744262</td>\n",
       "      <td>0.861453</td>\n",
       "      <td>0.855716</td>\n",
       "      <td>0.867267</td>\n",
       "      <td>0.867267</td>\n",
       "      <td>0.771843</td>\n",
       "      <td>0.702964</td>\n",
       "      <td>0.855686</td>\n",
       "      <td>0.855686</td>\n",
       "      <td>0.800816</td>\n",
       "      <td>0.864905</td>\n",
       "      <td>0.745570</td>\n",
       "      <td>0.745570</td>\n",
       "      <td>0.290567</td>\n",
       "      <td>0.946293</td>\n",
       "      <td>0.856863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.709800</td>\n",
       "      <td>0.810686</td>\n",
       "      <td>0.795731</td>\n",
       "      <td>0.795864</td>\n",
       "      <td>0.823558</td>\n",
       "      <td>0.786350</td>\n",
       "      <td>0.812236</td>\n",
       "      <td>0.836957</td>\n",
       "      <td>0.788934</td>\n",
       "      <td>0.788934</td>\n",
       "      <td>0.767613</td>\n",
       "      <td>0.739615</td>\n",
       "      <td>0.797814</td>\n",
       "      <td>0.797814</td>\n",
       "      <td>0.864240</td>\n",
       "      <td>0.888361</td>\n",
       "      <td>0.841395</td>\n",
       "      <td>0.841395</td>\n",
       "      <td>0.782267</td>\n",
       "      <td>0.718791</td>\n",
       "      <td>0.858039</td>\n",
       "      <td>0.858039</td>\n",
       "      <td>0.763473</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.249943</td>\n",
       "      <td>0.957310</td>\n",
       "      <td>0.888109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.647000</td>\n",
       "      <td>0.822920</td>\n",
       "      <td>0.795042</td>\n",
       "      <td>0.794728</td>\n",
       "      <td>0.835013</td>\n",
       "      <td>0.779743</td>\n",
       "      <td>0.815135</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>0.772541</td>\n",
       "      <td>0.772541</td>\n",
       "      <td>0.756912</td>\n",
       "      <td>0.800244</td>\n",
       "      <td>0.718033</td>\n",
       "      <td>0.718033</td>\n",
       "      <td>0.866286</td>\n",
       "      <td>0.880372</td>\n",
       "      <td>0.852643</td>\n",
       "      <td>0.852643</td>\n",
       "      <td>0.782404</td>\n",
       "      <td>0.684087</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.765283</td>\n",
       "      <td>0.947664</td>\n",
       "      <td>0.641772</td>\n",
       "      <td>0.641772</td>\n",
       "      <td>0.252008</td>\n",
       "      <td>0.955703</td>\n",
       "      <td>0.884113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.597900</td>\n",
       "      <td>0.761421</td>\n",
       "      <td>0.826027</td>\n",
       "      <td>0.826103</td>\n",
       "      <td>0.838531</td>\n",
       "      <td>0.817752</td>\n",
       "      <td>0.809365</td>\n",
       "      <td>0.887531</td>\n",
       "      <td>0.743852</td>\n",
       "      <td>0.743852</td>\n",
       "      <td>0.774440</td>\n",
       "      <td>0.774017</td>\n",
       "      <td>0.774863</td>\n",
       "      <td>0.774863</td>\n",
       "      <td>0.867689</td>\n",
       "      <td>0.876147</td>\n",
       "      <td>0.859393</td>\n",
       "      <td>0.859393</td>\n",
       "      <td>0.816296</td>\n",
       "      <td>0.786337</td>\n",
       "      <td>0.848627</td>\n",
       "      <td>0.848627</td>\n",
       "      <td>0.865311</td>\n",
       "      <td>0.868622</td>\n",
       "      <td>0.862025</td>\n",
       "      <td>0.862025</td>\n",
       "      <td>0.215745</td>\n",
       "      <td>0.961441</td>\n",
       "      <td>0.903402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.532200</td>\n",
       "      <td>0.794335</td>\n",
       "      <td>0.824420</td>\n",
       "      <td>0.824136</td>\n",
       "      <td>0.822623</td>\n",
       "      <td>0.832750</td>\n",
       "      <td>0.836435</td>\n",
       "      <td>0.801126</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.784573</td>\n",
       "      <td>0.791111</td>\n",
       "      <td>0.778142</td>\n",
       "      <td>0.778142</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.837054</td>\n",
       "      <td>0.843645</td>\n",
       "      <td>0.843645</td>\n",
       "      <td>0.813301</td>\n",
       "      <td>0.831286</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.861616</td>\n",
       "      <td>0.852540</td>\n",
       "      <td>0.870886</td>\n",
       "      <td>0.870886</td>\n",
       "      <td>0.206335</td>\n",
       "      <td>0.972688</td>\n",
       "      <td>0.916727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.486800</td>\n",
       "      <td>0.816855</td>\n",
       "      <td>0.827634</td>\n",
       "      <td>0.828675</td>\n",
       "      <td>0.830043</td>\n",
       "      <td>0.830636</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.769369</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.759210</td>\n",
       "      <td>0.742171</td>\n",
       "      <td>0.777049</td>\n",
       "      <td>0.777049</td>\n",
       "      <td>0.868100</td>\n",
       "      <td>0.897837</td>\n",
       "      <td>0.840270</td>\n",
       "      <td>0.840270</td>\n",
       "      <td>0.831721</td>\n",
       "      <td>0.820611</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.865952</td>\n",
       "      <td>0.920228</td>\n",
       "      <td>0.817722</td>\n",
       "      <td>0.817722</td>\n",
       "      <td>0.211613</td>\n",
       "      <td>0.963737</td>\n",
       "      <td>0.909727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.869482</td>\n",
       "      <td>0.808813</td>\n",
       "      <td>0.807356</td>\n",
       "      <td>0.799792</td>\n",
       "      <td>0.827781</td>\n",
       "      <td>0.798593</td>\n",
       "      <td>0.699538</td>\n",
       "      <td>0.930328</td>\n",
       "      <td>0.930328</td>\n",
       "      <td>0.730214</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.690710</td>\n",
       "      <td>0.690710</td>\n",
       "      <td>0.847200</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.876265</td>\n",
       "      <td>0.876265</td>\n",
       "      <td>0.802022</td>\n",
       "      <td>0.866242</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.865891</td>\n",
       "      <td>0.838671</td>\n",
       "      <td>0.894937</td>\n",
       "      <td>0.894937</td>\n",
       "      <td>0.222401</td>\n",
       "      <td>0.971540</td>\n",
       "      <td>0.915033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0000, Val Loss: 0.8608, Val F1: 0.0000, QWK: 0.8527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0000, Val Loss: 0.8436, Val F1: 0.0000, QWK: 0.8569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0000, Val Loss: 0.8107, Val F1: 0.0000, QWK: 0.8881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0000, Val Loss: 0.8229, Val F1: 0.0000, QWK: 0.8841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0000, Val Loss: 0.7614, Val F1: 0.0000, QWK: 0.9034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0000, Val Loss: 0.7943, Val F1: 0.0000, QWK: 0.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0000, Val Loss: 0.8169, Val F1: 0.0000, QWK: 0.9097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0000, Val Loss: 0.8695, Val F1: 0.0000, QWK: 0.9150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.6814, Val Loss: 0.8695, Val F1: 0.0000, QWK: 0.9150\n",
      "‚úÖ Trial checkpoint saved: ./checkpoints_bert/trial_5/final_epoch_8.ckpt\n",
      "‚úÖ Model files saved in: ./checkpoints_bert/trial_5\n",
      "üìä Trial 5 score: 0.9150 (Best: 0.9284)\n",
      "GPU Memory Used: 4.13 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>Learning_Rate</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Stage</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>Unfrozen_Layers</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>Validation Adjacent Accuracy</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñà‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Loss</td><td>‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñà</td></tr><tr><td>Validation MAE</td><td>‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation QWK</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/accuracy_negative</td><td>‚ñÖ‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñá‚ñá‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy_neutral</td><td>‚ñÅ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñà</td></tr><tr><td>eval/accuracy_positive</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>eval/adjacent_accuracy</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>eval/f1_extremely_negative</td><td>‚ñÉ‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/f1_extremely_positive</td><td>‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_negative</td><td>‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÑ</td></tr><tr><td>eval/f1_neutral</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñÖ‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>eval/f1_positive</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>eval/loss</td><td>‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñà</td></tr><tr><td>eval/mae</td><td>‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñÜ‚ñá‚ñÑ‚ñÑ</td></tr><tr><td>eval/precision_extremely_negative</td><td>‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_extremely_positive</td><td>‚ñÅ‚ñÑ‚ñá‚ñà‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñÉ</td></tr><tr><td>eval/precision_negative</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñá‚ñà‚ñÖ‚ñá‚ñá</td></tr><tr><td>eval/precision_neutral</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÉ‚ñà‚ñÇ‚ñÇ</td></tr><tr><td>eval/precision_positive</td><td>‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall_extremely_negative</td><td>‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>eval/recall_extremely_positive</td><td>‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/recall_negative</td><td>‚ñÖ‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñá‚ñá‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall_neutral</td><td>‚ñÅ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñà</td></tr><tr><td>eval/recall_positive</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñÅ‚ñà‚ñÅ‚ñÑ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÖ‚ñÇ‚ñà‚ñÖ‚ñà‚ñÅ‚ñà‚ñÖ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÖ‚ñÇ‚ñà‚ñÖ‚ñà‚ñÅ‚ñà‚ñÖ‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñá</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>8</td></tr><tr><td>Learning_Rate</td><td>0.00013</td></tr><tr><td>Stage</td><td>1</td></tr><tr><td>Train Accuracy</td><td>0</td></tr><tr><td>Train Loss</td><td>0.68145</td></tr><tr><td>Unfrozen_Layers</td><td>12</td></tr><tr><td>Validation Accuracy</td><td>0.80881</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.97154</td></tr><tr><td>Validation F1</td><td>0</td></tr><tr><td>Validation Loss</td><td>0.86948</td></tr><tr><td>Validation MAE</td><td>0.2224</td></tr><tr><td>Validation Precision</td><td>0</td></tr><tr><td>Validation QWK</td><td>0.91503</td></tr><tr><td>Validation Recall</td><td>0</td></tr><tr><td>eval/accuracy</td><td>0.80881</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>0.93033</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>0.89494</td></tr><tr><td>eval/accuracy_negative</td><td>0.69071</td></tr><tr><td>eval/accuracy_neutral</td><td>0.87627</td></tr><tr><td>eval/accuracy_positive</td><td>0.74667</td></tr><tr><td>eval/adjacent_accuracy</td><td>0.97154</td></tr><tr><td>eval/f1</td><td>0.80736</td></tr><tr><td>eval/f1_extremely_negative</td><td>0.79859</td></tr><tr><td>eval/f1_extremely_positive</td><td>0.86589</td></tr><tr><td>eval/f1_negative</td><td>0.73021</td></tr><tr><td>eval/f1_neutral</td><td>0.8472</td></tr><tr><td>eval/f1_positive</td><td>0.80202</td></tr><tr><td>eval/loss</td><td>0.86948</td></tr><tr><td>eval/mae</td><td>0.2224</td></tr><tr><td>eval/precision</td><td>0.79979</td></tr><tr><td>eval/precision_extremely_negative</td><td>0.69954</td></tr><tr><td>eval/precision_extremely_positive</td><td>0.83867</td></tr><tr><td>eval/precision_negative</td><td>0.77451</td></tr><tr><td>eval/precision_neutral</td><td>0.82</td></tr><tr><td>eval/precision_positive</td><td>0.86624</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>0.91503</td></tr><tr><td>eval/recall</td><td>0.82778</td></tr><tr><td>eval/recall_extremely_negative</td><td>0.93033</td></tr><tr><td>eval/recall_extremely_positive</td><td>0.89494</td></tr><tr><td>eval/recall_negative</td><td>0.69071</td></tr><tr><td>eval/recall_neutral</td><td>0.87627</td></tr><tr><td>eval/recall_positive</td><td>0.74667</td></tr><tr><td>eval/runtime</td><td>4.9943</td></tr><tr><td>eval/samples_per_second</td><td>872.392</td></tr><tr><td>eval/steps_per_second</td><td>7.008</td></tr><tr><td>total_flos</td><td>5085855799117392.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>3128</td></tr><tr><td>train/grad_norm</td><td>6.9537</td></tr><tr><td>train/learning_rate</td><td>7e-05</td></tr><tr><td>train/loss</td><td>0.4579</td></tr><tr><td>train_loss</td><td>0.68145</td></tr><tr><td>train_runtime</td><td>712.7136</td></tr><tr><td>train_samples_per_second</td><td>526.158</td></tr><tr><td>train_steps_per_second</td><td>8.229</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_5</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/4tbf43kz' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/4tbf43kz</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_083940-4tbf43kz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 08:52:26,345] Trial 5 finished with value: 0.9150326873962199 and parameters: {'learning_rate': 0.00013353508229757926, 'batch_size': 16, 'label_smoothing': 0.08395801184157484, 'num_epochs': 15, 'warmup_ratio': 0.1053887370783656, 'weight_decay': 0.057452381309670064, 'attention_dropout': 0.31314073094885464, 'hidden_dropout': 0.37098355380967046}. Best is trial 4 with value: 0.9284241099301461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 6 - TESTING THESE PARAMETERS:\n",
      "Learning Rate:      5.89e-05\n",
      "Epochs:             13\n",
      "Warmup Ratio:       0.127\n",
      "Weight Decay:       0.127\n",
      "Attention Dropout:  0.369\n",
      "Hidden Dropout:     0.354\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_085226-rui397ud</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/rui397ud' target=\"_blank\">trial_6</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/rui397ud' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/rui397ud</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1318007737.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(  # Uses ordinal loss always\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='588' max='1274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 588/1274 07:06 < 08:19, 1.37 it/s, Epoch 6/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Extremely Negative</th>\n",
       "      <th>Precision Extremely Negative</th>\n",
       "      <th>Recall Extremely Negative</th>\n",
       "      <th>Accuracy Extremely Negative</th>\n",
       "      <th>F1 Negative</th>\n",
       "      <th>Precision Negative</th>\n",
       "      <th>Recall Negative</th>\n",
       "      <th>Accuracy Negative</th>\n",
       "      <th>F1 Neutral</th>\n",
       "      <th>Precision Neutral</th>\n",
       "      <th>Recall Neutral</th>\n",
       "      <th>Accuracy Neutral</th>\n",
       "      <th>F1 Positive</th>\n",
       "      <th>Precision Positive</th>\n",
       "      <th>Recall Positive</th>\n",
       "      <th>Accuracy Positive</th>\n",
       "      <th>F1 Extremely Positive</th>\n",
       "      <th>Precision Extremely Positive</th>\n",
       "      <th>Recall Extremely Positive</th>\n",
       "      <th>Accuracy Extremely Positive</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Adjacent Accuracy</th>\n",
       "      <th>Quadratic Weighted Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.084726</td>\n",
       "      <td>0.636218</td>\n",
       "      <td>0.631181</td>\n",
       "      <td>0.645600</td>\n",
       "      <td>0.661586</td>\n",
       "      <td>0.721480</td>\n",
       "      <td>0.723711</td>\n",
       "      <td>0.719262</td>\n",
       "      <td>0.719262</td>\n",
       "      <td>0.605435</td>\n",
       "      <td>0.602162</td>\n",
       "      <td>0.608743</td>\n",
       "      <td>0.608743</td>\n",
       "      <td>0.705372</td>\n",
       "      <td>0.681342</td>\n",
       "      <td>0.731159</td>\n",
       "      <td>0.731159</td>\n",
       "      <td>0.517749</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.469020</td>\n",
       "      <td>0.469020</td>\n",
       "      <td>0.704805</td>\n",
       "      <td>0.643006</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>0.473261</td>\n",
       "      <td>0.908653</td>\n",
       "      <td>0.781145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.369800</td>\n",
       "      <td>0.900804</td>\n",
       "      <td>0.753959</td>\n",
       "      <td>0.753529</td>\n",
       "      <td>0.787928</td>\n",
       "      <td>0.732571</td>\n",
       "      <td>0.707046</td>\n",
       "      <td>0.890966</td>\n",
       "      <td>0.586066</td>\n",
       "      <td>0.586066</td>\n",
       "      <td>0.673626</td>\n",
       "      <td>0.677348</td>\n",
       "      <td>0.669945</td>\n",
       "      <td>0.669945</td>\n",
       "      <td>0.803688</td>\n",
       "      <td>0.775916</td>\n",
       "      <td>0.833521</td>\n",
       "      <td>0.833521</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.693570</td>\n",
       "      <td>0.829020</td>\n",
       "      <td>0.829020</td>\n",
       "      <td>0.815534</td>\n",
       "      <td>0.901840</td>\n",
       "      <td>0.744304</td>\n",
       "      <td>0.744304</td>\n",
       "      <td>0.305944</td>\n",
       "      <td>0.943998</td>\n",
       "      <td>0.855164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.952100</td>\n",
       "      <td>0.831594</td>\n",
       "      <td>0.795961</td>\n",
       "      <td>0.795375</td>\n",
       "      <td>0.799752</td>\n",
       "      <td>0.800930</td>\n",
       "      <td>0.804642</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.718841</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.677596</td>\n",
       "      <td>0.677596</td>\n",
       "      <td>0.840693</td>\n",
       "      <td>0.835556</td>\n",
       "      <td>0.845894</td>\n",
       "      <td>0.845894</td>\n",
       "      <td>0.783037</td>\n",
       "      <td>0.756955</td>\n",
       "      <td>0.810980</td>\n",
       "      <td>0.810980</td>\n",
       "      <td>0.847213</td>\n",
       "      <td>0.878912</td>\n",
       "      <td>0.817722</td>\n",
       "      <td>0.817722</td>\n",
       "      <td>0.250172</td>\n",
       "      <td>0.955933</td>\n",
       "      <td>0.893436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.807022</td>\n",
       "      <td>0.826027</td>\n",
       "      <td>0.826540</td>\n",
       "      <td>0.849037</td>\n",
       "      <td>0.811200</td>\n",
       "      <td>0.794842</td>\n",
       "      <td>0.928767</td>\n",
       "      <td>0.694672</td>\n",
       "      <td>0.694672</td>\n",
       "      <td>0.782105</td>\n",
       "      <td>0.754315</td>\n",
       "      <td>0.812022</td>\n",
       "      <td>0.812022</td>\n",
       "      <td>0.864524</td>\n",
       "      <td>0.882767</td>\n",
       "      <td>0.847019</td>\n",
       "      <td>0.847019</td>\n",
       "      <td>0.817204</td>\n",
       "      <td>0.774965</td>\n",
       "      <td>0.864314</td>\n",
       "      <td>0.864314</td>\n",
       "      <td>0.869908</td>\n",
       "      <td>0.904372</td>\n",
       "      <td>0.837975</td>\n",
       "      <td>0.837975</td>\n",
       "      <td>0.211384</td>\n",
       "      <td>0.963737</td>\n",
       "      <td>0.907429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.711100</td>\n",
       "      <td>0.843612</td>\n",
       "      <td>0.806059</td>\n",
       "      <td>0.805081</td>\n",
       "      <td>0.804970</td>\n",
       "      <td>0.814852</td>\n",
       "      <td>0.793767</td>\n",
       "      <td>0.718076</td>\n",
       "      <td>0.887295</td>\n",
       "      <td>0.887295</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.668852</td>\n",
       "      <td>0.668852</td>\n",
       "      <td>0.852071</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.809899</td>\n",
       "      <td>0.809899</td>\n",
       "      <td>0.798930</td>\n",
       "      <td>0.779269</td>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.859241</td>\n",
       "      <td>0.831754</td>\n",
       "      <td>0.888608</td>\n",
       "      <td>0.888608</td>\n",
       "      <td>0.233417</td>\n",
       "      <td>0.962589</td>\n",
       "      <td>0.906395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>0.819498</td>\n",
       "      <td>0.814322</td>\n",
       "      <td>0.815250</td>\n",
       "      <td>0.822282</td>\n",
       "      <td>0.810267</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.829646</td>\n",
       "      <td>0.768443</td>\n",
       "      <td>0.768443</td>\n",
       "      <td>0.764858</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.808743</td>\n",
       "      <td>0.808743</td>\n",
       "      <td>0.843931</td>\n",
       "      <td>0.868014</td>\n",
       "      <td>0.821147</td>\n",
       "      <td>0.821147</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.803695</td>\n",
       "      <td>0.818824</td>\n",
       "      <td>0.818824</td>\n",
       "      <td>0.858632</td>\n",
       "      <td>0.884564</td>\n",
       "      <td>0.834177</td>\n",
       "      <td>0.834177</td>\n",
       "      <td>0.221942</td>\n",
       "      <td>0.965573</td>\n",
       "      <td>0.907388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0000, Val Loss: 1.0847, Val F1: 0.0000, QWK: 0.7811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0000, Val Loss: 0.9008, Val F1: 0.0000, QWK: 0.8552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0000, Val Loss: 0.8316, Val F1: 0.0000, QWK: 0.8934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0000, Val Loss: 0.8070, Val F1: 0.0000, QWK: 0.9074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0000, Val Loss: 0.8436, Val F1: 0.0000, QWK: 0.9064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0000, Val Loss: 0.8195, Val F1: 0.0000, QWK: 0.9074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.8528, Val Loss: 0.8195, Val F1: 0.0000, QWK: 0.9074\n",
      "‚úÖ Trial checkpoint saved: ./checkpoints_bert/trial_6/final_epoch_6.ckpt\n",
      "‚úÖ Model files saved in: ./checkpoints_bert/trial_6\n",
      "üìä Trial 6 score: 0.9074 (Best: 0.9284)\n",
      "GPU Memory Used: 4.13 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà</td></tr><tr><td>Learning_Rate</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Stage</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>Unfrozen_Layers</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>Validation Adjacent Accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>Validation MAE</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation QWK</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>‚ñÑ‚ñÅ‚ñá‚ñÑ‚ñà‚ñÖ‚ñÖ</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÖ</td></tr><tr><td>eval/accuracy_negative</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñà‚ñà</td></tr><tr><td>eval/accuracy_neutral</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>eval/accuracy_positive</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/adjacent_accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/f1_extremely_negative</td><td>‚ñÇ‚ñÅ‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>eval/f1_extremely_positive</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1_negative</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/f1_neutral</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/f1_positive</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/mae</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/precision_extremely_negative</td><td>‚ñÅ‚ñá‚ñÇ‚ñà‚ñÅ‚ñÖ‚ñÖ</td></tr><tr><td>eval/precision_extremely_positive</td><td>‚ñÅ‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñá</td></tr><tr><td>eval/precision_negative</td><td>‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñà‚ñÖ‚ñÖ</td></tr><tr><td>eval/precision_neutral</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñá</td></tr><tr><td>eval/precision_positive</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall_extremely_negative</td><td>‚ñÑ‚ñÅ‚ñá‚ñÑ‚ñà‚ñÖ‚ñÖ</td></tr><tr><td>eval/recall_extremely_positive</td><td>‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÖ</td></tr><tr><td>eval/recall_negative</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñà‚ñà</td></tr><tr><td>eval/recall_neutral</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>eval/recall_positive</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÜ‚ñÖ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÜ‚ñÜ‚ñÅ‚ñá‚ñÉ‚ñÑ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÜ‚ñÜ‚ñÅ‚ñá‚ñÉ‚ñÑ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñà‚ñà‚ñÖ‚ñÉ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñà‚ñá‚ñÜ‚ñÑ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>6</td></tr><tr><td>Learning_Rate</td><td>6e-05</td></tr><tr><td>Stage</td><td>1</td></tr><tr><td>Train Accuracy</td><td>0</td></tr><tr><td>Train Loss</td><td>0.85279</td></tr><tr><td>Unfrozen_Layers</td><td>12</td></tr><tr><td>Validation Accuracy</td><td>0.81432</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.96557</td></tr><tr><td>Validation F1</td><td>0</td></tr><tr><td>Validation Loss</td><td>0.8195</td></tr><tr><td>Validation MAE</td><td>0.22194</td></tr><tr><td>Validation Precision</td><td>0</td></tr><tr><td>Validation QWK</td><td>0.90739</td></tr><tr><td>Validation Recall</td><td>0</td></tr><tr><td>eval/accuracy</td><td>0.81432</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>0.76844</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>0.83418</td></tr><tr><td>eval/accuracy_negative</td><td>0.80874</td></tr><tr><td>eval/accuracy_neutral</td><td>0.82115</td></tr><tr><td>eval/accuracy_positive</td><td>0.81882</td></tr><tr><td>eval/adjacent_accuracy</td><td>0.96557</td></tr><tr><td>eval/f1</td><td>0.81525</td></tr><tr><td>eval/f1_extremely_negative</td><td>0.79787</td></tr><tr><td>eval/f1_extremely_positive</td><td>0.85863</td></tr><tr><td>eval/f1_negative</td><td>0.76486</td></tr><tr><td>eval/f1_neutral</td><td>0.84393</td></tr><tr><td>eval/f1_positive</td><td>0.81119</td></tr><tr><td>eval/loss</td><td>0.8195</td></tr><tr><td>eval/mae</td><td>0.22194</td></tr><tr><td>eval/precision</td><td>0.82228</td></tr><tr><td>eval/precision_extremely_negative</td><td>0.82965</td></tr><tr><td>eval/precision_extremely_positive</td><td>0.88456</td></tr><tr><td>eval/precision_negative</td><td>0.72549</td></tr><tr><td>eval/precision_neutral</td><td>0.86801</td></tr><tr><td>eval/precision_positive</td><td>0.8037</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>0.90739</td></tr><tr><td>eval/recall</td><td>0.81027</td></tr><tr><td>eval/recall_extremely_negative</td><td>0.76844</td></tr><tr><td>eval/recall_extremely_positive</td><td>0.83418</td></tr><tr><td>eval/recall_negative</td><td>0.80874</td></tr><tr><td>eval/recall_neutral</td><td>0.82115</td></tr><tr><td>eval/recall_positive</td><td>0.81882</td></tr><tr><td>eval/runtime</td><td>6.3937</td></tr><tr><td>eval/samples_per_second</td><td>681.455</td></tr><tr><td>eval/steps_per_second</td><td>1.408</td></tr><tr><td>total_flos</td><td>3979785241397280.0</td></tr><tr><td>train/epoch</td><td>6</td></tr><tr><td>train/global_step</td><td>588</td></tr><tr><td>train/grad_norm</td><td>3.52568</td></tr><tr><td>train/learning_rate</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.6328</td></tr><tr><td>train_loss</td><td>0.85279</td></tr><tr><td>train_runtime</td><td>428.7466</td></tr><tr><td>train_samples_per_second</td><td>758.024</td></tr><tr><td>train_steps_per_second</td><td>2.971</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_6</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/rui397ud' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/rui397ud</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_085226-rui397ud/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 09:00:07,564] Trial 6 finished with value: 0.9073878147560263 and parameters: {'learning_rate': 5.889698182923651e-05, 'batch_size': 64, 'label_smoothing': 0.11649037836869071, 'num_epochs': 13, 'warmup_ratio': 0.12737001868114928, 'weight_decay': 0.12661182033842114, 'attention_dropout': 0.36941769572138095, 'hidden_dropout': 0.35417641175158643}. Best is trial 4 with value: 0.9284241099301461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 7 - TESTING THESE PARAMETERS:\n",
      "Learning Rate:      3.90e-04\n",
      "Epochs:             14\n",
      "Warmup Ratio:       0.060\n",
      "Weight Decay:       0.146\n",
      "Attention Dropout:  0.372\n",
      "Hidden Dropout:     0.367\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_090007-ckd0peyw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/ckd0peyw' target=\"_blank\">trial_7</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/ckd0peyw' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/ckd0peyw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1318007737.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(  # Uses ordinal loss always\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='784' max='2744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 784/2744 04:56 < 12:22, 2.64 it/s, Epoch 4/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Extremely Negative</th>\n",
       "      <th>Precision Extremely Negative</th>\n",
       "      <th>Recall Extremely Negative</th>\n",
       "      <th>Accuracy Extremely Negative</th>\n",
       "      <th>F1 Negative</th>\n",
       "      <th>Precision Negative</th>\n",
       "      <th>Recall Negative</th>\n",
       "      <th>Accuracy Negative</th>\n",
       "      <th>F1 Neutral</th>\n",
       "      <th>Precision Neutral</th>\n",
       "      <th>Recall Neutral</th>\n",
       "      <th>Accuracy Neutral</th>\n",
       "      <th>F1 Positive</th>\n",
       "      <th>Precision Positive</th>\n",
       "      <th>Recall Positive</th>\n",
       "      <th>Accuracy Positive</th>\n",
       "      <th>F1 Extremely Positive</th>\n",
       "      <th>Precision Extremely Positive</th>\n",
       "      <th>Recall Extremely Positive</th>\n",
       "      <th>Accuracy Extremely Positive</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Adjacent Accuracy</th>\n",
       "      <th>Quadratic Weighted Kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.243500</td>\n",
       "      <td>1.131034</td>\n",
       "      <td>0.669038</td>\n",
       "      <td>0.665516</td>\n",
       "      <td>0.706453</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.389344</td>\n",
       "      <td>0.389344</td>\n",
       "      <td>0.590832</td>\n",
       "      <td>0.612676</td>\n",
       "      <td>0.570492</td>\n",
       "      <td>0.570492</td>\n",
       "      <td>0.799540</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.782902</td>\n",
       "      <td>0.782902</td>\n",
       "      <td>0.654071</td>\n",
       "      <td>0.581907</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.705658</td>\n",
       "      <td>0.708812</td>\n",
       "      <td>0.702532</td>\n",
       "      <td>0.702532</td>\n",
       "      <td>0.470966</td>\n",
       "      <td>0.892128</td>\n",
       "      <td>0.724534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.093100</td>\n",
       "      <td>1.054831</td>\n",
       "      <td>0.676842</td>\n",
       "      <td>0.673687</td>\n",
       "      <td>0.712414</td>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.534266</td>\n",
       "      <td>0.841410</td>\n",
       "      <td>0.391393</td>\n",
       "      <td>0.391393</td>\n",
       "      <td>0.636961</td>\n",
       "      <td>0.557929</td>\n",
       "      <td>0.742077</td>\n",
       "      <td>0.742077</td>\n",
       "      <td>0.761578</td>\n",
       "      <td>0.774419</td>\n",
       "      <td>0.749156</td>\n",
       "      <td>0.749156</td>\n",
       "      <td>0.647413</td>\n",
       "      <td>0.662562</td>\n",
       "      <td>0.632941</td>\n",
       "      <td>0.632941</td>\n",
       "      <td>0.745846</td>\n",
       "      <td>0.725749</td>\n",
       "      <td>0.767089</td>\n",
       "      <td>0.767089</td>\n",
       "      <td>0.413817</td>\n",
       "      <td>0.920358</td>\n",
       "      <td>0.799576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.431200</td>\n",
       "      <td>1.659967</td>\n",
       "      <td>0.366537</td>\n",
       "      <td>0.301458</td>\n",
       "      <td>0.435534</td>\n",
       "      <td>0.412117</td>\n",
       "      <td>0.504418</td>\n",
       "      <td>0.414795</td>\n",
       "      <td>0.643443</td>\n",
       "      <td>0.643443</td>\n",
       "      <td>0.060362</td>\n",
       "      <td>0.379747</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.455829</td>\n",
       "      <td>0.302351</td>\n",
       "      <td>0.925759</td>\n",
       "      <td>0.925759</td>\n",
       "      <td>0.211527</td>\n",
       "      <td>0.436275</td>\n",
       "      <td>0.139608</td>\n",
       "      <td>0.139608</td>\n",
       "      <td>0.426757</td>\n",
       "      <td>0.644501</td>\n",
       "      <td>0.318987</td>\n",
       "      <td>0.318987</td>\n",
       "      <td>0.883865</td>\n",
       "      <td>0.816846</td>\n",
       "      <td>0.460083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.632000</td>\n",
       "      <td>1.807292</td>\n",
       "      <td>0.204039</td>\n",
       "      <td>0.069154</td>\n",
       "      <td>0.040808</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338925</td>\n",
       "      <td>0.204039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.089282</td>\n",
       "      <td>0.706679</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0000, Val Loss: 1.1310, Val F1: 0.0000, QWK: 0.7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0000, Val Loss: 1.0548, Val F1: 0.0000, QWK: 0.7996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0000, Val Loss: 1.6600, Val F1: 0.0000, QWK: 0.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0000, Val Loss: 1.8073, Val F1: 0.0000, QWK: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 1.2908, Val Loss: 1.8073, Val F1: 0.0000, QWK: 0.0000\n",
      "‚úÖ Trial checkpoint saved: ./checkpoints_bert/trial_7/final_epoch_4.ckpt\n",
      "‚úÖ Model files saved in: ./checkpoints_bert/trial_7\n",
      "üìä Trial 7 score: 0.0000 (Best: 0.9284)\n",
      "GPU Memory Used: 4.13 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñà</td></tr><tr><td>Learning_Rate</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Stage</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Train Loss</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>Unfrozen_Layers</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñà‚ñà‚ñÉ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Adjacent Accuracy</td><td>‚ñá‚ñà‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Loss</td><td>‚ñÇ‚ñÅ‚ñá‚ñà‚ñà</td></tr><tr><td>Validation MAE</td><td>‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñà</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation QWK</td><td>‚ñá‚ñà‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy</td><td>‚ñà‚ñà‚ñÉ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>‚ñÖ‚ñÖ‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>‚ñá‚ñà‚ñÑ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy_negative</td><td>‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/accuracy_neutral</td><td>‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/accuracy_positive</td><td>‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/adjacent_accuracy</td><td>‚ñá‚ñà‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1</td><td>‚ñà‚ñà‚ñÑ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1_extremely_negative</td><td>‚ñà‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1_extremely_positive</td><td>‚ñà‚ñà‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1_negative</td><td>‚ñá‚ñà‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1_neutral</td><td>‚ñà‚ñá‚ñÉ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1_positive</td><td>‚ñà‚ñà‚ñÉ‚ñÅ‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñÇ‚ñÅ‚ñá‚ñà‚ñà</td></tr><tr><td>eval/mae</td><td>‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/precision</td><td>‚ñà‚ñà‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_extremely_negative</td><td>‚ñà‚ñà‚ñÑ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_extremely_positive</td><td>‚ñà‚ñà‚ñá‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_negative</td><td>‚ñà‚ñá‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_neutral</td><td>‚ñà‚ñà‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision_positive</td><td>‚ñá‚ñà‚ñÜ‚ñÅ‚ñÅ</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>‚ñá‚ñà‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñà‚ñà‚ñÑ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall_extremely_negative</td><td>‚ñÖ‚ñÖ‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall_extremely_positive</td><td>‚ñá‚ñà‚ñÑ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall_negative</td><td>‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall_neutral</td><td>‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñà</td></tr><tr><td>eval/recall_positive</td><td>‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñá‚ñÜ‚ñá‚ñÅ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñá‚ñÜ‚ñá‚ñÅ‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÉ‚ñÉ‚ñà ‚ñÜ‚ñÜ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÉ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>4</td></tr><tr><td>Learning_Rate</td><td>0.00039</td></tr><tr><td>Stage</td><td>1</td></tr><tr><td>Train Accuracy</td><td>0</td></tr><tr><td>Train Loss</td><td>1.29082</td></tr><tr><td>Unfrozen_Layers</td><td>12</td></tr><tr><td>Validation Accuracy</td><td>0.20404</td></tr><tr><td>Validation Adjacent Accuracy</td><td>0.70668</td></tr><tr><td>Validation F1</td><td>0</td></tr><tr><td>Validation Loss</td><td>1.80729</td></tr><tr><td>Validation MAE</td><td>1.08928</td></tr><tr><td>Validation Precision</td><td>0</td></tr><tr><td>Validation QWK</td><td>0</td></tr><tr><td>Validation Recall</td><td>0</td></tr><tr><td>eval/accuracy</td><td>0.20404</td></tr><tr><td>eval/accuracy_extremely_negative</td><td>0</td></tr><tr><td>eval/accuracy_extremely_positive</td><td>0</td></tr><tr><td>eval/accuracy_negative</td><td>0</td></tr><tr><td>eval/accuracy_neutral</td><td>1</td></tr><tr><td>eval/accuracy_positive</td><td>0</td></tr><tr><td>eval/adjacent_accuracy</td><td>0.70668</td></tr><tr><td>eval/f1</td><td>0.06915</td></tr><tr><td>eval/f1_extremely_negative</td><td>0</td></tr><tr><td>eval/f1_extremely_positive</td><td>0</td></tr><tr><td>eval/f1_negative</td><td>0</td></tr><tr><td>eval/f1_neutral</td><td>0.33892</td></tr><tr><td>eval/f1_positive</td><td>0</td></tr><tr><td>eval/loss</td><td>1.80729</td></tr><tr><td>eval/mae</td><td>1.08928</td></tr><tr><td>eval/precision</td><td>0.04081</td></tr><tr><td>eval/precision_extremely_negative</td><td>0</td></tr><tr><td>eval/precision_extremely_positive</td><td>0</td></tr><tr><td>eval/precision_negative</td><td>0</td></tr><tr><td>eval/precision_neutral</td><td>0.20404</td></tr><tr><td>eval/precision_positive</td><td>0</td></tr><tr><td>eval/quadratic_weighted_kappa</td><td>0</td></tr><tr><td>eval/recall</td><td>0.2</td></tr><tr><td>eval/recall_extremely_negative</td><td>0</td></tr><tr><td>eval/recall_extremely_positive</td><td>0</td></tr><tr><td>eval/recall_negative</td><td>0</td></tr><tr><td>eval/recall_neutral</td><td>1</td></tr><tr><td>eval/recall_positive</td><td>0</td></tr><tr><td>eval/runtime</td><td>5.4852</td></tr><tr><td>eval/samples_per_second</td><td>794.323</td></tr><tr><td>eval/steps_per_second</td><td>3.282</td></tr><tr><td>total_flos</td><td>2555835077812848.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>784</td></tr><tr><td>train/grad_norm</td><td>0.17709</td></tr><tr><td>train/learning_rate</td><td>0.00035</td></tr><tr><td>train/loss</td><td>1.632</td></tr><tr><td>train_loss</td><td>1.29082</td></tr><tr><td>train_runtime</td><td>297.4465</td></tr><tr><td>train_samples_per_second</td><td>1176.682</td></tr><tr><td>train_steps_per_second</td><td>9.225</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_7</strong> at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/ckd0peyw' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss/runs/ckd0peyw</a><br> View project at: <a href='https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/covid-tweet-sentiment-hf-bert-regularloss</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_090007-ckd0peyw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 09:05:49,368] Trial 7 finished with value: 0.0 and parameters: {'learning_rate': 0.0003897773320704888, 'batch_size': 32, 'label_smoothing': 0.08451958274914737, 'num_epochs': 14, 'warmup_ratio': 0.06030116339682273, 'weight_decay': 0.14619808960138364, 'attention_dropout': 0.37166988585343874, 'hidden_dropout': 0.3669526106323674}. Best is trial 4 with value: 0.9284241099301461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters saved to best_params_bertweet_base.json\n",
      "Best score: 0.9284\n",
      "Best params: {'learning_rate': 0.00021229985961337253, 'batch_size': 32, 'label_smoothing': 0.11524364144454653, 'num_epochs': 11, 'warmup_ratio': 0.13273796418047804, 'weight_decay': 0.1121797776236029, 'attention_dropout': 0.33099760772482156, 'hidden_dropout': 0.38867822735790303}\n",
      "Completed 8/8 trials\n"
     ]
    }
   ],
   "source": [
    "# Run optimization\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=8)\n",
    "\n",
    "bert_results = save_best_hyperparameters(study, \"bertweet_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8f0dbd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f8f0dbd0",
    "outputId": "5537aede-8515-438f-85cc-5b737d135cf0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3255646991.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  test_trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='214' max='214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [214/214 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/deep-learning-project/wandb/run-20250801_090558-7jm9kwdk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayachn3-maya-bondar/huggingface/runs/7jm9kwdk' target=\"_blank\">./temp</a></strong> to <a href='https://wandb.ai/mayachn3-maya-bondar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayachn3-maya-bondar/huggingface' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayachn3-maya-bondar/huggingface/runs/7jm9kwdk' target=\"_blank\">https://wandb.ai/mayachn3-maya-bondar/huggingface/runs/7jm9kwdk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL TEST SET EVALUATION - COMPREHENSIVE ANALYSIS\n",
      "{'eval_loss': 0.6327186822891235, 'eval_model_preparation_time': 0.0037, 'eval_accuracy': 0.8233060747663551, 'eval_f1': 0.8223617028680927, 'eval_precision': 0.8209153033370565, 'eval_recall': 0.8357680469803682, 'eval_f1_extremely_negative': 0.8286384976525821, 'eval_precision_extremely_negative': 0.7986425339366516, 'eval_recall_extremely_negative': 0.8609756097560975, 'eval_accuracy_extremely_negative': 0.8609756097560975, 'eval_f1_negative': 0.7915869980879541, 'eval_precision_negative': 0.8214285714285714, 'eval_recall_negative': 0.7638376383763837, 'eval_accuracy_negative': 0.7638376383763837, 'eval_f1_neutral': 0.8396624472573839, 'eval_precision_neutral': 0.7824377457404981, 'eval_recall_neutral': 0.9059180576631259, 'eval_accuracy_neutral': 0.9059180576631259, 'eval_f1_positive': 0.8100278551532033, 'eval_precision_positive': 0.8644470868014269, 'eval_recall_positive': 0.7620545073375262, 'eval_accuracy_positive': 0.7620545073375262, 'eval_f1_extremely_positive': 0.8611570247933884, 'eval_precision_extremely_positive': 0.837620578778135, 'eval_recall_extremely_positive': 0.8860544217687075, 'eval_accuracy_extremely_positive': 0.8860544217687075, 'eval_mae': 0.20210280373831777, 'eval_adjacent_accuracy': 0.9763434579439252, 'eval_quadratic_weighted_kappa': 0.9235379118704358, 'eval_runtime': 6.5164, 'eval_samples_per_second': 525.44, 'eval_steps_per_second': 32.84}\n",
      "\n",
      "OVERALL CLASSIFICATION METRICS:\n",
      "Accuracy:           0.8233\n",
      "Validation QWK:     0.9235\n",
      "F1:                 0.8224\n",
      "Precision-Macro:    0.8209\n",
      "Recall-Macro:       0.8358\n",
      "\n",
      "ORDINAL-AWARE METRICS:\n",
      "Mean Absolute Error:        0.2021\n",
      "Adjacent Accuracy:          0.9763\n",
      "Quadratic Weighted Kappa:   0.9235\n",
      "\n",
      "PERFORMANCE METRICS:\n",
      "Inference Time:             0.0098 sec\n",
      "Model Size:                 2060.6 MB\n",
      "\n",
      "PER-CLASS PERFORMANCE ANALYSIS:\n",
      "\n",
      "Extremely Negative:\n",
      "  F1-Score:   0.8286\n",
      "  Precision:  0.7986\n",
      "  Recall:     0.8610\n",
      "  Accuracy:   0.8610\n",
      "\n",
      "Negative:\n",
      "  F1-Score:   0.7916\n",
      "  Precision:  0.8214\n",
      "  Recall:     0.7638\n",
      "  Accuracy:   0.7638\n",
      "\n",
      "Neutral:\n",
      "  F1-Score:   0.8397\n",
      "  Precision:  0.7824\n",
      "  Recall:     0.9059\n",
      "  Accuracy:   0.9059\n",
      "\n",
      "Positive:\n",
      "  F1-Score:   0.8100\n",
      "  Precision:  0.8644\n",
      "  Recall:     0.7621\n",
      "  Accuracy:   0.7621\n",
      "\n",
      "Extremely Positive:\n",
      "  F1-Score:   0.8612\n",
      "  Precision:  0.8376\n",
      "  Recall:     0.8861\n",
      "  Accuracy:   0.8861\n",
      "\n",
      "PERFORMANCE INSIGHTS:\n",
      "‚Ä¢ MAE 0.20: On average off by 0.20 sentiment levels\n",
      "‚Ä¢ Adjacent Accuracy 97.6%: Predictions within 1 sentiment level\n",
      "‚Ä¢ QWK 0.924: Excellent ordinal agreement\n",
      "‚Ä¢ Inference Speed: 101.6 predictions per second\n",
      "‚Ä¢ Model Efficiency: 2060.6 MB storage required\n",
      "\n",
      "CLASS-SPECIFIC INSIGHTS:\n",
      "‚Ä¢ Best performing class: Extremely Positive (F1: 0.8612)\n",
      "‚Ä¢ Most challenging class: Negative (F1: 0.7916)\n",
      "\n",
      "COVID SENTIMENT INSIGHTS:\n",
      "‚Ä¢ Extreme emotions (avg F1: 0.845): Well-handled\n",
      "‚Ä¢ Moderate emotions (avg F1: 0.801): Good performance\n",
      "‚Ä¢ Neutral sentiment (F1: 0.840): Well-identified\n",
      "\n",
      "üìä FINAL SUMMARY:\n",
      "F1: 0.8224 | QWK: 0.9235 | Inference: 0.0098s | Size: 2060.6MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "def calculate_model_metrics(model_path=\"./best_model_so_far\"):\n",
    "    \"\"\"Calculate inference time and model size\"\"\"\n",
    "\n",
    "    # 1. Calculate Model Size\n",
    "    def get_model_size_mb(path):\n",
    "        total_size = 0\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                total_size += os.path.getsize(filepath)\n",
    "        return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "    model_size_mb = get_model_size_mb(model_path)\n",
    "\n",
    "    # 2. Calculate Inference Time\n",
    "    def measure_inference_time():\n",
    "        # Load model for timing\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        tokenizer_for_timing = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\")\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Sample text for timing\n",
    "        sample_text = \"COVID vaccines are helping to reduce hospitalizations significantly.\"\n",
    "\n",
    "        # Warm-up runs (don't count these)\n",
    "        for _ in range(3):\n",
    "            inputs = tokenizer_for_timing(sample_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            if device.type == \"cuda\":\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                _ = model(**inputs)\n",
    "\n",
    "        # Actual timing runs\n",
    "        times = []\n",
    "        num_runs = 10\n",
    "\n",
    "        for _ in range(num_runs):\n",
    "            inputs = tokenizer_for_timing(sample_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            if device.type == \"cuda\":\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            start_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                _ = model(**inputs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            times.append(end_time - start_time)\n",
    "\n",
    "        # Return average inference time\n",
    "        avg_inference_time = sum(times) / len(times)\n",
    "        return avg_inference_time\n",
    "\n",
    "    inference_time = measure_inference_time()\n",
    "\n",
    "    return model_size_mb, inference_time\n",
    "\n",
    "# Load the best model\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "\n",
    "# Create trainer for test evaluation\n",
    "test_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./temp\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        remove_unused_columns=False,\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_detailed_metrics,  # Use the enhanced function!\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = test_trainer.evaluate(test_dataset)\n",
    "\n",
    "# Calculate performance metrics\n",
    "model_size_mb, inference_time_sec = calculate_model_metrics(best_model_path)\n",
    "\n",
    "print(\"FINAL TEST SET EVALUATION - COMPREHENSIVE ANALYSIS\")\n",
    "\n",
    "# Standard Classification Metrics\n",
    "print(test_results)\n",
    "print(\"\\nOVERALL CLASSIFICATION METRICS:\")\n",
    "print(f\"Accuracy:           {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Validation QWK:     {test_results['eval_quadratic_weighted_kappa']:.4f}\")\n",
    "print(f\"F1:                 {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Precision-Macro:    {test_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall-Macro:       {test_results['eval_recall']:.4f}\")\n",
    "\n",
    "# Ordinal-Aware Metrics\n",
    "print(\"\\nORDINAL-AWARE METRICS:\")\n",
    "print(f\"Mean Absolute Error:        {test_results['eval_mae']:.4f}\")\n",
    "print(f\"Adjacent Accuracy:          {test_results['eval_adjacent_accuracy']:.4f}\")\n",
    "print(f\"Quadratic Weighted Kappa:   {test_results['eval_quadratic_weighted_kappa']:.4f}\")\n",
    "\n",
    "# Performance Metrics\n",
    "print(\"\\nPERFORMANCE METRICS:\")\n",
    "print(f\"Inference Time:             {inference_time_sec:.4f} sec\")\n",
    "print(f\"Model Size:                 {model_size_mb:.1f} MB\")\n",
    "\n",
    "# Per-Class Detailed Analysis (PRESERVED)\n",
    "print(\"\\nPER-CLASS PERFORMANCE ANALYSIS:\")\n",
    "sentiment_classes = [\"extremely_negative\", \"negative\", \"neutral\", \"positive\", \"extremely_positive\"]\n",
    "class_display_names = [\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Extremely Positive\"]\n",
    "\n",
    "for i, (class_key, class_name) in enumerate(zip(sentiment_classes, class_display_names)):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  F1-Score:   {test_results.get(f'eval_f1_{class_key}', 0):.4f}\")\n",
    "    print(f\"  Precision:  {test_results.get(f'eval_precision_{class_key}', 0):.4f}\")\n",
    "    print(f\"  Recall:     {test_results.get(f'eval_accuracy_{class_key}', 0):.4f}\")\n",
    "    print(f\"  Accuracy:   {test_results.get(f'eval_accuracy_{class_key}', 0):.4f}\")\n",
    "\n",
    "# Performance Analysis (PRESERVED + ENHANCED)\n",
    "print(\"\\nPERFORMANCE INSIGHTS:\")\n",
    "print(f\"‚Ä¢ MAE {test_results['eval_mae']:.2f}: On average off by {test_results['eval_mae']:.2f} sentiment levels\")\n",
    "print(f\"‚Ä¢ Adjacent Accuracy {test_results['eval_adjacent_accuracy']:.1%}: Predictions within 1 sentiment level\")\n",
    "print(f\"‚Ä¢ QWK {test_results['eval_quadratic_weighted_kappa']:.3f}: {'Excellent' if test_results['eval_quadratic_weighted_kappa'] > 0.8 else 'Good' if test_results['eval_quadratic_weighted_kappa'] > 0.6 else 'Moderate'} ordinal agreement\")\n",
    "# Added inference speed and efficiency insights\n",
    "print(f\"‚Ä¢ Inference Speed: {1/inference_time_sec:.1f} predictions per second\")\n",
    "print(f\"‚Ä¢ Model Efficiency: {model_size_mb:.1f} MB storage required\")\n",
    "\n",
    "# Class Performance Analysis (PRESERVED)\n",
    "f1_scores = [test_results.get(f'eval_f1_{class_key}', 0) for class_key in sentiment_classes]\n",
    "best_class_idx = f1_scores.index(max(f1_scores))\n",
    "worst_class_idx = f1_scores.index(min(f1_scores))\n",
    "\n",
    "print(f\"\\nCLASS-SPECIFIC INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Best performing class: {class_display_names[best_class_idx]} (F1: {f1_scores[best_class_idx]:.4f})\")\n",
    "print(f\"‚Ä¢ Most challenging class: {class_display_names[worst_class_idx]} (F1: {f1_scores[worst_class_idx]:.4f})\")\n",
    "\n",
    "# COVID-specific insights (PRESERVED)\n",
    "extreme_avg = (f1_scores[0] + f1_scores[4]) / 2  # extremely negative + extremely positive\n",
    "moderate_avg = (f1_scores[1] + f1_scores[3]) / 2  # negative + positive\n",
    "neutral_score = f1_scores[2]\n",
    "\n",
    "print(f\"\\nCOVID SENTIMENT INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Extreme emotions (avg F1: {extreme_avg:.3f}): {'Challenging' if extreme_avg < 0.7 else 'Well-handled'}\")\n",
    "print(f\"‚Ä¢ Moderate emotions (avg F1: {moderate_avg:.3f}): {'Needs work' if moderate_avg < 0.75 else 'Good performance'}\")\n",
    "print(f\"‚Ä¢ Neutral sentiment (F1: {neutral_score:.3f}): {'Difficult to detect' if neutral_score < 0.8 else 'Well-identified'}\")\n",
    "\n",
    "# Final Summary with Key Metrics\n",
    "print(f\"\\nüìä FINAL SUMMARY:\")\n",
    "print(f\"F1: {test_results['eval_f1']:.4f} | QWK: {test_results['eval_quadratic_weighted_kappa']:.4f} | Inference: {inference_time_sec:.4f}s | Size: {model_size_mb:.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbqy2-UTV0q-",
   "metadata": {
    "id": "dbqy2-UTV0q-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "t8HZfeEkRm5q",
   "metadata": {
    "id": "t8HZfeEkRm5q"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "qfFcAYK09243",
   "metadata": {
    "id": "qfFcAYK09243"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "qfzxxnZlUwni",
   "metadata": {
    "id": "qfzxxnZlUwni"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8kbmFvP7ZE6X",
   "metadata": {
    "id": "8kbmFvP7ZE6X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
